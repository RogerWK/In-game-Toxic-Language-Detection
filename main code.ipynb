{"cells":[{"cell_type":"markdown","metadata":{"id":"H68Ol1ZBD24A"},"source":["#Load Library"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26611,"status":"ok","timestamp":1654070816456,"user":{"displayName":"roger wang","userId":"04313800435014207614"},"user_tz":-600},"id":"RX2b9AU8_BSA","outputId":"d8de08f1-83d8-4323-8031-9f8fa081382a"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","Mounted at /content/drive\n"]}],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","\n","import pickle\n","import numpy as np\n","from numpy import array\n","from numpy import argmax\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import OneHotEncoder\n","\n","from gensim.models import FastText\n","from gensim.models import Word2Vec\n","\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import f1_score\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import torch\n","import torch.autograd as autograd\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from tqdm.notebook import tqdm\n","\n","import pandas as pd\n","from tabulate import tabulate\n","\n","from operator import attrgetter\n","\n","torch.manual_seed(1)\n","\n","import time"]},{"cell_type":"markdown","metadata":{"id":"UP-vnc66Avfe"},"source":["#Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RXP0ImrpDY6t"},"outputs":[],"source":["training_data = pd.read_csv(\"/content/drive/MyDrive/COMP5046-NLP-AS2/Data/train.csv\")\n","testing_data = pd.read_csv(\"/content/drive/MyDrive/COMP5046-NLP-AS2/Data/test_without_labels.csv\")\n","validation_data = pd.read_csv(\"/content/drive/MyDrive/COMP5046-NLP-AS2/Data/val.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1653896455298,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"cpeYlNkmDg0s","outputId":"8a154794-f51c-43ac-e4eb-588ecb7f1499"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["               sents   labels\n","100             lmao        O\n","101  captain obvious      O O\n","102             ggwp        S\n","103              lol        O\n","104            right        O\n","..               ...      ...\n","195   you did not dc  P O O O\n","196              end        O\n","197         try hard      O O\n","198    end pls nomas    O O O\n","199               gg        S\n","\n","[100 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-6064de96-cd4a-4e85-8d6e-54b278c2d7f8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sents</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>100</th>\n","      <td>lmao</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>101</th>\n","      <td>captain obvious</td>\n","      <td>O O</td>\n","    </tr>\n","    <tr>\n","      <th>102</th>\n","      <td>ggwp</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>103</th>\n","      <td>lol</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>104</th>\n","      <td>right</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>195</th>\n","      <td>you did not dc</td>\n","      <td>P O O O</td>\n","    </tr>\n","    <tr>\n","      <th>196</th>\n","      <td>end</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>197</th>\n","      <td>try hard</td>\n","      <td>O O</td>\n","    </tr>\n","    <tr>\n","      <th>198</th>\n","      <td>end pls nomas</td>\n","      <td>O O O</td>\n","    </tr>\n","    <tr>\n","      <th>199</th>\n","      <td>gg</td>\n","      <td>S</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6064de96-cd4a-4e85-8d6e-54b278c2d7f8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6064de96-cd4a-4e85-8d6e-54b278c2d7f8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6064de96-cd4a-4e85-8d6e-54b278c2d7f8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}],"source":["training_data[100:200]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1654070819268,"user":{"displayName":"roger wang","userId":"04313800435014207614"},"user_tz":-600},"id":"ztWs9f-ZN9QJ","outputId":"521d54e9-1da4-41a6-89e4-dc8075244e85"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training data shape is:  (26078, 2)\n","Testing data shape is:  (500, 1)\n","Validation data shape is:  (8705, 2)\n"]}],"source":["print('Training data shape is: ',training_data.shape)\n","print('Testing data shape is: ',testing_data.shape)\n","print('Validation data shape is: ',validation_data.shape)"]},{"cell_type":"markdown","metadata":{"id":"K6HvZ57BFfXW"},"source":["#Data pre-processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QSFo9XtAbkkl"},"outputs":[],"source":["# Extract the labels and sents and store into List\n","\n","# Get the list of training data (sents)\n","training_sents=training_data['sents'].tolist()\n","# Get the list of corresponding labels for the training data \n","training_labels=training_data['labels'].tolist()\n","\n","# Get the list of validation data (sents)\n","validation_sents=validation_data['sents'].tolist()\n","# Get the list of corresponding labels for the validation data\n","validation_labels=validation_data['labels'].tolist()\n","\n","# Get the list of testing data (sents)\n","testing_sents=testing_data['sents'].tolist()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CGMevpKgLbmk"},"outputs":[],"source":["# PreProcess - remove [ ] from SEPA\n","training_sents_without_bk =[]\n","for i in range(len(training_sents)):\n","  training_sents_without_bk.append(training_sents[i].replace('[','').replace(']',''))\n","\n","validation_sents_without_bk =[]\n","for i in range(len(validation_sents)):\n","  validation_sents_without_bk.append(validation_sents[i].replace('[','').replace(']',''))\n","\n","testing_sents_without_bk =[]\n","for i in range(len(testing_sents)):\n","  testing_sents_without_bk.append(testing_sents[i].replace('[','').replace(']',''))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7N5bBrtUawed"},"outputs":[],"source":["#Dont have to delete this part, it is a good example that we might can discuss in our report about this evaluation\n","'''\n","# tokenized training sents\n","tokensized_training_docs=[]\n","for i in range (0,len(training_sents_without_bk)):\n","  tokenized_doc = word_tokenize(training_sents_without_bk[i]) #Tokenize words\n","  tokensized_training_docs.append(tokenized_doc)\n","\n","# tokenized validation sents\n","tokensized_validation_docs=[]\n","for i in range (0,len(validation_sents_without_bk)):\n","  tokenized_doc = word_tokenize(validation_sents_without_bk[i]) #Tokenize words\n","  tokensized_validation_docs.append(tokenized_doc)\n","  \n","# tokenized testing sents\n","tokensized_testing_docs=[]\n","for i in range (0,len(testing_sents_without_bk)):\n","  tokenized_doc = word_tokenize(testing_sents_without_bk[i]) #Tokenize words\n","  tokensized_testing_docs.append(tokenized_doc)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z5l5EMFcDm7X"},"outputs":[],"source":["# tokenized training sents\n","tokensized_training_docs=[]\n","for i in training_sents_without_bk:\n","  #print('inpout I is:', i)\n","  sub_list=list()\n","  i=i.split(\" \")\n","  for b in i:\n","    sub_list.append(b.lower())\n","  tokensized_training_docs.append(sub_list)\n","  #print('output is: ',sub_list )\n","\n","# tokenized validation sents\n","tokensized_validation_docs=[]\n","for i in validation_sents_without_bk:\n","  sub_list=list()\n","  i=i.split(\" \")\n","  for b in i:\n","    sub_list.append(b.lower())\n","  tokensized_validation_docs.append(sub_list)\n","\n","# tokenized testing sents\n","tokensized_testing_docs=[]\n","for i in testing_sents_without_bk:\n","  sub_list=list()\n","  i=i.split(\" \")\n","  for b in i:\n","    sub_list.append(b.lower())\n","  tokensized_testing_docs.append(sub_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lD9_TJeRADag"},"outputs":[],"source":["# tokenized training labels\n","tokensized_training_label_docs=[]\n","for i in training_labels:\n","  b=i.split(\" \")\n","  sub_label=list()\n","  for token in b:\n","    sub_label.append(token)\n","  tokensized_training_label_docs.append(sub_label)\n","\n","\n","# tokenized validation labels\n","tokensized_validation_label_docs=[]\n","for i in validation_labels:\n","  b=i.split(\" \")\n","  sub_label=list()\n","  for token in b:\n","    sub_label.append(token)\n","  tokensized_validation_label_docs.append(sub_label)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1653889198544,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"IICkh3uCdhvL","outputId":"3d87fe09-3be3-4677-9fa1-9bc2bd25f31e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['wow'],\n"," ['wtf'],\n"," ['wpe', 'wpe'],\n"," ['hahaha'],\n"," ['wtf'],\n"," ['i', 'cant', 'sepa', 'play', 'sepa', 'with', '4', 'trash'],\n"," ['bg'],\n"," ['#error!'],\n"," ['gg', 'sepa', 'report', 'my', 'team', 'rat', 'sepa', 'please'],\n"," ['ez', 'mid']]"]},"metadata":{},"execution_count":9}],"source":["#example training docs\n","tokensized_training_docs[0:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1653826958188,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"AtUyKkp1Cgj0","outputId":"e43af52a-cddb-406b-f5c9-4dd2672b5531"},"outputs":[{"data":{"text/plain":["[['O'],\n"," ['T'],\n"," ['O', 'O'],\n"," ['O'],\n"," ['T'],\n"," ['P', 'O', 'SEPA', 'O', 'SEPA', 'O', 'O', 'O'],\n"," ['O'],\n"," ['O'],\n"," ['S', 'SEPA', 'S', 'P', 'O', 'S', 'SEPA', 'O'],\n"," ['S', 'S']]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["#example training docs\n","tokensized_training_label_docs[0:10]"]},{"cell_type":"code","source":["all_label_flat=[]\n","for i in tokensized_training_label_docs:\n","  for t in i:\n","    all_label_flat.append(t)\n","print(len(all_label_flat))\n","print(all_label_flat[:10])\n","print (Counter(all_label_flat))\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","\n","labels, values = zip(*Counter(all_label_flat).items())\n","\n","indexes = np.arange(len(labels))\n","width = 0.8\n","\n","plt.bar(indexes, values, width)\n","plt.xticks(indexes + width * 0.05, labels)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":319},"id":"PlKIX2Tg4CZn","executionInfo":{"status":"ok","timestamp":1653913171921,"user_tz":-600,"elapsed":345,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"}},"outputId":"75f657c0-0877-4a09-9aaf-22b5cb214186"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["99611\n","['O', 'T', 'O', 'O', 'O', 'T', 'P', 'O', 'SEPA', 'O']\n","Counter({'O': 56815, 'P': 11997, 'SEPA': 10418, 'S': 10035, 'C': 4780, 'T': 4292, 'D': 1274})\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5zwpL09LDCaJ"},"outputs":[],"source":["#Test if there is a mismatch between tokensized train data and train label\n","count=0\n","for a,b in zip(tokensized_validation_docs, tokensized_validation_label_docs):\n","  count+=1\n","  if len(a)==len(b):\n","   print('All good')\n","  else:\n","    print('mismatch here: ', count)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":502,"status":"ok","timestamp":1653826970883,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"58kQcWVMpPVf","outputId":"b9ab481a-9a55-4243-9226-24ef7330a3ca"},"outputs":[{"data":{"text/plain":["26078"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["len(tokensized_training_label_docs)"]},{"cell_type":"markdown","metadata":{"id":"Rhn0V1PnCBww"},"source":["### Label encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-WMokalACGJu"},"outputs":[],"source":["START_TAG = \"<START>\"\n","STOP_TAG = \"<STOP>\"\n","#Create a dict for labels\n","\n","label_set=set()\n","\n","for i in training_labels:\n","  b=i.split(\" \")\n","  for token in b:\n","    label_set.add(token)\n","\n","label_set.add(START_TAG)\n","label_set.add(STOP_TAG)\n","\n","tag_to_ix = {\"C\": 0, \"D\": 1, \"O\": 2, \"P\":3, \"S\":4, \"SEPA\":5, \"T\":6, \"<START>\": 7, \"<STOP>\": 8}\n","ix_to_tag= {0:\"C\", 1:\"D\", 2:\"O\", 3:\"P\", 4:\"S\", 5:\"SEPA\", 6:\"T\", 7:\"<START>\", 8:\"<STOP>\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":352,"status":"ok","timestamp":1653792880871,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"H1IvmvpyWUB9","outputId":"b723d4f0-7fc4-4670-dd9a-3a95a769e099"},"outputs":[{"data":{"text/plain":["{'<START>', '<STOP>', 'C', 'D', 'O', 'P', 'S', 'SEPA', 'T'}"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["label_set"]},{"cell_type":"markdown","metadata":{"id":"Vt_6z2Ka-k_8"},"source":["###Convert dataset into index"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":286,"status":"ok","timestamp":1654070837862,"user":{"displayName":"roger wang","userId":"04313800435014207614"},"user_tz":-600},"id":"aHrnLLGqgnDZ","outputId":"4e9cdfc5-24c0-47c4-a6c4-dfa1fe542209"},"outputs":[{"output_type":"stream","name":"stdout","text":["11240\n"]}],"source":["#combine 3 data sets to generate word list\n","tokensized_sents_full=tokensized_training_docs+tokensized_validation_docs+tokensized_testing_docs\n","unique_word_set=set()\n","for sent in tokensized_sents_full:\n","  for word in sent:\n","    unique_word_set.add(word)\n","\n","word_list=list(unique_word_set)\n","word_list.sort()\n","word_dict = {w: i for i, w in enumerate(word_list)}\n","\n","print(len(word_list))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pBdVwqC7-oFq"},"outputs":[],"source":["def to_index(data, to_ix):\n","    input_index_list = []\n","    for sent in data:\n","        input_index_list.append([to_ix[w] for w in sent])\n","    return input_index_list\n","\n","#input index\n","train_input_index =  to_index(tokensized_training_docs,word_dict)\n","train_output_index = to_index(tokensized_training_label_docs,tag_to_ix)\n","\n","val_input_index = to_index(tokensized_validation_docs,word_dict)\n","val_output_index = to_index(tokensized_validation_label_docs,tag_to_ix)\n","\n","test_input_index = to_index(tokensized_testing_docs,word_dict)\n"]},{"cell_type":"markdown","metadata":{"id":"RFsUO7MZM1jh"},"source":["#Evaluation function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J6P-OdpGM4vv"},"outputs":[],"source":["def calculate_f1(y_pred, y_true):\n","  y_pred=np.array(y_pred)\n","  y_true=np.array(y_true)\n","\n","  f1=f1_score(y_pred, y_true, average='micro')\n","  f1_T_label=f1_score(y_pred[y_true==tag_to_ix['T']], y_true[y_true==tag_to_ix['T']], average='micro')\n","  f1_S_label=f1_score(y_pred[y_true==tag_to_ix['S']], y_true[y_true==tag_to_ix['S']], average='micro')\n","  f1_P_label=f1_score(y_pred[y_true==tag_to_ix['P']], y_true[y_true==tag_to_ix['P']], average='micro')\n","  f1_O_label=f1_score(y_pred[y_true==tag_to_ix['O']], y_true[y_true==tag_to_ix['O']], average='micro')\n","  f1_D_label=f1_score(y_pred[y_true==tag_to_ix['D']], y_true[y_true==tag_to_ix['D']], average='micro')\n","  f1_C_label=f1_score(y_pred[y_true==tag_to_ix['C']], y_true[y_true==tag_to_ix['C']], average='micro')\n","\n","  return f1, f1_T_label ,f1_S_label, f1_P_label, f1_O_label, f1_D_label, f1_C_label\n"]},{"cell_type":"markdown","metadata":{"id":"bjHN4W_XDzxm"},"source":["# Input Embedding"]},{"cell_type":"markdown","metadata":{"id":"R7pZ5N9OHfxH"},"source":["##Syntactic Textual Feature Embedding\n","e.g. : PoS tag information, Dependency Path, etc"]},{"cell_type":"markdown","metadata":{"id":"UemM94nVHFsQ"},"source":["###POS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10702,"status":"ok","timestamp":1654073281300,"user":{"displayName":"roger wang","userId":"04313800435014207614"},"user_tz":-600},"id":"A3-aKqIB1ruP","outputId":"f089e32b-bae5-401f-f199-169603f70297"},"outputs":[{"output_type":"stream","name":"stdout","text":["11240\n","['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n"," 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X']\n","[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n","[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n","{'ADJ': 0, 'ADP': 1, 'ADV': 2, 'AUX': 3, 'CCONJ': 4, 'DET': 5, 'INTJ': 6, 'NOUN': 7, 'NUM': 8, 'PART': 9, 'PRON': 10, 'PROPN': 11, 'PUNCT': 12, 'SCONJ': 13, 'SYM': 14, 'VERB': 15, 'X': 16}\n"]}],"source":["# generate lookup info for POS one hot encoding \n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","POS =[]\n","POS_set = set()\n","\n","# use nlp.pipe() instead of nlp() to process multiple texts more efficiently\n","for doc in nlp.pipe(word_list):\n","    if len(doc) > 0:\n","        POS.append(doc[0].pos_)\n","        POS_set.add(doc[0].pos_)\n","print(len(POS))  #------ POS has all the name of POS e.g X, PUNCT\n","\n","# generate unique list of POS taggers\n","POS_list=list(POS_set)\n","\n","# Convert to array to put in Sklearn Encoders\n","POS_list.sort()\n","values = array(POS_list)\n","print(values)\n","\n","# integer encode\n","label_encoder = LabelEncoder()\n","integer_encoded = label_encoder.fit_transform(values)\n","print(integer_encoded)\n","\n","# binary encode\n","onehot_encoder = OneHotEncoder(sparse=False)\n","integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n","onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n","print(onehot_encoded)\n","\n","# Generate POS tagging index dictionary, e.g ADJ : 0 \n","POS_index = {}\n","ind = 0\n","for word in POS_list:\n","    POS_index[word] = ind\n","    ind += 1\n","print(POS_index)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1654073281302,"user":{"displayName":"roger wang","userId":"04313800435014207614"},"user_tz":-600},"id":"nLiOiTInsstZ","outputId":"3f1cd166-fdcf-40c4-a593-45ddc455f997"},"outputs":[{"output_type":"stream","name":"stdout","text":["11240\n","(11240, 17)\n"]}],"source":["#generate POS one hot encoding for word_list\n","\n","#generate index of each sentence e.g 2,7,etc\n","list_index=[]\n","for i in POS: #POS_all [0:5]:\n","  list_index.append(POS_index[i])\n","\n","# convert index to onehot_encoded\n","POS2Vector = []\n","for i in list_index:\n","  POS2Vector.append(onehot_encoded[i])\n","\n","print(len(POS2Vector))\n","POS_embedding_matrix = np.array(POS2Vector)\n","print(POS_embedding_matrix.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1654073281304,"user":{"displayName":"roger wang","userId":"04313800435014207614"},"user_tz":-600},"id":"OdB4rY7Fp2Ff","outputId":"d18c97c0-5c12-47f3-9808-f602b8d6d6f6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"]},"metadata":{},"execution_count":30}],"source":["POS_embedding_matrix[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1654073281305,"user":{"displayName":"roger wang","userId":"04313800435014207614"},"user_tz":-600},"id":"wj6OYCRWKLjW","outputId":"677cb27c-f694-49c7-986d-15f840a5aa1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["['NOUN', 'PUNCT', 'PUNCT', 'PUNCT', 'NUM', 'NUM', 'NUM', 'X', 'PUNCT', 'NUM']\n","[array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])]\n"]}],"source":["#example of POS2Vector\n","print(POS[0:10])\n","print(POS2Vector[0:10])"]},{"cell_type":"markdown","metadata":{"id":"xNw43IG6HmX_"},"source":["##Semantic Textual Feature Embedding\n","e.g.Word Embeddings (Word2Vec, ELMO, etc."]},{"cell_type":"markdown","metadata":{"id":"cwBkUfLvg5Sr"},"source":["###Word2Vec-Skipgram"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sk09pHjGkMn1"},"outputs":[],"source":["#train a skip-gram W2V model\n","w2v_sg_model = Word2Vec(tokensized_sents_full, size=200, window=5, min_count=1, workers=4, sg=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1653827033229,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"qWEyS2Poj-y8","outputId":"58307ab4-80dc-400f-db42-df4fc37801bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_items([('#error!', 0), ('.', 1), ('0', 2), ('00', 3), ('000', 4), ('000000', 5), ('000000000000000', 6), ('00100', 7), ('0162282307', 8), ('025', 9), ('040', 10), ('06', 11), ('0:00', 12), ('0coins', 13), ('0o', 14), ('0v5', 15), ('0ward', 16), ('1', 17), ('1.', 18), ('10', 19), ('100', 20), ('1000', 21), ('10000', 22), ('100000', 23), ('100hp', 24), ('101', 25), ('10120', 26), ('102', 27), ('103', 28), ('10k', 29), ('10min', 30), ('10mins', 31), ('10pm', 32), ('10x', 33), ('11', 34), ('110', 35), ('1110', 36), ('113', 37), ('113924617', 38), ('115', 39), ('1150', 40), ('118', 41), ('1188', 42), ('119', 43), ('11mins', 44), ('11pm', 45), ('12', 46), ('120s', 47), ('12k', 48), ('12mins', 49), ('12y', 50), ('13', 51), ('130cannot', 52), ('134', 53), ('1389461397419', 54), ('13:8', 55), ('13mins', 56), ('14', 57), ('1400', 58), ('143', 59), ('1450', 60), ('149', 61), ('15', 62), ('150', 63), ('1500', 64), ('15200', 65), ('1584', 66), ('15min', 67), ('15y', 68), ('16', 69), ('1600', 70), ('1651', 71), ('17', 72), ('175', 73), ('179', 74), ('18', 75), ('180', 76), ('185', 77), ('19', 78), ('190', 79), ('1973', 80), ('198', 81), ('1:11', 82), ('1:23', 83), ('1:30', 84), ('1:7', 85), ('1by', 86), ('1by1', 87), ('1cd', 88), ('1hr', 89), ('1k', 90), ('1m', 91), ('1min', 92), ('1mins', 93), ('1on1', 94), ('1rs', 95), ('1rst', 96), ('1st', 97), ('1ult', 98), ('1v', 99), ('1v1', 100), ('1v2', 101), ('1v3', 102), ('1v4', 103), ('1v5', 104), ('1v9', 105), ('1vs', 106), ('1vs1', 107), ('1vs5', 108), ('1vs9', 109), ('1vs9090123', 110), ('1x', 111), ('1x1', 112), ('1x5', 113), ('1year', 114), ('1z1', 115), ('2', 116), ('20', 117), ('200', 118), ('2000', 119), ('2005', 120), ('2008', 121), ('200deward', 122), ('201', 123), ('2010', 124), ('2012', 125), ('2015', 126), ('2016', 127), ('2018', 128), ('2020', 129), ('20mins', 130), ('20sec', 131), ('21', 132), ('210', 133), ('2100', 134), ('211', 135), ('2111', 136), ('213', 137), ('2171', 138), ('21mins', 139), ('22', 140), ('2248', 141), ('228', 142), ('22min', 143), ('22mins', 144), ('23', 145), ('2300', 146), ('233', 147), ('23:59', 148), ('24', 149), ('242', 150), ('247', 151), ('24hrs', 152), ('25', 153), ('250', 154), ('2500', 155), ('2500ms', 156), ('25mins', 157), ('25mmr', 158), ('26', 159), ('266', 160), ('27', 161), ('2721', 162), ('274', 163), ('275', 164), ('27min', 165), ('28', 166), ('2800', 167), ('29', 168), ('29hp', 169), ('2:11', 170), ('2bad', 171), ('2compos', 172), ('2dedos', 173), ('2e', 174), ('2easy', 175), ('2es4us', 176), ('2ez', 177), ('2ez4wr', 178), ('2fckng', 179), ('2gud', 180), ('2hits', 181), ('2hp', 182), ('2hr', 183), ('2in', 184), ('2k', 185), ('2kmmr', 186), ('2m', 187), ('2many', 188), ('2mid', 189), ('2min', 190), ('2mins', 191), ('2month', 192), ('2months', 193), ('2moro', 194), ('2nd', 195), ('2non', 196), ('2others', 197), ('2v1', 198), ('2v5', 199), ('2v8', 200), ('2vice', 201), ('2x', 202), ('2x3', 203), ('2x8', 204), ('3', 205), ('30', 206), ('300', 207), ('3000', 208), ('300mins', 209), ('30am', 210), ('30k', 211), ('30m', 212), ('30min', 213), ('30mins', 214), ('30minutes', 215), ('30s', 216), ('30sec', 217), ('30seconds', 218), ('31', 219), ('32', 220), ('322', 221), ('329', 222), ('33', 223), ('330', 224), ('3300', 225), ('332', 226), ('338', 227), ('34', 228), ('340', 229), ('345', 230), ('348', 231), ('34min', 232), ('35', 233), ('350', 234), ('352', 235), ('35m', 236), ('36', 237), ('360', 238), ('361', 239), ('37', 240), ('38', 241), ('38flesh', 242), ('39', 243), ('3die', 244), ('3k', 245), ('3kforlife', 246), ('3kmmr', 247), ('3kscrub', 248), ('3m', 249), ('3min', 250), ('3mins', 251), ('3miss', 252), ('3mretards', 253), ('3pl', 254), ('3ple', 255), ('3rd', 256), ('3secs', 257), ('3streak', 258), ('3th', 259), ('3times', 260), ('3v', 261), ('3v1', 262), ('3v5', 263), ('3v7', 264), ('3vs5', 265), ('3winstreak', 266), ('3x', 267), ('3z', 268), ('4', 269), ('40', 270), ('400', 271), ('4000', 272), ('400g', 273), ('400ms', 274), ('40min', 275), ('40mins', 276), ('41', 277), ('419', 278), ('42', 279), ('420', 280), ('43', 281), ('4300', 282), ('43232', 283), ('44', 284), ('44455', 285), ('44th', 286), ('45', 287), ('4500', 288), ('46mins', 289), ('47', 290), ('48', 291), ('4800', 292), ('49', 293), ('490', 294), ('4:10', 295), ('4chan', 296), ('4d', 297), ('4dr', 298), ('4e', 299), ('4et', 300), ('4head', 301), ('4k', 302), ('4k6', 303), ('4ks', 304), ('4lyf', 305), ('4min', 306), ('4n4n', 307), ('4th', 308), ('4this', 309), ('4v', 310), ('4v1', 311), ('4v4', 312), ('4v5', 313), ('4v6', 314), ('4vs', 315), ('4vs4', 316), ('4vs5', 317), ('4x', 318), ('4x5', 319), ('5', 320), ('5)', 321), ('50', 322), ('500', 323), ('5000', 324), ('500gold', 325), ('500h', 326), ('50:00', 327), ('50min', 328), ('50point', 329), ('50th', 330), ('51', 331), ('515', 332), ('5150', 333), ('52', 334), ('5200', 335), ('524', 336), ('527', 337), ('53', 338), ('530', 339), ('54', 340), ('540', 341), ('55', 342), ('5500', 343), ('555', 344), ('5555', 345), ('5555555555555', 346), ('55min', 347), ('56', 348), ('56min', 349), ('5700', 350), ('576', 351), ('58', 352), ('59', 353), ('5:18', 354), ('5:25', 355), ('5hp', 356), ('5k', 357), ('5k4', 358), ('5kego', 359), ('5l', 360), ('5man', 361), ('5min', 362), ('5mins', 363), ('5minutes', 364), ('5mnt', 365), ('5s', 366), ('5th', 367), ('5v', 368), ('5v1', 369), ('5v3', 370), ('5v4', 371), ('5v5', 372), ('5vs1', 373), ('5vs4', 374), ('5x', 375), ('5x4', 376), ('6', 377), ('60', 378), ('600', 379), ('60000k', 380), ('60min', 381), ('61', 382), ('6100', 383), ('6200', 384), ('63', 385), ('6350', 386), ('65', 387), ('66=65', 388), ('66mins', 389), ('68', 390), ('69', 391), ('6k', 392), ('6mid', 393), ('6v4', 394), ('7', 395), ('70', 396), ('700', 397), ('71', 398), ('71commends', 399), ('727', 400), ('748', 401), ('75', 402), ('7k', 403), ('7min', 404), ('7u7', 405), ('8', 406), ('80', 407), ('800', 408), ('830', 409), ('833', 410), ('84', 411), ('85', 412), ('85st', 413), ('87mb', 414), ('88', 415), ('881', 416), ('89', 417), ('890', 418), ('8d', 419), ('8k', 420), ('8kg', 421), ('8kmmr', 422), ('8ks', 423), ('8min', 424), ('8th', 425), ('8wins', 426), ('8with', 427), ('9', 428), ('9(', 429), ('90', 430), ('900', 431), ('9000', 432), ('900gold', 433), ('900hp', 434), ('938', 435), ('959', 436), ('96', 437), ('999', 438), ('9999', 439), ('9999k', 440), ('9k', 441), ('9x', 442), (':', 443), (':#', 444), (\":'\", 445), (':(', 446), (':)', 447), (':)0', 448), (':*', 449), (':-', 450), (':/', 451), (':0', 452), (':3', 453), ('::)', 454), (':::::::::::::::::::::::::::)', 455), ('::@', 456), ('::_', 457), ('::d', 458), ('::dd', 459), (':;', 460), (':=', 461), (':>', 462), (':@', 463), (':\\\\', 464), (':^', 465), (':_', 466), (':`', 467), (':c', 468), (':d', 469), (':dd', 470), (':ddd', 471), (':dddd', 472), (':ddddd', 473), (':ddddddd', 474), (':ddddddddddd', 475), (':dddddddddddd', 476), (':ddddddddddddd', 477), (':dddddddddddddd', 478), (':ddddddddddddddddddd', 479), (':dddddddddddddddddddddddd', 480), (':ddddddddddddddddddddddddddddddd', 481), (':derp', 482), (':egkaya', 483), (':fmaop', 484), (':l', 485), (':laugh', 486), (':m', 487), (':o', 488), (':ooo', 489), (':p', 490), (':po', 491), (':pp', 492), (':q', 493), (':s', 494), (':salty', 495), (':slaty', 496), (':v', 497), (':x', 498), (':{', 499), (':|', 500), (':\\ue006', 501), (';', 502), (';!', 503), (';(', 504), (';)', 505), (';-;', 506), (';/', 507), (';3', 508), (';:d', 509), (';;', 510), (';\\\\', 511), (';_:', 512), (';_;', 513), ('<', 514), ('<3', 515), ('<<', 516), ('<<<<<', 517), ('=', 518), ('=(', 519), ('=)', 520), ('=-', 521), ('=-=', 522), ('=.=', 523), ('=/', 524), ('=3=', 525), ('=6k', 526), ('==', 527), ('===3', 528), ('=\\\\', 529), ('=d', 530), ('=dd', 531), ('=gg', 532), ('=m', 533), ('=p', 534), ('=raport', 535), ('=s', 536), ('=trash', 537), ('=w', 538), ('`', 539), ('a', 540), ('a3', 541), ('aa', 542), ('aaa', 543), ('aaaa', 544), ('aaaaa', 545), ('aaaaaa', 546), ('aaaaaaaaaaaaaaa', 547), ('aaaaaaaaaaaaaaaaa', 548), ('aaaaaaaaaaaaaaaaaa', 549), ('aaaahaaajajaa', 550), ('aaaha', 551), ('aaahajahahahahhahahahahahaha', 552), ('aabut', 553), ('aadu', 554), ('aaha', 555), ('aahaha', 556), ('aahahhaa', 557), ('aahahhahahah', 558), ('aahha', 559), ('aahhaha', 560), ('aaxax', 561), ('aaya', 562), ('aayy', 563), ('aazazazazaza', 564), ('aba', 565), ('abaddon', 566), ('abadon', 567), ('abanbdoned', 568), ('abandon', 569), ('abandoned', 570), ('abandoning', 571), ('abante', 572), ('abanuti', 573), ('abba', 574), ('abbadon', 575), ('abbadong', 576), ('abbandon', 577), ('abbandons', 578), ('abd', 579), ('abddon', 580), ('abilities', 581), ('ability', 582), ('abit', 583), ('abited', 584), ('able', 585), ('abondon', 586), ('abortion', 587), ('about', 588), ('above', 589), ('abreak', 590), ('abse', 591), ('absolute', 592), ('absolutely', 593), ('absolutley', 594), ('absurb', 595), ('abt', 596), ('abuse', 597), ('abuser', 598), ('abusing', 599), ('abusive', 600), ('abysal', 601), ('abyss', 602), ('ac', 603), ('acalming', 604), ('acc', 605), ('accbuyers', 606), ('accept', 607), ('accepted', 608), ('accident', 609), ('accidental', 610), ('accidentally', 611), ('accont', 612), ('account', 613), ('accountbuyer', 614), ('accounts', 615), ('accout', 616), ('accoutn', 617), ('acha', 618), ('ache', 619), ('achi', 620), ('achievement', 621), ('acid', 622), ('ackt', 623), ('aclhe', 624), ('aclick', 625), ('acommend', 626), ('acount', 627), ('act', 628), ('acted', 629), ('acting', 630), ('action', 631), ('activated', 632), ('active', 633), ('actual', 634), ('actualizacion', 635), ('actually', 636), ('actualy', 637), ('actualyl', 638), ('actuially', 639), ('acutaly', 640), ('acutalyl', 641), ('ad', 642), ('adadas', 643), ('adboys', 644), ('add', 645), ('added', 646), ('address', 647), ('addtime', 648), ('adik', 649), ('adjusted', 650), ('admire', 651), ('admit', 652), ('adn', 653), ('adorno', 654), ('adoro', 655), ('ads', 656), ('advantage', 657), ('ady', 658), ('aeg', 659), ('aeghis', 660), ('aegis', 661), ('aego', 662), ('aeguiis', 663), ('aelg', 664), ('aer', 665), ('aesthetics', 666), ('aewfiooiufbfon', 667), ('af', 668), ('afaik', 669), ('afasihon', 670), ('aff', 671), ('affect', 672), ('afff', 673), ('affirmative', 674), ('afford', 675), ('afk', 676), ('afkd', 677), ('afking', 678), ('afkk', 679), ('afks', 680), ('afl', 681), ('afraid', 682), ('afrer', 683), ('africa', 684), ('after', 685), ('afterall', 686), ('afterwards', 687), ('afucking', 688), ('aga', 689), ('agad', 690), ('agaga', 691), ('agagin', 692), ('again', 693), ('against', 694), ('againstt', 695), ('againsty', 696), ('againt', 697), ('againts', 698), ('agaisnt', 699), ('agaisntt', 700), ('agamis', 701), ('aganim', 702), ('aganimas', 703), ('agans', 704), ('agaw', 705), ('age', 706), ('aggg', 707), ('agggg', 708), ('aggresif', 709), ('aggresive', 710), ('aggressive', 711), ('aggro', 712), ('agh', 713), ('agha', 714), ('aghahahhaha', 715), ('aghanim', 716), ('aghanims', 717), ('aghs', 718), ('agian', 719), ('agies', 720), ('agio', 721), ('agk', 722), ('agme', 723), ('agnus', 724), ('ago', 725), ('agradecele', 726), ('agree', 727), ('agreed', 728), ('agreee', 729), ('agressif', 730), ('agressive', 731), ('agro', 732), ('ags', 733), ('aguanta', 734), ('ah', 735), ('aha', 736), ('ahaah', 737), ('ahaaha', 738), ('ahaahaah', 739), ('ahaahaha', 740), ('ahaahahahah', 741), ('ahaahahahha', 742), ('ahadd', 743), ('ahah', 744), ('ahaha', 745), ('ahahaa', 746), ('ahahaah', 747), ('ahahaahah', 748), ('ahahah', 749), ('ahahaha', 750), ('ahahahaah', 751), ('ahahahaahaha', 752), ('ahahahah', 753), ('ahahahaha', 754), ('ahahahahah', 755), ('ahahahahaha', 756), ('ahahahahahahaah', 757), ('ahahahahahahaha', 758), ('ahahahahahahahah', 759), ('ahahahahahahahahha', 760), ('ahahahahahahhaha', 761), ('ahahahahahha', 762), ('ahahahahha', 763), ('ahahahh', 764), ('ahahahha', 765), ('ahahahhaa', 766), ('ahahahhaahha', 767), ('ahahahhahahah', 768), ('ahahahjajhajajajja', 769), ('ahahasha', 770), ('ahahayeah', 771), ('ahahgah', 772), ('ahahh', 773), ('ahahha', 774), ('ahahhaa', 775), ('ahahhaah', 776), ('ahahhaahahah', 777), ('ahahhah', 778), ('ahahhahaha', 779), ('ahayhayhay', 780), ('ahead', 781), ('ahev', 782), ('ahgaha', 783), ('ahgahahahaha', 784), ('ahgahhaa', 785), ('ahghaha', 786), ('ahghahahah', 787), ('ahh', 788), ('ahha', 789), ('ahhaa', 790), ('ahhaah', 791), ('ahhah', 792), ('ahhaha', 793), ('ahhahaa', 794), ('ahhahaha', 795), ('ahhahahaa', 796), ('ahhahahah', 797), ('ahhahahaha', 798), ('ahhahahahaha', 799), ('ahhahha', 800), ('ahhasda', 801), ('ahhh', 802), ('ahhhh', 803), ('ahhhhhhhh', 804), ('ahhhhhhhhhhhhhh', 805), ('ahi', 806), ('ahiuhi', 807), ('ahmed', 808), ('ahora', 809), ('ahrd', 810), ('ahs', 811), ('ahv', 812), ('ahve', 813), ('ahwh', 814), ('ahyahaha', 815), ('ahyhahha', 816), ('ai', 817), ('aids', 818), ('aie', 819), ('aight', 820), ('aiiiii', 821), ('aim', 822), ('aime', 823), ('aiming', 824), ('ainda', 825), ('ainsley', 826), ('aint', 827), ('air', 828), ('aise', 829), ('ait', 830), ('aiya', 831), ('aiyo', 832), ('ajahhahahaa', 833), ('ajaja', 834), ('ajajajaja', 835), ('ajajajajaa', 836), ('ajajajajahahahah', 837), ('ajajajajaja', 838), ('ajajajja', 839), ('ajajajjaaj', 840), ('ajajja', 841), ('ajjaja', 842), ('ajjajaj', 843), ('ajjajaja', 844), ('ajo', 845), ('ak', 846), ('aka', 847), ('akakakakakaka', 848), ('akbaaar', 849), ('akbar', 850), ('akf', 851), ('akiro', 852), ('akk', 853), ('ako', 854), ('akowokwakoawo', 855), ('akoy', 856), ('aksdakjdsjkaskdja', 857), ('aku', 858), ('al', 859), ('ala', 860), ('alachi', 861), ('alahu', 862), ('alarm', 863), ('alc', 864), ('alca', 865), ('alce', 866), ('alceh', 867), ('alcehmist', 868), ('alch', 869), ('alche', 870), ('alchee', 871), ('alcheeeee', 872), ('alchem', 873), ('alchemas', 874), ('alchemist', 875), ('alchesa', 876), ('alchi', 877), ('alchie', 878), ('alchim', 879), ('alchj', 880), ('alchje', 881), ('alchs', 882), ('ald', 883), ('ale', 884), ('alee', 885), ('aleg', 886), ('aleready', 887), ('alert', 888), ('alesso', 889), ('alg', 890), ('algo', 891), ('alguien', 892), ('alien', 893), ('alienware', 894), ('aliev', 895), ('alive', 896), ('alkemist', 897), ('all', 898), ('allah', 899), ('allahu', 900), ('allhis', 901), ('alliance', 902), ('allied', 903), ('allies', 904), ('alll', 905), ('allllll', 906), ('allov', 907), ('allow', 908), ('allowed', 909), ('allways', 910), ('ally', 911), ('almost', 912), ('alo', 913), ('alolololo', 914), ('alone', 915), ('along', 916), ('aloone', 917), ('alot', 918), ('alowed', 919), ('alpha', 920), ('alr', 921), ('alrady', 922), ('alrd', 923), ('alrdy', 924), ('alreadty', 925), ('already', 926), ('alreayd', 927), ('alredy', 928), ('alrigfht', 929), ('alrigh', 930), ('alright', 931), ('alrighty', 932), ('alrite', 933), ('als', 934), ('alsdasd', 935), ('also', 936), ('alt', 937), ('altest', 938), ('although', 939), ('alttab', 940), ('aluukhawakbar', 941), ('alway', 942), ('always', 943), ('alwayz', 944), ('alwkawkakw', 945), ('alwys', 946), ('alyansov', 947), ('alyer', 948), ('alyway', 949), ('am', 950), ('ama', 951), ('amazing', 952), ('ambulance', 953), ('amchong', 954), ('amed', 955), ('amen', 956), ('america', 957), ('american', 958), ('amf', 959), ('amiga', 960), ('amigos', 961), ('amirite', 962), ('amk', 963), ('amke', 964), ('amming', 965), ('ammirite', 966), ('among', 967), ('amor', 968), ('amount', 969), ('ampas', 970), ('ampota', 971), ('amte', 972), ('amulet', 973), ('an', 974), ('anal', 975), ('analed', 976), ('analfisting', 977), ('ancient', 978), ('ancients', 979), ('and', 980), ('andrea', 981), ('andu', 982), ('andwaitforhimself', 983), ('ang', 984), ('angle', 985), ('angry', 986), ('anient', 987), ('animais', 988), ('animal', 989), ('animales', 990), ('animation', 991), ('anime', 992), ('aniston', 993), ('anjign', 994), ('anjing', 995), ('annoucner', 996), ('annoy', 997), ('annoyance', 998), ('annoyed', 999), ('annoyin', 1000), ('annoying', 1001), ('annus', 1002), ('ano', 1003), ('another', 1004), ('anothing', 1005), ('anough', 1006), ('answer', 1007), ('ant', 1008), ('anthem', 1009), ('anti', 1010), ('antifun', 1011), ('antigay', 1012), ('antimage', 1013), ('ants', 1014), ('anu', 1015), ('anus', 1016), ('anw', 1017), ('any', 1018), ('any1', 1019), ('anybodsy', 1020), ('anybody', 1021), ('anyday', 1022), ('anymmore', 1023), ('anymore', 1024), ('anyone', 1025), ('anything', 1026), ('anytime', 1027), ('anyting', 1028), ('anyway', 1029), ('anyways', 1030), ('anyyhing', 1031), ('ao', 1032), ('aoe', 1033), ('aosdsdg', 1034), ('aoxaoxaoxaoao', 1035), ('ap', 1036), ('apa', 1037), ('apahal', 1038), ('aparet', 1039), ('ape', 1040), ('apes', 1041), ('apologise', 1042), ('appak', 1043), ('apparently', 1044), ('appeared', 1045), ('appears', 1046), ('apperantly', 1047), ('apperently', 1048), ('apperenty', 1049), ('applaud', 1050), ('appreciate', 1051), ('aprty', 1052), ('aqop', 1053), ('aquilla', 1054), ('ar', 1055), ('ar1se', 1056), ('arab', 1057), ('aracana', 1058), ('arbol', 1059), ('arcana', 1060), ('arcane', 1061), ('are', 1062), ('area', 1063), ('arent', 1064), ('arer', 1065), ('aretard', 1066), ('areu', 1067), ('arguing', 1068), ('argument', 1069), ('arise', 1070), ('arl', 1071), ('arlmet', 1072), ('arm', 1073), ('armlet', 1074), ('armor', 1075), ('armour', 1076), ('armoure', 1077), ('arms', 1078), ('army', 1079), ('arnt', 1080), ('aroound', 1081), ('around', 1082), ('arrivall', 1083), ('arrogant', 1084), ('arrow', 1085), ('arrowed', 1086), ('arrows', 1087), ('arruine', 1088), ('arryin', 1089), ('arse', 1090), ('arsehole', 1091), ('art', 1092), ('article', 1093), ('artist', 1094), ('ary', 1095), ('as', 1096), ('asa', 1097), ('asad', 1098), ('asap', 1099), ('asco', 1100), ('asd', 1101), ('asdf', 1102), ('asdfsadfasdfasdfasdfasfasdfasdfsafasdfsdafsadfasfadsfasdfasdfasfasdfasdfsadfsadfasdfasdfasdfadsfasdfsadfdasfdsafdasfasdfasdfas', 1103), ('asdgfikom', 1104), ('asdgsdgh', 1105), ('asdijanosdas', 1106), ('asdjsaduhhdas', 1107), ('asdkljasdjklasdasd', 1108), ('asdpoijasoidj', 1109), ('aseeeeeee', 1110), ('asfd', 1111), ('asi', 1112), ('asia', 1113), ('asian', 1114), ('asik', 1115), ('asingle', 1116), ('asissts', 1117), ('ask', 1118), ('askd', 1119), ('asked', 1120), ('askin', 1121), ('asking', 1122), ('asks', 1123), ('aslong', 1124), ('asmruf', 1125), ('aso', 1126), ('asquerosa', 1127), ('asquitos', 1128), ('ass', 1129), ('assasin', 1130), ('assassinate', 1131), ('assault', 1132), ('asscrack', 1133), ('asses', 1134), ('asshit', 1135), ('asshoels', 1136), ('asshole', 1137), ('assholes', 1138), ('assist', 1139), ('assistrs', 1140), ('assists', 1141), ('asssisst', 1142), ('astack', 1143), ('astart', 1144), ('astral', 1145), ('astraling', 1146), ('astrar', 1147), ('asu', 1148), ('aswell', 1149), ('at', 1150), ('ata', 1151), ('atack', 1152), ('atar', 1153), ('atard', 1154), ('atay', 1155), ('ate', 1156), ('atitude', 1157), ('atlas', 1158), ('atlast', 1159), ('atleast', 1160), ('atlest', 1161), ('atm', 1162), ('atos', 1163), ('atras', 1164), ('att', 1165), ('atta', 1166), ('attack', 1167), ('attacking', 1168), ('attacks', 1169), ('attaturk', 1170), ('attempted', 1171), ('attention', 1172), ('attitude', 1173), ('au', 1174), ('auawj', 1175), ('auch', 1176), ('auchhhh', 1177), ('ault', 1178), ('aura', 1179), ('aus', 1180), ('aushduasd', 1181), ('aussie', 1182), ('australia', 1183), ('autis', 1184), ('autism', 1185), ('autist', 1186), ('autistic', 1187), ('auto', 1188), ('autoattack', 1189), ('autoezgamew', 1190), ('autolose', 1191), ('autos', 1192), ('autowin', 1193), ('autsist', 1194), ('avatar', 1195), ('ave', 1196), ('avenge', 1197), ('average', 1198), ('avicii', 1199), ('avrg', 1200), ('aw', 1201), ('aw2', 1202), ('awaj', 1203), ('awal', 1204), ('aware', 1205), ('awarness', 1206), ('awaw', 1207), ('away', 1208), ('aways', 1209), ('awe', 1210), ('awefully', 1211), ('awer', 1212), ('awesome', 1213), ('awfuil', 1214), ('awful', 1215), ('awgmi13i12g', 1216), ('awhi', 1217), ('awhile', 1218), ('awit', 1219), ('awj', 1220), ('awkawkakwkwa', 1221), ('awkawkaw', 1222), ('awkward', 1223), ('awtsu', 1224), ('awtx', 1225), ('aww', 1226), ('awwh', 1227), ('awww', 1228), ('awwww', 1229), ('awwwww', 1230), ('ax', 1231), ('axaxa', 1232), ('axaxax', 1233), ('axaxaxa', 1234), ('axaxxa', 1235), ('axaxxaxa', 1236), ('axe', 1237), ('axxaxa', 1238), ('axxaxaxaaxaxax', 1239), ('ay', 1240), ('ayaw', 1241), ('aye', 1242), ('aylmao', 1243), ('ayubowan', 1244), ('ayuden', 1245), ('ayy', 1246), ('ayylmao', 1247), ('ayylotus', 1248), ('ayyyy', 1249), ('ayyyyy', 1250), ('azaza', 1251), ('azzzz', 1252), ('b', 1253), ('b4', 1254), ('b8', 1255), ('b9', 1256), ('ba', 1257), ('baaaaaag', 1258), ('baara', 1259), ('baban', 1260), ('babay', 1261), ('babe', 1262), ('babeeeeeeeeeeee', 1263), ('babes', 1264), ('babi', 1265), ('babies', 1266), ('babilo', 1267), ('baboons', 1268), ('baby', 1269), ('baby4', 1270), ('babysat', 1271), ('bacan', 1272), ('back', 1273), ('backd00r', 1274), ('backdoor', 1275), ('backfired', 1276), ('backing', 1277), ('backpack', 1278), ('backtrack', 1279), ('backtracks', 1280), ('backup', 1281), ('backwards', 1282), ('bacl', 1283), ('bacod', 1284), ('bad', 1285), ('badabing', 1286), ('badddddddddddddddddddd', 1287), ('badf', 1288), ('badluck', 1289), ('badly', 1290), ('bads', 1291), ('badtrri', 1292), ('bae', 1293), ('bag', 1294), ('bagay', 1295), ('baggg', 1296), ('bagging', 1297), ('bago', 1298), ('bags', 1299), ('bahaha', 1300), ('bahahahah', 1301), ('bai', 1302), ('baia', 1303), ('bain', 1304), ('bait', 1305), ('baited', 1306), ('baits', 1307), ('baitttttt', 1308), ('bajchs', 1309), ('bak', 1310), ('baka', 1311), ('bakbuki', 1312), ('bakc', 1313), ('bala', 1314), ('balalaika', 1315), ('balanar', 1316), ('balanced', 1317), ('balck', 1318), ('ball', 1319), ('balls', 1320), ('balnce', 1321), ('bam', 1322), ('ban', 1323), ('bandot', 1324), ('bane', 1325), ('baneeee', 1326), ('bang', 1327), ('banged', 1328), ('banging', 1329), ('bangsat', 1330), ('bank', 1331), ('banned', 1332), ('banter', 1333), ('bar', 1334), ('bara', 1335), ('baracks', 1336), ('barat', 1337), ('barbaric', 1338), ('barbie', 1339), ('barely', 1340), ('barelya', 1341), ('bark', 1342), ('barking', 1343), ('barracks', 1344), ('barrier', 1345), ('bart', 1346), ('bas', 1347), ('basdh', 1348), ('base', 1349), ('based', 1350), ('basement', 1351), ('baserace', 1352), ('bash', 1353), ('bashed', 1354), ('basher', 1355), ('bashes', 1356), ('bashing', 1357), ('basic', 1358), ('basically', 1359), ('basics', 1360), ('basiluss', 1361), ('basin', 1362), ('basket', 1363), ('basmati', 1364), ('bastard', 1365), ('bastards', 1366), ('bastarrds', 1367), ('basted', 1368), ('basterds', 1369), ('basura', 1370), ('basuras', 1371), ('bat', 1372), ('bate', 1373), ('bater', 1374), ('bath', 1375), ('bathroom', 1376), ('batrider', 1377), ('battle', 1378), ('battlefield', 1379), ('battlefury', 1380), ('batya', 1381), ('bauer', 1382), ('bay', 1383), ('bayu', 1384), ('bb', 1385), ('bbopes', 1386), ('bbs', 1387), ('bby', 1388), ('bc', 1389), ('bcak', 1390), ('bcause', 1391), ('bcos', 1392), ('bcoz', 1393), ('bcuz', 1394), ('bd', 1395), ('be', 1396), ('beack', 1397), ('beaner', 1398), ('bear', 1399), ('beast', 1400), ('beastmaster', 1401), ('beastmode', 1402), ('beat', 1403), ('beaten', 1404), ('beating', 1405), ('beautiful', 1406), ('because', 1407), ('beciae', 1408), ('become', 1409), ('becouse', 1410), ('becoz', 1411), ('becuase', 1412), ('becuse', 1413), ('bed', 1414), ('bedroom', 1415), ('bedzmozgliy', 1416), ('beeee', 1417), ('beeeeeee', 1418), ('beef', 1419), ('beeing', 1420), ('been', 1421), ('beer', 1422), ('beffore', 1423), ('befoire', 1424), ('before', 1425), ('beforehand', 1426), ('beg', 1427), ('begging', 1428), ('beggining', 1429), ('begi', 1430), ('begin', 1431), ('begining', 1432), ('beginning', 1433), ('begins', 1434), ('beh', 1435), ('behave', 1436), ('behind', 1437), ('being', 1438), ('beinga', 1439), ('bek', 1440), ('bel', 1441), ('beleive', 1442), ('believe', 1443), ('believes', 1444), ('belive', 1445), ('bell', 1446), ('belond', 1447), ('belong', 1448), ('belongs', 1449), ('belorussian', 1450), ('below', 1451), ('bem', 1452), ('ben', 1453), ('bending', 1454), ('beneath', 1455), ('bengap', 1456), ('benjaz', 1457), ('benla', 1458), ('benn', 1459), ('bes', 1460), ('beside', 1461), ('besides', 1462), ('best', 1463), ('bested', 1464), ('bestest', 1465), ('bet', 1466), ('betcha', 1467), ('betrayed', 1468), ('bets', 1469), ('better', 1470), ('betulan', 1471), ('between', 1472), ('beware', 1473), ('bey', 1474), ('beyond', 1475), ('bf', 1476), ('bfury', 1477), ('bfyrt', 1478), ('bg', 1479), ('bgt', 1480), ('bgtau', 1481), ('bh', 1482), ('bhai', 1483), ('bher', 1484), ('bhhot', 1485), ('bhitter', 1486), ('bhs', 1487), ('bi', 1488), ('biasa', 1489), ('biatch', 1490), ('biblethump', 1491), ('bich', 1492), ('bicht', 1493), ('bick', 1494), ('bicth', 1495), ('bieliebers', 1496), ('bien', 1497), ('bieng', 1498), ('big', 1499), ('bigbadbird', 1500), ('bigger', 1501), ('biggest', 1502), ('bigyan', 1503), ('bike', 1504), ('bility', 1505), ('bill', 1506), ('billing', 1507), ('bills', 1508), ('bings', 1509), ('binvoker', 1510), ('biomusor', 1511), ('biotch', 1512), ('biper', 1513), ('bird', 1514), ('birth', 1515), ('birthday', 1516), ('bisaya', 1517), ('biscuit', 1518), ('bish', 1519), ('bit', 1520), ('bitach', 1521), ('bitc', 1522), ('bitcch', 1523), ('bitch', 1524), ('bitchas', 1525), ('bitchass', 1526), ('bitched', 1527), ('bitcheeees', 1528), ('bitchery', 1529), ('bitches', 1530), ('bitchez', 1531), ('bitchh', 1532), ('bitchhhhhhh', 1533), ('bitching', 1534), ('bite', 1535), ('bited', 1536), ('bithc', 1537), ('bithces', 1538), ('bitter', 1539), ('bitvh', 1540), ('bivaet', 1541), ('bj', 1542), ('bk', 1543), ('bkb', 1544), ('bkbk', 1545), ('bkbless', 1546), ('bkbs', 1547), ('bl', 1548), ('bla', 1549), ('blabla', 1550), ('blablabla', 1551), ('blachhole', 1552), ('black', 1553), ('blackhole', 1554), ('blackout', 1555), ('blackput', 1556), ('blaco', 1557), ('blade', 1558), ('blademail', 1559), ('blademaiol', 1560), ('bladmail', 1561), ('blakc', 1562), ('blame', 1563), ('blamed', 1564), ('blamee', 1565), ('blameee', 1566), ('blamer', 1567), ('blames', 1568), ('blaming', 1569), ('blamming', 1570), ('blanced', 1571), ('blank', 1572), ('blasphemy', 1573), ('blasts', 1574), ('blat', 1575), ('bleat', 1576), ('bledos', 1577), ('blem', 1578), ('bleme', 1579), ('bless', 1580), ('blia', 1581), ('blind', 1582), ('blink', 1583), ('blinked', 1584), ('blinking', 1585), ('blinks', 1586), ('block', 1587), ('blocked', 1588), ('blockers', 1589), ('blodd', 1590), ('blog', 1591), ('blood', 1592), ('bloodseeker', 1593), ('bloody', 1594), ('blow', 1595), ('blowed', 1596), ('blowjob', 1597), ('blrr', 1598), ('blue', 1599), ('bluff', 1600), ('bluffing', 1601), ('blur', 1602), ('blushes', 1603), ('blya', 1604), ('blyaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaat', 1605), ('blyadi', 1606), ('blyat', 1607), ('blyj', 1608), ('blyuat', 1609), ('bm', 1610), ('bmw', 1611), ('bn', 1612), ('bnooob', 1613), ('bnro', 1614), ('bnut', 1615), ('bnyak', 1616), ('bo', 1617), ('board', 1618), ('boat', 1619), ('bobbbbbbbbb', 1620), ('bobby', 1621), ('bobo', 1622), ('bobob', 1623), ('bobong', 1624), ('bodied', 1625), ('body', 1626), ('bodyguard', 1627), ('bog', 1628), ('boga', 1629), ('bogo', 1630), ('boi', 1631), ('boid', 1632), ('boii', 1633), ('bois', 1634), ('boiz', 1635), ('bolas', 1636), ('bolder', 1637), ('boludo', 1638), ('bom', 1639), ('bomb', 1640), ('bombaghostdota', 1641), ('bomber', 1642), ('bombs', 1643), ('bomji', 1644), ('bommmm', 1645), ('bone', 1646), ('boner', 1647), ('bonert', 1648), ('bonus', 1649), ('boob', 1650), ('boobo', 1651), ('boobs', 1652), ('boohoo', 1653), ('booii', 1654), ('book', 1655), ('boom', 1656), ('booo', 1657), ('booom', 1658), ('boooo', 1659), ('boooom', 1660), ('booooooooooooooom', 1661), ('boost', 1662), ('booste', 1663), ('boosted', 1664), ('booster', 1665), ('boosting', 1666), ('boot', 1667), ('bootcamp', 1668), ('bootcamping', 1669), ('boots', 1670), ('booty', 1671), ('booya', 1672), ('bor', 1673), ('borderline', 1674), ('bored', 1675), ('boring', 1676), ('boris', 1677), ('born', 1678), ('bos', 1679), ('boshit', 1680), ('boss', 1681), ('bosting', 1682), ('bosy', 1683), ('bot', 1684), ('bota', 1685), ('botbo9t', 1686), ('botbot', 1687), ('both', 1688), ('bother', 1689), ('bothered', 1690), ('bothering', 1691), ('bothers', 1692), ('bothrunes', 1693), ('botlane', 1694), ('botle', 1695), ('botm', 1696), ('botoom', 1697), ('bototm', 1698), ('bots', 1699), ('bottle', 1700), ('bottles', 1701), ('bottom', 1702), ('boty', 1703), ('botz', 1704), ('bought', 1705), ('bouhgt', 1706), ('bounce', 1707), ('bounces', 1708), ('bounty', 1709), ('bout', 1710), ('boutt', 1711), ('bow', 1712), ('boy', 1713), ('boyfriend', 1714), ('boyos', 1715), ('boys', 1716), ('boyss', 1717), ('boyssssss', 1718), ('boysssssssssssssss', 1719), ('boyus', 1720), ('boyz', 1721), ('boyzz', 1722), ('bp', 1723), ('br', 1724), ('bra', 1725), ('braaaaaaaaaaaa', 1726), ('bracket', 1727), ('brackins', 1728), ('brag', 1729), ('brah', 1730), ('brahs', 1731), ('brai', 1732), ('braidnead', 1733), ('brain', 1734), ('braindamaged', 1735), ('braindead', 1736), ('braindeadm', 1737), ('brained', 1738), ('brainless', 1739), ('brainly', 1740), ('brains', 1741), ('braker', 1742), ('brakes', 1743), ('branch', 1744), ('brat', 1745), ('bratan', 1746), ('bratans', 1747), ('brave', 1748), ('bravo', 1749), ('braz1l', 1750), ('brazzers', 1751), ('brb', 1752), ('break', 1753), ('breaker', 1754), ('breakes', 1755), ('breaks', 1756), ('breathe', 1757), ('breathing', 1758), ('breed', 1759), ('breez', 1760), ('breeze', 1761), ('breezy', 1762), ('breezzy', 1763), ('brew', 1764), ('brewmaster', 1765), ('brezzy', 1766), ('brian', 1767), ('briezi', 1768), ('brigade', 1769), ('bright', 1770), ('brilliant', 1771), ('bring', 1772), ('bringing', 1773), ('brings', 1774), ('brist', 1775), ('briste', 1776), ('bristle', 1777), ('bristleback', 1778), ('bro', 1779), ('broadswor', 1780), ('brobounty', 1781), ('broi', 1782), ('broke', 1783), ('broken', 1784), ('brokenn', 1785), ('brood', 1786), ('bros', 1787), ('broser', 1788), ('brosky', 1789), ('brotha', 1790), ('brother', 1791), ('brought', 1792), ('brounty', 1793), ('brown', 1794), ('browser', 1795), ('brraainnss', 1796), ('brraiinnss', 1797), ('bruh', 1798), ('bruhh', 1799), ('bruhhh', 1800), ('brushan', 1801), ('brutal', 1802), ('bruto', 1803), ('brutos', 1804), ('bruuuhh', 1805), ('bruv', 1806), ('bruz', 1807), ('bs', 1808), ('bset', 1809), ('bst', 1810), ('btf', 1811), ('btich', 1812), ('btm', 1813), ('btmm', 1814), ('btr', 1815), ('btu', 1816), ('btw', 1817), ('bu', 1818), ('buback', 1819), ('bubulok', 1820), ('bucks', 1821), ('bucther', 1822), ('bud', 1823), ('buddy', 1824), ('buds', 1825), ('buff', 1826), ('buffalo', 1827), ('bug', 1828), ('bugging', 1829), ('bugok', 1830), ('buhat', 1831), ('build', 1832), ('builddings', 1833), ('building', 1834), ('builds', 1835), ('built', 1836), ('bukake', 1837), ('bulb', 1838), ('buld', 1839), ('buldog', 1840), ('bulgarian', 1841), ('bull', 1842), ('bulldog', 1843), ('bullied', 1844), ('bullshit', 1845), ('bullshitting', 1846), ('bullshjit', 1847), ('bully', 1848), ('bulshit', 1849), ('bulsshit', 1850), ('buly', 1851), ('bum', 1852), ('bumhole', 1853), ('bummer', 1854), ('bunch', 1855), ('buns', 1856), ('buong', 1857), ('burden', 1858), ('burdenb', 1859), ('burdeness', 1860), ('burdens', 1861), ('burn', 1862), ('burning', 1863), ('burns', 1864), ('burrito', 1865), ('burro', 1866), ('burros', 1867), ('burrow', 1868), ('bush', 1869), ('businees', 1870), ('business', 1871), ('businessman', 1872), ('busy', 1873), ('but', 1874), ('buter', 1875), ('butt', 1876), ('butterfl', 1877), ('butterfly', 1878), ('butters', 1879), ('butthurt', 1880), ('butto', 1881), ('button', 1882), ('butu', 1883), ('butuh', 1884), ('buy', 1885), ('buy1', 1886), ('buyabakc', 1887), ('buyback', 1888), ('buybacks', 1889), ('buyer', 1890), ('buyers', 1891), ('buying', 1892), ('buys', 1893), ('bvbr', 1894), ('bvlast', 1895), ('bvoid', 1896), ('bwahahha', 1897), ('bwersit', 1898), ('bx', 1899), ('by', 1900), ('bye', 1901), ('bye2x', 1902), ('byebek', 1903), ('byebye', 1904), ('byee', 1905), ('bying', 1906), ('byr', 1907), ('bz', 1908), ('c', 1909), ('c0meback', 1910), ('ca', 1911), ('caarry', 1912), ('cabros', 1913), ('cac', 1914), ('caca', 1915), ('cactuses', 1916), ('cada', 1917), ('cafe', 1918), ('caga', 1919), ('cagada', 1920), ('cago', 1921), ('cahging', 1922), ('cai', 1923), ('cake', 1924), ('cal', 1925), ('calculate', 1926), ('calculated', 1927), ('calculator', 1928), ('california', 1929), ('call', 1930), ('calla', 1931), ('callada', 1932), ('callate', 1933), ('called', 1934), ('calling', 1935), ('calls', 1936), ('calm', 1937), ('calms', 1938), ('came', 1939), ('camp', 1940), ('campaign', 1941), ('camped', 1942), ('camping', 1943), ('camps', 1944), ('can', 1945), ('canadians', 1946), ('cancel', 1947), ('cancer', 1948), ('cancerous', 1949), ('cancers', 1950), ('candy', 1951), ('cangkeman', 1952), ('cannot', 1953), ('canoot', 1954), ('cans', 1955), ('cant', 1956), ('canyou', 1957), ('cao', 1958), ('cap', 1959), ('cape', 1960), ('capital', 1961), ('caps', 1962), ('capt', 1963), ('captain', 1964), ('car', 1965), ('cara', 1966), ('carai', 1967), ('carajo', 1968), ('care', 1969), ('careful', 1970), ('cares', 1971), ('caried', 1972), ('carl', 1973), ('carri', 1974), ('carried', 1975), ('carries', 1976), ('carrt', 1977), ('carrubick', 1978), ('carry', 1979), ('carryin', 1980), ('carrying', 1981), ('carrys', 1982), ('cary', 1983), ('caryr', 1984), ('caryy', 1985), ('cas', 1986), ('case', 1987), ('casi', 1988), ('cask', 1989), ('cast', 1990), ('casters', 1991), ('casting', 1992), ('casual', 1993), ('cat', 1994), ('catapult', 1995), ('catapults', 1996), ('catch', 1997), ('catcher', 1998), ('catches', 1999), ('catchy', 2000), ('category', 2001), ('cath', 2002), ('cats', 2003), ('caucasian', 2004), ('caught', 2005), ('cause', 2006), ('causer', 2007), ('causing', 2008), ('cavada', 2009), ('cave', 2010), ('caya', 2011), ('cb', 2012), ('cc', 2013), ('ccc', 2014), ('ccntt', 2015), ('ccurious', 2016), ('cd', 2017), ('cdec', 2018), ('ce4rf', 2019), ('ceasepool', 2020), ('ceba', 2021), ('ceci', 2022), ('ceeeeeeeeeeeeeeeeeeeena', 2023), ('celebrate', 2024), ('cen', 2025), ('cena', 2026), ('cenaaaaaa', 2027), ('cenaaaaaaaaaaaaa', 2028), ('cent', 2029), ('centaur', 2030), ('centaurs', 2031), ('century', 2032), ('cepet2', 2033), ('cerape', 2034), ('cerebro', 2035), ('certainly', 2036), ('certainty', 2037), ('cf', 2038), ('cfg', 2039), ('cghfibdftncz', 2040), ('ch', 2041), ('chaeter', 2042), ('chains', 2043), ('chair', 2044), ('challange', 2045), ('challenge', 2046), ('champ', 2047), ('champion', 2048), ('chance', 2049), ('chances', 2050), ('chang', 2051), ('change', 2052), ('changed', 2053), ('chanses', 2054), ('chaos', 2055), ('chapstick', 2056), ('charge', 2057), ('charger', 2058), ('charges', 2059), ('charhing', 2060), ('chase', 2061), ('chased', 2062), ('chaserr', 2063), ('chasinbg', 2064), ('chasing', 2065), ('chat', 2066), ('chating', 2067), ('chats', 2068), ('chatting', 2069), ('chatwheel', 2070), ('chavstick', 2071), ('chcua', 2072), ('che', 2073), ('cheaper', 2074), ('cheat', 2075), ('cheaters', 2076), ('check', 2077), ('checked', 2078), ('cheeckt', 2079), ('cheecky', 2080), ('cheeeers', 2081), ('cheeky', 2082), ('cheekyness', 2083), ('cheers', 2084), ('cheese', 2085), ('chegamos', 2086), ('cheisnese', 2087), ('chemical', 2088), ('chemistry', 2089), ('chen', 2090), ('chepo', 2091), ('chers', 2092), ('chese', 2093), ('chest', 2094), ('chi', 2095), ('chick', 2096), ('chicken', 2097), ('chickenon', 2098), ('chickens', 2099), ('chicks', 2100), ('chiil', 2101), ('child', 2102), ('childhood', 2103), ('children', 2104), ('chill', 2105), ('chilled', 2106), ('chilling', 2107), ('chillyou', 2108), ('chimp', 2109), ('china', 2110), ('chinese', 2111), ('ching', 2112), ('chink', 2113), ('chinks', 2114), ('chipprel', 2115), ('chistris', 2116), ('chloride', 2117), ('chmo', 2118), ('cho', 2119), ('chode', 2120), ('choice', 2121), ('choise', 2122), ('chong', 2123), ('chongs', 2124), ('chop', 2125), ('chops', 2126), ('chorno', 2127), ('christ', 2128), ('christmas', 2129), ('christopher', 2130), ('chrnoo', 2131), ('chron', 2132), ('chronio', 2133), ('chrono', 2134), ('chto', 2135), ('chtobi', 2136), ('chtobu', 2137), ('chuchamadre', 2138), ('chuck', 2139), ('chupa', 2140), ('chupadmela', 2141), ('church', 2142), ('cigarettes', 2143), ('cimputer', 2144), ('circus', 2145), ('cis', 2146), ('cj', 2147), ('ck', 2148), ('ckckck', 2149), ('ckeckek', 2150), ('ckise', 2151), ('cl', 2152), ('claim', 2153), ('claimed', 2154), ('claims', 2155), ('clap', 2156), ('clapclap', 2157), ('clarity', 2158), ('clark', 2159), ('clash', 2160), ('clashing', 2161), ('classes', 2162), ('classic', 2163), ('classy', 2164), ('clcockwerk', 2165), ('clean', 2166), ('clear', 2167), ('clearly', 2168), ('cleavbe', 2169), ('cleave', 2170), ('clement', 2171), ('clever', 2172), ('cli', 2173), ('click', 2174), ('clicked', 2175), ('clickers', 2176), ('clicking', 2177), ('client', 2178), ('cliff', 2179), ('cliffs', 2180), ('climb', 2181), ('clinic', 2182), ('clink', 2183), ('clinks', 2184), ('clinkz', 2185), ('clinkzz', 2186), ('clinton', 2187), ('cloack', 2188), ('cloak', 2189), ('clock', 2190), ('clocks', 2191), ('clockwerk', 2192), ('clockwork', 2193), ('clok', 2194), ('clokc', 2195), ('close', 2196), ('closed', 2197), ('closers', 2198), ('closes', 2199), ('closest', 2200), ('clothes', 2201), ('clown', 2202), ('clowns', 2203), ('clowny', 2204), ('clq', 2205), ('club', 2206), ('clue', 2207), ('clueless', 2208), ('clutch', 2209), ('cm', 2210), ('cmap', 2211), ('cmd', 2212), ('cmere', 2213), ('cmnd', 2214), ('cmon', 2215), ('cmtr', 2216), ('cna', 2217), ('cnacer', 2218), ('cnacnceled', 2219), ('cnosite', 2220), ('cnt', 2221), ('co', 2222), ('coaching', 2223), ('coal', 2224), ('coast', 2225), ('cocck', 2226), ('cochino', 2227), ('cock', 2228), ('cocks', 2229), ('cocksuck', 2230), ('cocksucker', 2231), ('cockwork', 2232), ('cocky', 2233), ('coco', 2234), ('code', 2235), ('coel', 2236), ('coem', 2237), ('coemd', 2238), ('coffee', 2239), ('coiier', 2240), ('coin', 2241), ('coinin', 2242), ('coins', 2243), ('coinz', 2244), ('coiuld', 2245), ('cok', 2246), ('cokking', 2247), ('cold', 2248), ('coldsnap', 2249), ('colour', 2250), ('com', 2251), ('comback', 2252), ('combackl', 2253), ('combak', 2254), ('combine', 2255), ('combined', 2256), ('combo', 2257), ('comdy', 2258), ('come', 2259), ('comeback', 2260), ('comebak', 2261), ('comeeeeee', 2262), ('comemnd', 2263), ('comend', 2264), ('comendded', 2265), ('comenden', 2266), ('coment', 2267), ('comento', 2268), ('comeo', 2269), ('comeon', 2270), ('comes', 2271), ('comew', 2272), ('comfort', 2273), ('comin', 2274), ('coming', 2275), ('comited', 2276), ('comke', 2277), ('comm', 2278), ('command', 2279), ('commdnd', 2280), ('comme', 2281), ('commead', 2282), ('commed', 2283), ('commedn', 2284), ('commemd', 2285), ('commen', 2286), ('commend', 2287), ('commend2', 2288), ('commendable', 2289), ('commendation', 2290), ('commende', 2291), ('commended', 2292), ('commending', 2293), ('commendos', 2294), ('commends', 2295), ('commened', 2296), ('comment', 2297), ('commerd', 2298), ('commin', 2299), ('comming', 2300), ('commit', 2301), ('commited', 2302), ('commment', 2303), ('commmmmmmmmmmmmmmmmed', 2304), ('commned', 2305), ('common', 2306), ('commonwealth', 2307), ('communication', 2308), ('comn', 2309), ('comnig', 2310), ('como', 2311), ('comon', 2312), ('comp', 2313), ('compared', 2314), ('comparing', 2315), ('compassion', 2316), ('compatriotas', 2317), ('compedium', 2318), ('compendium', 2319), ('competent', 2320), ('competitive', 2321), ('competnent', 2322), ('complain', 2323), ('complaining', 2324), ('complains', 2325), ('complaning', 2326), ('completa', 2327), ('complete', 2328), ('completely', 2329), ('complex', 2330), ('compliment', 2331), ('compo', 2332), ('component', 2333), ('comprar', 2334), ('comprende', 2335), ('comps', 2336), ('computer', 2337), ('computers', 2338), ('comr', 2339), ('comunity', 2340), ('comunnication', 2341), ('con', 2342), ('concedes', 2343), ('concernedstudent1950', 2344), ('conchetumare', 2345), ('condom', 2346), ('conecting', 2347), ('confidence', 2348), ('confident', 2349), ('confirm', 2350), ('confirmed', 2351), ('confsued', 2352), ('confused', 2353), ('congrats', 2354), ('congratulations', 2355), ('conneccting', 2356), ('connect', 2357), ('connecting', 2358), ('connectio', 2359), ('connection', 2360), ('conred', 2361), ('consecutive', 2362), ('considered', 2363), ('considering', 2364), ('constantly', 2365), ('continue', 2366), ('contribution', 2367), ('contrita', 2368), ('control', 2369), ('converted', 2370), ('coo', 2371), ('cookie', 2372), ('cooking', 2373), ('cookkk', 2374), ('cool', 2375), ('coold', 2376), ('cooldown', 2377), ('cooldowns', 2378), ('coomend', 2379), ('coommend', 2380), ('cooooooomeeeend', 2381), ('cooperate', 2382), ('coordinator', 2383), ('cooties', 2384), ('cop', 2385), ('copo', 2386), ('copter', 2387), ('copy', 2388), ('copyright', 2389), ('core', 2390), ('cores', 2391), ('corno', 2392), ('corpse', 2393), ('correct', 2394), ('cos', 2395), ('cose', 2396), ('cosmetic', 2397), ('cosplay', 2398), ('cost', 2399), ('costanza', 2400), ('cotl', 2401), ('cotnra', 2402), ('coudlve', 2403), ('cough', 2404), ('couild', 2405), ('could', 2406), ('coulda', 2407), ('couldnt', 2408), ('count', 2409), ('counted', 2410), ('counter', 2411), ('countered', 2412), ('counterpick', 2413), ('counterpicker', 2414), ('counters', 2415), ('counterthrow', 2416), ('countless', 2417), ('country', 2418), ('counts', 2419), ('couple', 2420), ('cour', 2421), ('coureirs', 2422), ('couri', 2423), ('courier', 2424), ('couriers', 2425), ('courriers', 2426), ('cours', 2427), ('course', 2428), ('court', 2429), ('couse', 2430), ('coutnerpicked', 2431), ('coutnerpickers', 2432), ('couz', 2433), ('cover', 2434), ('covered', 2435), ('cow', 2436), ('coward', 2437), ('cows', 2438), ('coz', 2439), ('cozuld', 2440), ('cpcugu', 2441), ('cpt', 2442), ('cpu', 2443), ('cracked', 2444), ('crackwhores', 2445), ('crafty', 2446), ('craggies', 2447), ('craggt', 2448), ('craggy', 2449), ('cram', 2450), ('craos', 2451), ('crap', 2452), ('craps', 2453), ('craptastic', 2454), ('crash', 2455), ('crashed', 2456), ('crashing', 2457), ('crazy', 2458), ('creamy', 2459), ('creap', 2460), ('creaps', 2461), ('creapwave', 2462), ('create', 2463), ('created', 2464), ('creater', 2465), ('creator', 2466), ('creatyre', 2467), ('creazy', 2468), ('credit', 2469), ('credits', 2470), ('cree', 2471), ('creeeeeeeps', 2472), ('creeep', 2473), ('creeepy', 2474), ('creep', 2475), ('creepo', 2476), ('creeps', 2477), ('creepz', 2478), ('creo', 2479), ('crep', 2480), ('crereps', 2481), ('crfat', 2482), ('crhist', 2483), ('cri', 2484), ('cried', 2485), ('crimson', 2486), ('crispy', 2487), ('cristalis', 2488), ('crit', 2489), ('crita', 2490), ('crited', 2491), ('critical', 2492), ('criticism', 2493), ('crits', 2494), ('crom', 2495), ('crono', 2496), ('cross', 2497), ('crow', 2498), ('crrrrunch', 2499), ('crtm', 2500), ('cruel', 2501), ('crumbles', 2502), ('crush', 2503), ('cry', 2504), ('crybabies', 2505), ('crying', 2506), ('crys', 2507), ('crystal', 2508), ('cryts', 2509), ('cryyy', 2510), ('cs', 2511), ('cse', 2512), ('csing', 2513), ('csmr', 2514), ('csw', 2515), ('ctm', 2516), ('ctmr', 2517), ('ctr', 2518), ('ctrl', 2519), ('cuck', 2520), ('cucks', 2521), ('cucumbertube', 2522), ('cud', 2523), ('cuello', 2524), ('cuh', 2525), ('cuint', 2526), ('cuirass', 2527), ('culd', 2528), ('culda', 2529), ('culling', 2530), ('culpa', 2531), ('cult', 2532), ('cum', 2533), ('cumb', 2534), ('cumback', 2535), ('cumbacket', 2536), ('cumbag', 2537), ('cumbak', 2538), ('cumed', 2539), ('cuming', 2540), ('cummin', 2541), ('cumming', 2542), ('cumwaste', 2543), ('cung', 2544), ('cunt', 2545), ('cuntka', 2546), ('cunts', 2547), ('cuntss', 2548), ('cunty', 2549), ('cuoi', 2550), ('cuppa', 2551), ('cupu', 2552), ('cur', 2553), ('curier', 2554), ('current', 2555), ('currier', 2556), ('curse', 2557), ('cus', 2558), ('cut', 2559), ('cute', 2560), ('cutie', 2561), ('cuz', 2562), ('cw', 2563), ('cya', 2564), ('cyak', 2565), ('cybercafe', 2566), ('cyclone', 2567), ('cyka', 2568), ('cykaseeker', 2569), ('cyou', 2570), ('cz', 2571), ('d', 2572), ('d0', 2573), ('d2', 2574), ('da', 2575), ('daaaaaaa', 2576), ('daaaamn', 2577), ('daaamn', 2578), ('daaazehl', 2579), ('daamn', 2580), ('dad', 2581), ('dada', 2582), ('dadadada', 2583), ('dadadqa', 2584), ('daddy', 2585), ('daddys', 2586), ('dads', 2587), ('dady', 2588), ('daedalus', 2589), ('dafu', 2590), ('dafuq', 2591), ('dag', 2592), ('daga', 2593), ('dager', 2594), ('dagg', 2595), ('dagger', 2596), ('dagon', 2597), ('daj', 2598), ('dallae', 2599), ('dallli', 2600), ('dam', 2601), ('damage', 2602), ('damaged', 2603), ('dame', 2604), ('damge', 2605), ('dami', 2606), ('damimit', 2607), ('damm', 2608), ('dammit', 2609), ('dammn', 2610), ('damn', 2611), ('damned', 2612), ('damo', 2613), ('dan', 2614), ('dance', 2615), ('dancing', 2616), ('dandom', 2617), ('dang', 2618), ('danger', 2619), ('danh', 2620), ('dank', 2621), ('danny', 2622), ('dansgame', 2623), ('dapto', 2624), ('daqui', 2625), ('dar', 2626), ('dare', 2627), ('dark', 2628), ('darling', 2629), ('darme', 2630), ('das', 2631), ('dasjklpfsegscg', 2632), ('dat', 2633), ('date', 2634), ('dats', 2635), ('daun', 2636), ('dauni', 2637), ('daunov', 2638), ('davai', 2639), ('davai2', 2640), ('davay', 2641), ('dawg', 2642), ('dawn', 2643), ('day', 2644), ('dayn', 2645), ('dayna', 2646), ('daynov', 2647), ('days', 2648), ('dayt', 2649), ('dayum', 2650), ('dayuuuuuum', 2651), ('dayuuuuuuuuuuuuuuuuuuuuuuuuum', 2652), ('dayyyuum', 2653), ('daz', 2654), ('dazel', 2655), ('dazl', 2656), ('dazle', 2657), ('dazz', 2658), ('dazzle', 2659), ('dc', 2660), ('dced', 2661), ('dces', 2662), ('dcm', 2663), ('dcp', 2664), ('dcs', 2665), ('dd', 2666), ('ddd', 2667), ('dddd', 2668), ('dddddd', 2669), ('dde', 2670), ('ddint', 2671), ('ddiobnt', 2672), ('ddos', 2673), ('ddosi', 2674), ('ddoss', 2675), ('ddosser', 2676), ('ddue', 2677), ('de', 2678), ('dead', 2679), ('deads', 2680), ('deal', 2681), ('dealer', 2682), ('dealing', 2683), ('dean', 2684), ('dear', 2685), ('death', 2686), ('deaths', 2687), ('debili', 2688), ('debuff', 2689), ('decay', 2690), ('decent', 2691), ('decide', 2692), ('decided', 2693), ('decision', 2694), ('decline', 2695), ('deco', 2696), ('ded', 2697), ('dedication', 2698), ('dedz', 2699), ('dee', 2700), ('deed', 2701), ('deee', 2702), ('deef', 2703), ('deek', 2704), ('deep', 2705), ('deez', 2706), ('def', 2707), ('defance', 2708), ('defeat', 2709), ('defeding', 2710), ('defence', 2711), ('defend', 2712), ('defende', 2713), ('defendere', 2714), ('defending', 2715), ('defenive', 2716), ('defense', 2717), ('deff', 2718), ('deffffff', 2719), ('deffiing', 2720), ('deffing', 2721), ('deffs', 2722), ('defiance', 2723), ('defim', 2724), ('defing', 2725), ('definitelly', 2726), ('definitely', 2727), ('definitly', 2728), ('defnd', 2729), ('degancia', 2730), ('dei', 2731), ('deine', 2732), ('deinfinately', 2733), ('dejen', 2734), ('dela', 2735), ('delate', 2736), ('delay', 2737), ('delayed', 2738), ('dele', 2739), ('delet', 2740), ('delete', 2741), ('deleted', 2742), ('delivers', 2743), ('delte', 2744), ('deluxe', 2745), ('dem', 2746), ('demage', 2747), ('demand', 2748), ('demolished', 2749), ('demon', 2750), ('demons', 2751), ('demoralize', 2752), ('demorar', 2753), ('den', 2754), ('dende', 2755), ('dendi', 2756), ('dendiface', 2757), ('dendo', 2758), ('deni', 2759), ('denie', 2760), ('denied', 2761), ('denies', 2762), ('deny', 2763), ('denyyyyyyyyyyyy', 2764), ('deon', 2765), ('department', 2766), ('depends', 2767), ('depot', 2768), ('depression', 2769), ('deputa', 2770), ('der', 2771), ('deram', 2772), ('deranged', 2773), ('derma', 2774), ('des', 2775), ('desable', 2776), ('describe', 2777), ('desd', 2778), ('desease', 2779), ('deserv', 2780), ('deserve', 2781), ('deserved', 2782), ('deserver', 2783), ('deserves', 2784), ('desesperado', 2785), ('designed', 2786), ('desisti', 2787), ('desk', 2788), ('deso', 2789), ('desolator', 2790), ('desperate', 2791), ('desperationj', 2792), ('despite', 2793), ('dessa', 2794), ('destoy', 2795), ('destroy', 2796), ('destroyed', 2797), ('det', 2798), ('detected', 2799), ('detection', 2800), ('deti', 2801), ('deus', 2802), ('devil', 2803), ('devoting', 2804), ('deward', 2805), ('dewarding', 2806), ('df', 2807), ('dfc', 2808), ('dfsgahzwreas', 2809), ('dfvb', 2810), ('dh', 2811), ('di', 2812), ('diam', 2813), ('diarrea', 2814), ('diccks', 2815), ('dick', 2816), ('dickhead', 2817), ('dickheads', 2818), ('dickriding', 2819), ('dicks', 2820), ('diclk', 2821), ('dictates', 2822), ('did', 2823), ('diddnt', 2824), ('dide', 2825), ('dident', 2826), ('didjnt', 2827), ('didn', 2828), ('didnr', 2829), ('didnt', 2830), ('didntt', 2831), ('didt', 2832), ('die', 2833), ('dieback', 2834), ('diebacks', 2835), ('died', 2836), ('dieddddd', 2837), ('dieee', 2838), ('dieofcancer', 2839), ('dies', 2840), ('diferent', 2841), ('diff', 2842), ('difference', 2843), ('different', 2844), ('diffferent', 2845), ('difficuklt', 2846), ('difficult', 2847), ('diffu', 2848), ('diffusal', 2849), ('difusal', 2850), ('difussalk', 2851), ('diga', 2852), ('digusting', 2853), ('diiiickkkk', 2854), ('diiot', 2855), ('dijo', 2856), ('dildo', 2857), ('dillon', 2858), ('din', 2859), ('dingue', 2860), ('dint', 2861), ('dip', 2862), ('dir', 2863), ('dira', 2864), ('dire', 2865), ('direct', 2866), ('directly', 2867), ('direto', 2868), ('dirge', 2869), ('dirt', 2870), ('dirty', 2871), ('dis', 2872), ('disable', 2873), ('disappearing', 2874), ('disastah', 2875), ('disaster', 2876), ('disco', 2877), ('disconect', 2878), ('disconected', 2879), ('disconnect', 2880), ('disconnected', 2881), ('disconnecting', 2882), ('disconnects', 2883), ('disconnnecting', 2884), ('discoonect', 2885), ('discorvered', 2886), ('discovered', 2887), ('discuss', 2888), ('discussing', 2889), ('disease', 2890), ('disgrace', 2891), ('disgusting', 2892), ('disonect', 2893), ('dispersion', 2894), ('disposal', 2895), ('disr', 2896), ('disra', 2897), ('disraprot', 2898), ('disro', 2899), ('disrup', 2900), ('disrupter', 2901), ('disruptor', 2902), ('dist', 2903), ('distance', 2904), ('distract', 2905), ('disturb', 2906), ('ditch', 2907), ('ditched', 2908), ('ditto', 2909), ('diurrrr', 2910), ('divck', 2911), ('dive', 2912), ('divers', 2913), ('diversion', 2914), ('divertito', 2915), ('divided', 2916), ('divin', 2917), ('divina', 2918), ('divine', 2919), ('diving', 2920), ('diwali', 2921), ('diz', 2922), ('dizaztah', 2923), ('dizazterr', 2924), ('djon', 2925), ('dk', 2926), ('dkajwd', 2927), ('dl', 2928), ('dm', 2929), ('dmage', 2930), ('dmg', 2931), ('dmge', 2932), ('dmid', 2933), ('dmkm', 2934), ('dn', 2935), ('dneis', 2936), ('dnno', 2937), ('dno', 2938), ('dnt', 2939), ('do', 2940), ('dochu', 2941), ('docto', 2942), ('doctor', 2943), ('doctors', 2944), ('dodge', 2945), ('dodging', 2946), ('doe', 2947), ('doeee', 2948), ('doens', 2949), ('doenst', 2950), ('does', 2951), ('doesn', 2952), ('doesnt', 2953), ('doestn', 2954), ('dog', 2955), ('doge', 2956), ('dogeee', 2957), ('dogged', 2958), ('dogggg', 2959), ('dogs', 2960), ('dogshit', 2961), ('dogshits', 2962), ('doh', 2963), ('doign', 2964), ('doin', 2965), ('doing', 2966), ('doint', 2967), ('doita', 2968), ('doiung', 2969), ('dollar', 2970), ('dollars', 2971), ('doller', 2972), ('doma', 2973), ('dombshit', 2974), ('domestics', 2975), ('dominator', 2976), ('domu', 2977), ('don', 2978), ('donald', 2979), ('donate', 2980), ('donde', 2981), ('dondo', 2982), ('done', 2983), ('donee', 2984), ('dong', 2985), ('donger', 2986), ('donkey', 2987), ('donno', 2988), ('donnt', 2989), ('donp', 2990), ('dont', 2991), ('dontg', 2992), ('dony', 2993), ('doo', 2994), ('doods', 2995), ('dooes', 2996), ('doom', 2997), ('doomed', 2998), ('doomn', 2999), ('dooms', 3000), ('doono', 3001), ('doooo', 3002), ('doooom', 3003), ('dooooom', 3004), ('dooooooooom', 3005), ('door', 3006), ('doot', 3007), ('dop', 3008), ('dorito', 3009), ('dormer', 3010), ('dos', 3011), ('dosnt', 3012), ('dot', 3013), ('dota', 3014), ('dota2', 3015), ('dota2smut', 3016), ('dotabuff', 3017), ('dotacinema', 3018), ('dotaing', 3019), ('dotka', 3020), ('dotn', 3021), ('doto', 3022), ('dotra', 3023), ('dots', 3024), ('dotta', 3025), ('double', 3026), ('doubt', 3027), ('douche', 3028), ('dousnt', 3029), ('dove', 3030), ('down', 3031), ('downer', 3032), ('downies', 3033), ('download', 3034), ('downs', 3035), ('downschild', 3036), ('downside', 3037), ('downtown', 3038), ('dp', 3039), ('dpmne', 3040), ('dps', 3041), ('dr', 3042), ('draft', 3043), ('drag', 3044), ('dragged', 3045), ('dragging', 3046), ('dragon', 3047), ('dragons', 3048), ('drain', 3049), ('dramatic', 3050), ('draon', 3051), ('draw', 3052), ('drawing', 3053), ('dream', 3054), ('dreaming', 3055), ('dreams', 3056), ('dreamteam', 3057), ('dress', 3058), ('drgaon', 3059), ('drian', 3060), ('dribble', 3061), ('drink', 3062), ('drinking', 3063), ('driver', 3064), ('dro4it', 3065), ('drop', 3066), ('droped', 3067), ('dropped', 3068), ('dropping', 3069), ('drops', 3070), ('drow', 3071), ('drowwwwwww', 3072), ('drug', 3073), ('drugogo', 3074), ('drugs', 3075), ('druid', 3076), ('drum', 3077), ('drums', 3078), ('drunk', 3079), ('drying', 3080), ('ds', 3081), ('dsahdaslkdhwuroeyri32212', 3082), ('dslmdlas', 3083), ('dsnt', 3084), ('dst', 3085), ('dts', 3086), ('du', 3087), ('dual', 3088), ('dualed', 3089), ('duallane', 3090), ('dualllllllllllllllllllllllllllllllllllllllllllll', 3091), ('duble', 3092), ('dude', 3093), ('duded', 3094), ('dudes', 3095), ('due', 3096), ('dued', 3097), ('duel', 3098), ('duelos', 3099), ('duels', 3100), ('duken', 3101), ('dumb', 3102), ('dumbass', 3103), ('dumbcunt', 3104), ('dumber', 3105), ('dumbest', 3106), ('dumbfuck', 3107), ('dumbs', 3108), ('dumbshit', 3109), ('dumbster', 3110), ('dumbuck', 3111), ('dump', 3112), ('dumpster', 3113), ('dumpstered', 3114), ('dun', 3115), ('duneo', 3116), ('dunk', 3117), ('dunno', 3118), ('duno', 3119), ('dunoe', 3120), ('dunwan', 3121), ('duo', 3122), ('dup', 3123), ('dupp', 3124), ('dura', 3125), ('duration', 3126), ('during', 3127), ('dusa', 3128), ('dust', 3129), ('dusttttt', 3130), ('duu', 3131), ('duuude', 3132), ('duuuude', 3133), ('duza', 3134), ('duzel', 3135), ('dw', 3136), ('dweller', 3137), ('dxxdxdxdxdxd', 3138), ('dying', 3139), ('dynasty', 3140), ('dzzle', 3141), ('e', 3142), ('ea', 3143), ('eaaaaaaaaaaaaaaaaaasy', 3144), ('each', 3145), ('ead', 3146), ('eaeaea', 3147), ('eager', 3148), ('eagis', 3149), ('eaglesong', 3150), ('eaisy', 3151), ('eaiuheaiuheaiueaiuheaea', 3152), ('eally', 3153), ('eam', 3154), ('eanwhile', 3155), ('ear', 3156), ('earlier', 3157), ('early', 3158), ('earlyy', 3159), ('earn', 3160), ('earth', 3161), ('earthsahker', 3162), ('earthshaker', 3163), ('eartshaker', 3164), ('easerion', 3165), ('easi', 3166), ('easier', 3167), ('easiest', 3168), ('easily', 3169), ('easir', 3170), ('east', 3171), ('easy', 3172), ('easyx', 3173), ('easyyyyyyyyyyyyyyyyyyy', 3174), ('eat', 3175), ('eath', 3176), ('eating', 3177), ('eaxe', 3178), ('eayh', 3179), ('eaz', 3180), ('eazy', 3181), ('eazzz', 3182), ('eazzzzzzzzzzz', 3183), ('ebal', 3184), ('ebalo', 3185), ('ebanat', 3186), ('ebanie', 3187), ('ebaniy', 3188), ('ebanoe', 3189), ('ebanutie', 3190), ('ebanyyt', 3191), ('ebashil', 3192), ('ebat', 3193), ('ebaty', 3194), ('ebd', 3195), ('ebnutii', 3196), ('ebola', 3197), ('ebqal', 3198), ('ebtter', 3199), ('ec', 3200), ('ecchi', 3201), ('echjo', 3202), ('echo', 3203), ('ecks', 3204), ('eco', 3205), ('ecuador', 3206), ('ecvho', 3207), ('ed', 3208), ('edad', 3209), ('edge', 3210), ('edi', 3211), ('edn', 3212), ('educated', 3213), ('education', 3214), ('edz', 3215), ('ee', 3216), ('eeeee', 3217), ('eeeeeeeeeeeeeeez', 3218), ('eeeeeet', 3219), ('eeeeet', 3220), ('eeeidiot', 3221), ('eeeooo', 3222), ('eeezzz', 3223), ('eerroooooy', 3224), ('eez', 3225), ('ef', 3226), ('effect', 3227), ('effeect', 3228), ('effigy', 3229), ('effor', 3230), ('effort', 3231), ('eg', 3232), ('ege', 3233), ('egege', 3234), ('eggs', 3235), ('egm', 3236), ('ego', 3237), ('egofkl', 3238), ('egypt', 3239), ('eh', 3240), ('ehat', 3241), ('ehco', 3242), ('ehd', 3243), ('eheh', 3244), ('ehehe', 3245), ('ehehehe', 3246), ('ehehehehee', 3247), ('eheree', 3248), ('ehh', 3249), ('ehp', 3250), ('ehueheuheu', 3251), ('eiiii', 3252), ('einfach', 3253), ('either', 3254), ('ejejejej', 3255), ('ekkk', 3256), ('eks', 3257), ('el', 3258), ('elad', 3259), ('elder', 3260), ('ele', 3261), ('electric', 3262), ('electricity', 3263), ('elefuckinggiggle', 3264), ('elegiggle', 3265), ('eleniggle', 3266), ('elfticle', 3267), ('elkse', 3268), ('elo', 3269), ('elsa', 3270), ('else', 3271), ('elses', 3272), ('elvinskin', 3273), ('em', 3274), ('email', 3275), ('embarasing', 3276), ('embarrasing', 3277), ('ember', 3278), ('embermon', 3279), ('embers', 3280), ('emd', 3281), ('emdio', 3282), ('emebr', 3283), ('emer', 3284), ('emergency', 3285), ('emil', 3286), ('eminem', 3287), ('emnd', 3288), ('emo', 3289), ('emore', 3290), ('emote', 3291), ('emotional', 3292), ('empire', 3293), ('en', 3294), ('ena', 3295), ('enbd', 3296), ('encan', 3297), ('ench', 3298), ('enchant', 3299), ('enchantress', 3300), ('encountered', 3301), ('encouragement', 3302), ('end', 3303), ('end2', 3304), ('endead', 3305), ('ended', 3306), ('ending', 3307), ('endlessly', 3308), ('eneemies', 3309), ('enemies', 3310), ('enemy', 3311), ('energy', 3312), ('eng', 3313), ('engage', 3314), ('engaged', 3315), ('engagments', 3316), ('engalndski', 3317), ('engines', 3318), ('england', 3319), ('englihs', 3320), ('englioh', 3321), ('englis', 3322), ('english', 3323), ('eni', 3324), ('enig', 3325), ('eniggerma', 3326), ('enigma', 3327), ('enimga', 3328), ('enjouy', 3329), ('enjoy', 3330), ('enjoying', 3331), ('enmy', 3332), ('ennd', 3333), ('enopjouay', 3334), ('enough', 3335), ('enought', 3336), ('ensd', 3337), ('ensinar', 3338), ('ent', 3339), ('enter', 3340), ('entertain', 3341), ('entertaining', 3342), ('entiendame', 3343), ('entire', 3344), ('entre', 3345), ('entren', 3346), ('entrense', 3347), ('enuf', 3348), ('env', 3349), ('enviroment', 3350), ('enxt', 3351), ('enyway', 3352), ('eos', 3353), ('ep', 3354), ('epenis', 3355), ('epic', 3356), ('epig', 3357), ('eport', 3358), ('eppi', 3359), ('epwoer', 3360), ('equally', 3361), ('er', 3362), ('era', 3363), ('eral', 3364), ('erdogan', 3365), ('ere', 3366), ('eres', 3367), ('erious', 3368), ('erm', 3369), ('ermm', 3370), ('err', 3371), ('erroe', 3372), ('error', 3373), ('errro', 3374), ('errrr', 3375), ('eruvians', 3376), ('erz', 3377), ('es', 3378), ('esa', 3379), ('esas', 3380), ('esasy', 3381), ('escape', 3382), ('escort', 3383), ('ese', 3384), ('esos', 3385), ('especially', 3386), ('especialy', 3387), ('espect', 3388), ('esports', 3389), ('essa', 3390), ('esse', 3391), ('essence', 3392), ('essez', 3393), ('esss', 3394), ('est', 3395), ('esta', 3396), ('estas', 3397), ('este', 3398), ('esteemed', 3399), ('esteroids', 3400), ('estrelas', 3401), ('esx', 3402), ('et', 3403), ('etc', 3404), ('etcetcetc', 3405), ('eternal', 3406), ('ethan', 3407), ('etime', 3408), ('etix', 3409), ('eto', 3410), ('etting', 3411), ('eu', 3412), ('eul', 3413), ('euls', 3414), ('eulsed', 3415), ('euro', 3416), ('europa', 3417), ('europe', 3418), ('european', 3419), ('eus', 3420), ('euuuuuuuuuuuuuuuropppppppppeeeeeeeeeeeeeeeeeeeeeeeeeeee', 3421), ('euw', 3422), ('eva', 3423), ('evahh', 3424), ('evasion', 3425), ('evasiooon', 3426), ('evberywhere', 3427), ('eve', 3428), ('eveeeerrrrrrr', 3429), ('even', 3430), ('evening', 3431), ('ever', 3432), ('evertiem', 3433), ('every', 3434), ('every1', 3435), ('everyday', 3436), ('everyon', 3437), ('everyone', 3438), ('everyones', 3439), ('everysingle', 3440), ('everything', 3441), ('everytime', 3442), ('everyting', 3443), ('everywehere', 3444), ('everywhere', 3445), ('evne', 3446), ('ew', 3447), ('ewait', 3448), ('ewe', 3449), ('ewtd', 3450), ('ex', 3451), ('exactly', 3452), ('exam', 3453), ('exams', 3454), ('excelelnt', 3455), ('excellent', 3456), ('except', 3457), ('excite', 3458), ('excited', 3459), ('excuse', 3460), ('excuses', 3461), ('exe', 3462), ('exec', 3463), ('exex', 3464), ('exicte', 3465), ('exist', 3466), ('existing', 3467), ('exited', 3468), ('exort', 3469), ('exp', 3470), ('expecially', 3471), ('expect', 3472), ('expected', 3473), ('expecting', 3474), ('expects', 3475), ('expensice', 3476), ('expensive', 3477), ('expire', 3478), ('explain', 3479), ('explane', 3480), ('exploded', 3481), ('express', 3482), ('expressed', 3483), ('exprinece', 3484), ('extend', 3485), ('extent', 3486), ('extra', 3487), ('exyz', 3488), ('exz', 3489), ('ey', 3490), ('eye', 3491), ('eyeballs', 3492), ('eyes', 3493), ('eys', 3494), ('ez', 3495), ('ez4', 3496), ('eza', 3497), ('ezaasasaas', 3498), ('ezaoso', 3499), ('eze', 3500), ('ezeist', 3501), ('ezez', 3502), ('ezezeze', 3503), ('ezflex', 3504), ('ezgame', 3505), ('ezi', 3506), ('eziest', 3507), ('ezist', 3508), ('ezmid', 3509), ('ezmmr', 3510), ('ezmode', 3511), ('ezpz', 3512), ('ezs', 3513), ('ezszz', 3514), ('ezt', 3515), ('ezwp', 3516), ('ezy', 3517), ('ezz', 3518), ('ezzz', 3519), ('ezzzy', 3520), ('ezzzz', 3521), ('ezzzzzz', 3522), ('ezzzzzzzzzz', 3523), ('ezzzzzzzzzzzz', 3524), ('ezzzzzzzzzzzzzz', 3525), ('ezzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz', 3526), ('f', 3527), ('f4', 3528), ('f9', 3529), ('fa', 3530), ('fab', 3531), ('face', 3532), ('facebook', 3533), ('facepalm', 3534), ('facil', 3535), ('fack', 3536), ('facke', 3537), ('fact', 3538), ('factas', 3539), ('factor', 3540), ('facts', 3541), ('fae', 3542), ('fag', 3543), ('fagate', 3544), ('fagets', 3545), ('faggerts', 3546), ('faggets', 3547), ('faggors', 3548), ('faggorts', 3549), ('faggot', 3550), ('faggots', 3551), ('fagnuts', 3552), ('fags', 3553), ('fail', 3554), ('failed', 3555), ('failing', 3556), ('fair', 3557), ('fairly', 3558), ('fairplay', 3559), ('fait', 3560), ('faith', 3561), ('faithless', 3562), ('fak', 3563), ('faka', 3564), ('fake', 3565), ('fakkkkkkkkk', 3566), ('fall', 3567), ('fallen', 3568), ('falling', 3569), ('fallout', 3570), ('false', 3571), ('fam', 3572), ('famapage', 3573), ('fame', 3574), ('familiars', 3575), ('family', 3576), ('famr', 3577), ('fan', 3578), ('fancy', 3579), ('fani', 3580), ('fans', 3581), ('fantastic', 3582), ('fantatsic', 3583), ('fappy', 3584), ('faq', 3585), ('far', 3586), ('fare', 3587), ('fark', 3588), ('farm', 3589), ('farm2', 3590), ('farmed', 3591), ('farmei', 3592), ('farmer', 3593), ('farmers', 3594), ('farming', 3595), ('farmlane', 3596), ('farmou', 3597), ('farms', 3598), ('farmt', 3599), ('farmville', 3600), ('farrrrrrk', 3601), ('fast', 3602), ('faster', 3603), ('fasters', 3604), ('fastr', 3605), ('fat', 3606), ('fate', 3607), ('fatfuck', 3608), ('father', 3609), ('fatter', 3610), ('fatto', 3611), ('fattttttttttttttttttttttttttttttt', 3612), ('fatty', 3613), ('fault', 3614), ('favor', 3615), ('favorite', 3616), ('favotire', 3617), ('favour', 3618), ('fayze', 3619), ('fb', 3620), ('fcj', 3621), ('fck', 3622), ('fcken', 3623), ('fcker', 3624), ('fckin', 3625), ('fcking', 3626), ('fckng', 3627), ('fcktard', 3628), ('fcku', 3629), ('fcuk', 3630), ('fcukin', 3631), ('fcup', 3632), ('fdp', 3633), ('fead', 3634), ('feal', 3635), ('fear', 3636), ('featuring', 3637), ('fed', 3638), ('fedeer', 3639), ('feder', 3640), ('feed', 3641), ('feeded', 3642), ('feedeen', 3643), ('feeden', 3644), ('feeder', 3645), ('feederino', 3646), ('feeders', 3647), ('feeding', 3648), ('feeds', 3649), ('feedy', 3650), ('feeed', 3651), ('feeeder', 3652), ('feeeeeed', 3653), ('feel', 3654), ('feeler', 3655), ('feeling', 3656), ('feelling', 3657), ('feels', 3658), ('feelsbadman', 3659), ('feet', 3660), ('fegget', 3661), ('feggets', 3662), ('feggots', 3663), ('fegs', 3664), ('fel', 3665), ('felankor', 3666), ('felix', 3667), ('fell', 3668), ('fellas', 3669), ('fellow', 3670), ('felt', 3671), ('female', 3672), ('feminazi', 3673), ('fend', 3674), ('fest', 3675), ('festival', 3676), ('few', 3677), ('fez', 3678), ('ff', 3679), ('ffed', 3680), ('ffeded', 3681), ('fff', 3682), ('ffs', 3683), ('fg', 3684), ('fgogogoogog', 3685), ('fgsief', 3686), ('fgts', 3687), ('fguck', 3688), ('fhm', 3689), ('fianlly', 3690), ('fickijg', 3691), ('ficking', 3692), ('fickler', 3693), ('fickong', 3694), ('ficks', 3695), ('fid', 3696), ('fidel', 3697), ('fiend', 3698), ('fiesta', 3699), ('fight', 3700), ('fighting', 3701), ('fights', 3702), ('figueiredo', 3703), ('figure', 3704), ('figured', 3705), ('fihgt', 3706), ('fiiight', 3707), ('filho', 3708), ('filipinop', 3709), ('filipinos', 3710), ('filthy', 3711), ('fin', 3712), ('final', 3713), ('finally', 3714), ('finaly', 3715), ('find', 3716), ('finding', 3717), ('finds', 3718), ('fine', 3719), ('fing', 3720), ('finger', 3721), ('fingered', 3722), ('fingering', 3723), ('fingers', 3724), ('finis', 3725), ('finish', 3726), ('finished', 3727), ('finn', 3728), ('finnaly', 3729), ('finsh', 3730), ('finsih', 3731), ('fir', 3732), ('fire', 3733), ('fired', 3734), ('fireworks', 3735), ('firme', 3736), ('firs', 3737), ('first', 3738), ('firstbloodion', 3739), ('firstpickl', 3740), ('firsts', 3741), ('fish', 3742), ('fishnet', 3743), ('fissure', 3744), ('fist', 3745), ('fisure', 3746), ('fit', 3747), ('fite', 3748), ('fits', 3749), ('fiuck', 3750), ('five', 3751), ('fix', 3752), ('fixed', 3753), ('fixing', 3754), ('fiz', 3755), ('fjuck', 3756), ('fk', 3757), ('fkb', 3758), ('fkboi', 3759), ('fkc', 3760), ('fkcing', 3761), ('fkcung', 3762), ('fked', 3763), ('fkin', 3764), ('fking', 3765), ('fkinnnnnnnnn', 3766), ('fkn', 3767), ('fkng', 3768), ('fku', 3769), ('fl', 3770), ('flag', 3771), ('flame', 3772), ('flamed', 3773), ('flamer', 3774), ('flames', 3775), ('flaming', 3776), ('flamming', 3777), ('flare', 3778), ('flarex', 3779), ('flash', 3780), ('flashmob', 3781), ('flashy', 3782), ('flaw', 3783), ('flaws', 3784), ('flesh', 3785), ('float', 3786), ('floatey', 3787), ('fly', 3788), ('flying', 3789), ('fml', 3790), ('fnatic', 3791), ('fng', 3792), ('fnish', 3793), ('fo', 3794), ('focken', 3795), ('focker', 3796), ('fockin', 3797), ('focus', 3798), ('focusing', 3799), ('focusings', 3800), ('fode', 3801), ('fog', 3802), ('fokin', 3803), ('fokkkeeen', 3804), ('folks', 3805), ('follow', 3806), ('following', 3807), ('fone', 3808), ('font', 3809), ('foo', 3810), ('food', 3811), ('foodtrip', 3812), ('fool', 3813), ('fools', 3814), ('foook', 3815), ('footfalls', 3816), ('footlocker', 3817), ('for', 3818), ('force', 3819), ('forced', 3820), ('forcestaff', 3821), ('fore', 3822), ('forest', 3823), ('forever', 3824), ('forget', 3825), ('forgive', 3826), ('forgot', 3827), ('form', 3828), ('fortify', 3829), ('fortnight', 3830), ('fortune', 3831), ('foru', 3832), ('forver', 3833), ('forward', 3834), ('fought', 3835), ('foul', 3836), ('found', 3837), ('fountain', 3838), ('fountaion', 3839), ('fow', 3840), ('fps', 3841), ('fq', 3842), ('fr', 3843), ('frags', 3844), ('france', 3845), ('frank', 3846), ('frankfurt', 3847), ('fraps', 3848), ('frduikfaimcl', 3849), ('fre', 3850), ('freak', 3851), ('freakin', 3852), ('freaking', 3853), ('free', 3854), ('freeand', 3855), ('freee', 3856), ('freefarm', 3857), ('freefarme', 3858), ('freefarmed', 3859), ('freely', 3860), ('freewin', 3861), ('freeze', 3862), ('freezes', 3863), ('freezing', 3864), ('freidn', 3865), ('freind', 3866), ('french', 3867), ('frenchie', 3868), ('frend', 3869), ('frens', 3870), ('frere', 3871), ('fresh', 3872), ('fri', 3873), ('frieenddd', 3874), ('friend', 3875), ('friendliness', 3876), ('friendly', 3877), ('friends', 3878), ('friendship', 3879), ('friendsip', 3880), ('friennd', 3881), ('frist', 3882), ('frm', 3883), ('fro', 3884), ('frog', 3885), ('from', 3886), ('front', 3887), ('frost', 3888), ('frozen', 3889), ('frustrated', 3890), ('fssiisure', 3891), ('fsuck', 3892), ('ft', 3893), ('ftw', 3894), ('fu', 3895), ('fu9ck', 3896), ('fuark', 3897), ('fuaskf', 3898), ('fuc', 3899), ('fucekr', 3900), ('fucik', 3901), ('fucj', 3902), ('fucjk', 3903), ('fuck', 3904), ('fucka', 3905), ('fuckboys', 3906), ('fucke', 3907), ('fucked', 3908), ('fuckeduplife', 3909), ('fucken', 3910), ('fucker', 3911), ('fuckerclink', 3912), ('fuckers', 3913), ('fuckhead', 3914), ('fuckig', 3915), ('fuckign', 3916), ('fuckiig', 3917), ('fuckikgn', 3918), ('fuckikn', 3919), ('fuckimg', 3920), ('fuckin', 3921), ('fuckinearth', 3922), ('fuckinf', 3923), ('fucking', 3924), ('fuckingf', 3925), ('fuckingg', 3926), ('fuckingh', 3927), ('fuckingmmr', 3928), ('fuckings', 3929), ('fuckinng', 3930), ('fuckiong', 3931), ('fuckj', 3932), ('fuckker', 3933), ('fuckkin', 3934), ('fuckkk', 3935), ('fuckl', 3936), ('fuckler', 3937), ('fuckload', 3938), ('fuckme', 3939), ('fuckn', 3940), ('fuckng', 3941), ('fucknig', 3942), ('fuckong', 3943), ('fucks', 3944), ('fucksake', 3945), ('fucksdf', 3946), ('fucksticks', 3947), ('fucktad', 3948), ('fucktard', 3949), ('fucktards', 3950), ('fuckton', 3951), ('fucktrash', 3952), ('fucku', 3953), ('fuckwad', 3954), ('fuckwit', 3955), ('fuckwits', 3956), ('fuckyou', 3957), ('fucl', 3958), ('fucong', 3959), ('fucvk', 3960), ('fude', 3961), ('fue', 3962), ('fug', 3963), ('fught', 3964), ('fuhrzen', 3965), ('fuicken', 3966), ('fuicking', 3967), ('fuk', 3968), ('fukboi', 3969), ('fukcing', 3970), ('fukciong', 3971), ('fuken', 3972), ('fukenzio', 3973), ('fukin', 3974), ('fuking', 3975), ('fukking', 3976), ('fukn', 3977), ('fuknh', 3978), ('ful', 3979), ('full', 3980), ('fullslotted', 3981), ('fuman', 3982), ('fuming', 3983), ('fun', 3984), ('funniest', 3985), ('funnik', 3986), ('funnily', 3987), ('funnny', 3988), ('funny', 3989), ('funy', 3990), ('fuq', 3991), ('fuqn', 3992), ('fura', 3993), ('furi', 3994), ('furion', 3995), ('furious', 3996), ('furry', 3997), ('fururo', 3998), ('fury', 3999), ('future', 4000), ('fututre', 4001), ('fuuck', 4002), ('fuxk', 4003), ('fuyck', 4004), ('fuyckjerfe', 4005), ('fuycku', 4006), ('fuzzy', 4007), ('fv', 4008), ('fvb', 4009), ('fvbr2uwieoakf8u49gjirufjnghyju3iuwjfmg', 4010), ('fvck', 4011), ('fvcking', 4012), ('fvking', 4013), ('fw', 4014), ('fwajhwfhbwfahbawfbhwfbhawf', 4015), ('fxxk', 4016), ('fy', 4017), ('fycj', 4018), ('fyi', 4019), ('fyuck', 4020), ('fyuuuhhh', 4021), ('g', 4022), ('g3g3', 4023), ('ga', 4024), ('gaaaame', 4025), ('gabe', 4026), ('gaben', 4027), ('gae', 4028), ('gaem', 4029), ('gaems', 4030), ('gaga', 4031), ('gagaga', 4032), ('gagagaga', 4033), ('gaglhwslhw', 4034), ('gago', 4035), ('gagos', 4036), ('gahaha', 4037), ('gahahfa', 4038), ('gahahhhhh', 4039), ('gahi', 4040), ('gained', 4041), ('gais', 4042), ('galactic', 4043), ('galahd', 4044), ('galeng', 4045), ('galet', 4046), ('galing', 4047), ('galsndlasndj', 4048), ('galwang', 4049), ('gam', 4050), ('gamburger', 4051), ('game', 4052), ('gameplan', 4053), ('gameplay', 4054), ('gamer', 4055), ('gamers', 4056), ('gameruining', 4057), ('games', 4058), ('gamesr', 4059), ('gaming', 4060), ('gamme', 4061), ('gammer', 4062), ('gamming', 4063), ('gamne', 4064), ('gan', 4065), ('gander', 4066), ('gane', 4067), ('gang', 4068), ('gangbang', 4069), ('gangerion', 4070), ('ganging', 4071), ('gangsta', 4072), ('gank', 4073), ('ganked', 4074), ('ganker', 4075), ('gankin', 4076), ('ganking', 4077), ('ganks', 4078), ('ganr', 4079), ('gap', 4080), ('garavce', 4081), ('garbage', 4082), ('gary', 4083), ('gas', 4084), ('gass', 4085), ('gate', 4086), ('gaurdians', 4087), ('gave', 4088), ('gawn', 4089), ('gay', 4090), ('gaya', 4091), ('gaybin', 4092), ('gaycumback', 4093), ('gaycunt', 4094), ('gayer', 4095), ('gayest', 4096), ('gayish', 4097), ('gays', 4098), ('gayshit', 4099), ('gayteam', 4100), ('gayyy', 4101), ('gayyyyyyyyy', 4102), ('gb', 4103), ('gbaha', 4104), ('gbarrel', 4105), ('gde', 4106), ('ge', 4107), ('geam', 4108), ('geasy', 4109), ('gee', 4110), ('geeeeeeeeeeeeeeeeeee', 4111), ('geeeeeeeeeeeeeeeeeeeeeeeee', 4112), ('geeeeeeeeeegeeeeeeeeeeeeeeeeee', 4113), ('geeett', 4114), ('geege', 4115), ('geegee', 4116), ('geen', 4117), ('geez', 4118), ('geg', 4119), ('gege', 4120), ('gegege', 4121), ('gegegegegeegeegegegeeg', 4122), ('gegge', 4123), ('geh', 4124), ('gem', 4125), ('gemom', 4126), ('gems', 4127), ('general', 4128), ('genius', 4129), ('genoius', 4130), ('gente', 4131), ('gentelmen', 4132), ('gents', 4133), ('genuinely', 4134), ('george', 4135), ('ger', 4136), ('geral', 4137), ('german', 4138), ('germans', 4139), ('germany', 4140), ('get', 4141), ('getem', 4142), ('gets', 4143), ('gettin', 4144), ('getting', 4145), ('gez', 4146), ('gf', 4147), ('gfamer', 4148), ('gfg', 4149), ('gfs', 4150), ('gfun', 4151), ('gg', 4152), ('gget', 4153), ('ggez', 4154), ('ggf', 4155), ('ggg', 4156), ('gggame', 4157), ('gggg', 4158), ('ggggg', 4159), ('gggggg', 4160), ('ggggggg', 4161), ('gggggggggggg', 4162), ('gggggggggggggg', 4163), ('ggggggggggggggg', 4164), ('ggggggggggggggggg', 4165), ('gggggggggggggggggggg', 4166), ('ggggggggggggggggggggggg', 4167), ('ggggggggggggggggggggggggggggg', 4168), ('gggggggggggggggggggggggggggggggggggggggg', 4169), ('gggwp', 4170), ('ggl', 4171), ('ggness', 4172), ('ggng', 4173), ('ggnore', 4174), ('ggo', 4175), ('ggoing', 4176), ('ggp', 4177), ('ggpw', 4178), ('ggs', 4179), ('ggsf', 4180), ('ggty', 4181), ('gguys', 4182), ('ggw', 4183), ('ggwep', 4184), ('ggwo', 4185), ('ggwomi', 4186), ('ggwp', 4187), ('ggwpp', 4188), ('gh', 4189), ('gha', 4190), ('ghad', 4191), ('ghaha', 4192), ('ghg', 4193), ('ghhaha', 4194), ('ghhahhaha', 4195), ('gho', 4196), ('ghost', 4197), ('ghots', 4198), ('ghrthj', 4199), ('ghusk', 4200), ('gia', 4201), ('gib', 4202), ('gichiaan', 4203), ('gif', 4204), ('giff', 4205), ('gift', 4206), ('gifts', 4207), ('gigi', 4208), ('gigil', 4209), ('gigilmats', 4210), ('gimme', 4211), ('giove', 4212), ('girl', 4213), ('girlfriend', 4214), ('girls', 4215), ('girlssss', 4216), ('giting', 4217), ('giusy', 4218), ('giv', 4219), ('give', 4220), ('given', 4221), ('gives', 4222), ('givin', 4223), ('giving', 4224), ('gj', 4225), ('gjhf', 4226), ('gk', 4227), ('gkp', 4228), ('gl', 4229), ('glad', 4230), ('glaives', 4231), ('glaubst', 4232), ('glboys', 4233), ('glfhf', 4234), ('glfu', 4235), ('glgl', 4236), ('glglgl', 4237), ('glglgllglglglgl', 4238), ('glhf', 4239), ('glim', 4240), ('glimmer', 4241), ('glimpse', 4242), ('glitch', 4243), ('global', 4244), ('globals', 4245), ('globl', 4246), ('glose', 4247), ('glyphs', 4248), ('gma', 4249), ('gmae', 4250), ('gme', 4251), ('gna', 4252), ('gnholb', 4253), ('gntie', 4254), ('gnyt', 4255), ('go', 4256), ('gob', 4257), ('goblack', 4258), ('goblok', 4259), ('god', 4260), ('godd', 4261), ('goddamit', 4262), ('goddamn', 4263), ('goddddddddddddddddddddddddddddddd', 4264), ('godlike', 4265), ('godong', 4266), ('godot', 4267), ('gods', 4268), ('godthel', 4269), ('goes', 4270), ('gogo', 4271), ('gogog', 4272), ('gogogo', 4273), ('gogogog', 4274), ('gogoogogogogogogogo', 4275), ('goi', 4276), ('goin', 4277), ('going', 4278), ('goiod', 4279), ('gok', 4280), ('golbal', 4281), ('gold', 4282), ('golden', 4283), ('goldi', 4284), ('golds', 4285), ('golem', 4286), ('golems', 4287), ('golpear', 4288), ('golum', 4289), ('gona', 4290), ('gondar', 4291), ('gone', 4292), ('gonna', 4293), ('goo', 4294), ('good', 4295), ('goodboy', 4296), ('goodbye', 4297), ('gooddddddddddddd', 4298), ('goodgame', 4299), ('goodness', 4300), ('goodnight', 4301), ('goods', 4302), ('goof', 4303), ('google', 4304), ('googoo', 4305), ('gook', 4306), ('gooo', 4307), ('goood', 4308), ('goooo', 4309), ('gooooood', 4310), ('gor', 4311), ('gordito', 4312), ('gorillas', 4313), ('gorrila', 4314), ('gos', 4315), ('gosu', 4316), ('got', 4317), ('gotcha', 4318), ('goteem', 4319), ('gotem', 4320), ('gotta', 4321), ('gotten', 4322), ('gottes', 4323), ('gotya', 4324), ('gould', 4325), ('goverment', 4326), ('govna', 4327), ('gpm', 4328), ('gqame', 4329), ('gr8', 4330), ('gracefully', 4331), ('gracias', 4332), ('grammar', 4333), ('grand', 4334), ('grandmother', 4335), ('grandmothers', 4336), ('grandpa', 4337), ('graphh', 4338), ('graphic', 4339), ('graphics', 4340), ('grapic', 4341), ('gratification', 4342), ('grats', 4343), ('gratz', 4344), ('grave', 4345), ('graves', 4346), ('greaaaaaaat', 4347), ('great', 4348), ('greatest', 4349), ('greaves', 4350), ('gree', 4351), ('greece', 4352), ('greed', 4353), ('greedisgood', 4354), ('greedy', 4355), ('greek', 4356), ('greeks', 4357), ('green', 4358), ('greenm', 4359), ('greeting', 4360), ('greevees', 4361), ('greeves', 4362), ('gret', 4363), ('grey', 4364), ('grieving', 4365), ('grill', 4366), ('grills', 4367), ('ground', 4368), ('group', 4369), ('grp', 4370), ('grredy', 4371), ('grrr', 4372), ('grub', 4373), ('gry', 4374), ('gryas', 4375), ('gryo', 4376), ('gs', 4377), ('gt', 4378), ('gtamne', 4379), ('gtf', 4380), ('gtfo', 4381), ('gtg', 4382), ('gto', 4383), ('gtting', 4384), ('gua', 4385), ('guarantee', 4386), ('guardian', 4387), ('guarding', 4388), ('gucci', 4389), ('gud', 4390), ('guess', 4391), ('guesss', 4392), ('guesxzs', 4393), ('guinness', 4394), ('guiys', 4395), ('gun', 4396), ('guna', 4397), ('gunggong', 4398), ('gunna', 4399), ('gus', 4400), ('guse', 4401), ('gusta', 4402), ('gustaria', 4403), ('gustav', 4404), ('guts', 4405), ('gutta', 4406), ('guy', 4407), ('guys', 4408), ('guysreport', 4409), ('guyss', 4410), ('guysss', 4411), ('guyz', 4412), ('guyzzz', 4413), ('guyzzzz', 4414), ('gv', 4415), ('gwp', 4416), ('gws', 4417), ('gyri', 4418), ('gyro', 4419), ('gyrohc', 4420), ('gys', 4421), ('gyus', 4422), ('h', 4423), ('h3h3', 4424), ('h3h3h3', 4425), ('h4ijuwiksdfuyimuyijamjy879ialnbyhabhgl', 4426), ('h8', 4427), ('ha', 4428), ('ha2h2hh2h232h2ha', 4429), ('haa', 4430), ('haaa', 4431), ('haaaaaaaaaaaaaaaa', 4432), ('haaaaaaaaaaaaaaaaaaaa', 4433), ('haaaaaaaaaaaaaaaaaaaaaa', 4434), ('haaaaaaaaaaammmmmmmmmmmmmeeeeeeeeee', 4435), ('haah', 4436), ('haaha', 4437), ('haahah', 4438), ('haahaha', 4439), ('haahahah', 4440), ('haahahaha', 4441), ('haahahahha', 4442), ('haahahhaa', 4443), ('haahh', 4444), ('haahha', 4445), ('haahhaa', 4446), ('haahhaah', 4447), ('habia', 4448), ('habit', 4449), ('habla', 4450), ('hablar', 4451), ('hac', 4452), ('hack', 4453), ('hacked', 4454), ('hacker', 4455), ('hacks', 4456), ('had', 4457), ('haduken', 4458), ('hae', 4459), ('haehaehae', 4460), ('haets', 4461), ('haf', 4462), ('hagaga', 4463), ('hah', 4464), ('haha', 4465), ('haha1', 4466), ('hahaa', 4467), ('hahaah', 4468), ('hahaahaha', 4469), ('hahaahahaha', 4470), ('hahaahahahahaha', 4471), ('hahaahha', 4472), ('hahah', 4473), ('hahaha', 4474), ('hahahaa', 4475), ('hahahaah', 4476), ('hahahaahah', 4477), ('hahahaahhahahaah', 4478), ('hahahaahhahahhaaaah', 4479), ('hahahah', 4480), ('hahahaha', 4481), ('hahahahaa', 4482), ('hahahahaah', 4483), ('hahahahaahaha', 4484), ('hahahahah', 4485), ('hahahahaha', 4486), ('hahahahahaahahahahaha', 4487), ('hahahahahah', 4488), ('hahahahahaha', 4489), ('hahahahahahaahahaha', 4490), ('hahahahahahah', 4491), ('hahahahahahaha', 4492), ('hahahahahahahaha', 4493), ('hahahahahahahahah', 4494), ('hahahahahahahahaha', 4495), ('hahahahahahahahahaha', 4496), ('hahahahahahahahahahahahahaha', 4497), ('hahahahahahahahahahahahahahahhahahahahaha', 4498), ('hahahahahahhaahahah', 4499), ('hahahahahha', 4500), ('hahahahahhaah', 4501), ('hahahahahhahaa', 4502), ('hahahahahhahah', 4503), ('hahahahh', 4504), ('hahahahha', 4505), ('hahahahhaa', 4506), ('hahahahhaha', 4507), ('hahahahhahaha', 4508), ('hahahahhahahahhahahahahhahahha', 4509), ('hahahai', 4510), ('hahahh', 4511), ('hahahha', 4512), ('hahahhaa', 4513), ('hahahhaah', 4514), ('hahahhaha', 4515), ('hahahhahaa', 4516), ('hahahhahah', 4517), ('hahahhahahaha', 4518), ('hahahhahahahah', 4519), ('hahahhahahahhahahahhahaha', 4520), ('hahahhahahha', 4521), ('hahahhahhaha', 4522), ('hahahiz', 4523), ('hahaihah', 4524), ('hahaiz', 4525), ('hahaqha', 4526), ('hahg', 4527), ('hahga', 4528), ('hahha', 4529), ('hahhaa', 4530), ('hahhah', 4531), ('hahhaha', 4532), ('hahhahah', 4533), ('hahhahaha', 4534), ('hahhahahahahaha', 4535), ('hahhahha', 4536), ('hahhha', 4537), ('hahhhahahhahah', 4538), ('hahyahahahe', 4539), ('hai', 4540), ('hail', 4541), ('hair', 4542), ('hais', 4543), ('haist', 4544), ('haiz', 4545), ('haizzz', 4546), ('hakey', 4547), ('hakyouken', 4548), ('halaga', 4549), ('half', 4550), ('hallo', 4551), ('halp', 4552), ('hand', 4553), ('handedly', 4554), ('handicap', 4555), ('handing', 4556), ('handle', 4557), ('handling', 4558), ('hands', 4559), ('hang', 4560), ('hanged', 4561), ('hanging', 4562), ('hans', 4563), ('hao', 4564), ('happeend', 4565), ('happen', 4566), ('happend', 4567), ('happened', 4568), ('happenign', 4569), ('happening', 4570), ('happens', 4571), ('happy', 4572), ('harassing', 4573), ('hard', 4574), ('hardcore', 4575), ('harder', 4576), ('hardest', 4577), ('hardhomie', 4578), ('hardlane', 4579), ('hardlaner', 4580), ('hardly', 4581), ('hards', 4582), ('harem', 4583), ('harm', 4584), ('haromo', 4585), ('harper', 4586), ('harras', 4587), ('harrass', 4588), ('harriott', 4589), ('harsh', 4590), ('has', 4591), ('hash', 4592), ('hasnt', 4593), ('hast', 4594), ('hastag', 4595), ('haste', 4596), ('hat', 4597), ('hate', 4598), ('haters', 4599), ('hates', 4600), ('hati', 4601), ('hating', 4602), ('hav', 4603), ('havbe', 4604), ('have', 4605), ('haveen', 4606), ('havenet', 4607), ('havent', 4608), ('havin', 4609), ('having', 4610), ('havn', 4611), ('havnt', 4612), ('havta', 4613), ('hawhwahaw', 4614), ('hay', 4615), ('hayne', 4616), ('hayo', 4617), ('hayop', 4618), ('hays', 4619), ('hayshit', 4620), ('hayy', 4621), ('hayz', 4622), ('hc', 4623), ('hcicken', 4624), ('hdtn', 4625), ('he', 4626), ('head', 4627), ('heads', 4628), ('headshot', 4629), ('headshots', 4630), ('heal', 4631), ('healed', 4632), ('healer', 4633), ('healers', 4634), ('healing', 4635), ('health', 4636), ('healthbar', 4637), ('healtrain', 4638), ('heang', 4639), ('heap', 4640), ('heaps', 4641), ('hear', 4642), ('heard', 4643), ('heart', 4644), ('heathens', 4645), ('heaven', 4646), ('heavy', 4647), ('hechas', 4648), ('hecho', 4649), ('heck', 4650), ('hed', 4651), ('hee', 4652), ('heee', 4653), ('heeeeeeeeeee', 4654), ('heeeeeeeeeeeeeeeee', 4655), ('heeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeres', 4656), ('heeh', 4657), ('heehehehhe', 4658), ('heff', 4659), ('heh', 4660), ('hehe', 4661), ('heheh', 4662), ('hehehe', 4663), ('hehehehe', 4664), ('heheheheheeheheheheheheheeheheheheheheheheheehehehehehehee', 4665), ('hehez', 4666), ('hehhe', 4667), ('hehje', 4668), ('hei', 4669), ('helenalive', 4670), ('helic', 4671), ('helkl', 4672), ('hell', 4673), ('hello', 4674), ('helloo', 4675), ('hellooooo', 4676), ('helo', 4677), ('help', 4678), ('helped', 4679), ('helping', 4680), ('helpless', 4681), ('helps', 4682), ('helvete', 4683), ('hen', 4684), ('hence', 4685), ('heng', 4686), ('her', 4687), ('here', 4688), ('heree', 4689), ('heretics', 4690), ('hero', 4691), ('heroes', 4692), ('heroess', 4693), ('heros', 4694), ('herrreeee', 4695), ('herself', 4696), ('herto', 4697), ('hes', 4698), ('het', 4699), ('hetero', 4700), ('heterosexual', 4701), ('hetp', 4702), ('hetting', 4703), ('heuheu', 4704), ('heuhue', 4705), ('hex', 4706), ('hey', 4707), ('heywk', 4708), ('hf', 4709), ('hfdifyt', 4710), ('hfoqho', 4711), ('hfr', 4712), ('hfsdjsfd', 4713), ('hg', 4714), ('hgahaha', 4715), ('hgahahah', 4716), ('hgahahaha', 4717), ('hgahhaha', 4718), ('hghaha', 4719), ('hghahahahaah', 4720), ('hgrsoiuherjs', 4721), ('hh', 4722), ('hha', 4723), ('hhaa', 4724), ('hhaha', 4725), ('hhahaha', 4726), ('hhahahahahahahahahahahaa', 4727), ('hhahahahhahahah', 4728), ('hhahahahhahahaha', 4729), ('hhahha', 4730), ('hhehe', 4731), ('hhh', 4732), ('hhhahahah', 4733), ('hhhh', 4734), ('hhhhhhhhhhh', 4735), ('hhhhhhhhhhhhh', 4736), ('hhm', 4737), ('hhqhqqhhq', 4738), ('hhshsdhsd', 4739), ('hhtfu', 4740), ('hi', 4741), ('hid', 4742), ('hidding', 4743), ('hide', 4744), ('hiding', 4745), ('hie', 4746), ('hieroglyphics', 4747), ('high', 4748), ('higher', 4749), ('highest', 4750), ('highground', 4751), ('highhhhhh', 4752), ('highlight', 4753), ('highlights', 4754), ('hihg', 4755), ('hihhiihi', 4756), ('hihi', 4757), ('hiii', 4758), ('hiiii', 4759), ('hijos', 4760), ('hikhik', 4761), ('hilakay', 4762), ('hilang', 4763), ('hilarious', 4764), ('hilariously', 4765), ('hill', 4766), ('him', 4767), ('himn', 4768), ('hims', 4769), ('himself', 4770), ('hio', 4771), ('hiohi', 4772), ('his', 4773), ('hisssssssssssss', 4774), ('history', 4775), ('hit', 4776), ('hitbox', 4777), ('hits', 4778), ('hitter', 4779), ('hitters', 4780), ('hitting', 4781), ('hizo', 4782), ('hjaha', 4783), ('hjahh', 4784), ('hje', 4785), ('hlp', 4786), ('hm', 4787), ('hmh', 4788), ('hmm', 4789), ('hmmm', 4790), ('hmmmm', 4791), ('hmmmmmmm', 4792), ('hmmmmmmmmmmmmmmm', 4793), ('ho', 4794), ('hoe', 4795), ('hoenst', 4796), ('hoes', 4797), ('hoho', 4798), ('hohohho', 4799), ('hohoho', 4800), ('hoi', 4801), ('hoiw', 4802), ('hol', 4803), ('hola', 4804), ('hold', 4805), ('holding', 4806), ('hole', 4807), ('holes', 4808), ('holu', 4809), ('holy', 4810), ('holys', 4811), ('holyshit', 4812), ('home', 4813), ('homeboy', 4814), ('homeless', 4815), ('homie', 4816), ('homies', 4817), ('homo', 4818), ('homophobic', 4819), ('homosexual', 4820), ('hon', 4821), ('honest', 4822), ('honestly', 4823), ('honesty', 4824), ('honey', 4825), ('honor', 4826), ('honorable', 4827), ('honour', 4828), ('honw', 4829), ('hoo', 4830), ('hood', 4831), ('hook', 4832), ('hooked', 4833), ('hooking', 4834), ('hookl', 4835), ('hooks', 4836), ('hookshot', 4837), ('hoook', 4838), ('hooorrrryyy', 4839), ('hope', 4840), ('hopefully', 4841), ('hopeless', 4842), ('hoping', 4843), ('hopping', 4844), ('horn', 4845), ('horns', 4846), ('horny', 4847), ('horoshiy', 4848), ('horrible', 4849), ('horrivel', 4850), ('horse', 4851), ('horseman', 4852), ('hospital', 4853), ('hosue', 4854), ('hot', 4855), ('hotkey', 4856), ('houes', 4857), ('houise', 4858), ('hour', 4859), ('hours', 4860), ('house', 4861), ('houses', 4862), ('hove', 4863), ('how', 4864), ('howd', 4865), ('howdu', 4866), ('howlonghave', 4867), ('hows', 4868), ('hoy', 4869), ('hoylfuck', 4870), ('hp', 4871), ('hpd', 4872), ('hphhp', 4873), ('hpta', 4874), ('hr', 4875), ('hrs', 4876), ('hrun', 4877), ('hs', 4878), ('hsjjahaha', 4879), ('hsldhlwhwl', 4880), ('hsould', 4881), ('htat', 4882), ('hte', 4883), ('htfu', 4884), ('htings', 4885), ('htink', 4886), ('htjghn', 4887), ('http', 4888), ('https', 4889), ('hu', 4890), ('hua', 4891), ('huauha', 4892), ('hue', 4893), ('huehue', 4894), ('huehuehuehue', 4895), ('huehueuhuehuehuehuehuehuehuehue', 4896), ('huesos', 4897), ('huesosi', 4898), ('huevie', 4899), ('hug', 4900), ('huge', 4901), ('huh', 4902), ('huhhuhuhuhuh', 4903), ('huhuh', 4904), ('huhuhu', 4905), ('huhuhuhuhuhuhhahaahahahauahuahuahauhauhauhauhuahauhauahuahuahauahuahuheheuehuheuehuehuehue', 4906), ('hui', 4907), ('huk', 4908), ('human', 4909), ('humble', 4910), ('humilition', 4911), ('humor', 4912), ('hundy', 4913), ('hung', 4914), ('hunt', 4915), ('hunter', 4916), ('hurry', 4917), ('hurt', 4918), ('hurts', 4919), ('hus', 4920), ('hush', 4921), ('husk', 4922), ('huska', 4923), ('huskar', 4924), ('husker', 4925), ('husky', 4926), ('huts', 4927), ('huy', 4928), ('huys', 4929), ('hv', 4930), ('hve', 4931), ('hvnt', 4932), ('hw', 4933), ('hwats', 4934), ('hwo', 4935), ('hyaha', 4936), ('hybrid', 4937), ('hype', 4938), ('hyung', 4939), ('hz', 4940), ('hzjv', 4941), ('i', 4942), ('iam', 4943), ('ibama', 4944), ('ibig', 4945), ('ibrahimovich', 4946), ('ibrb', 4947), ('ibundak', 4948), ('ic', 4949), ('icant', 4950), ('ice', 4951), ('iceblast', 4952), ('icefrog', 4953), ('iceshards', 4954), ('ich', 4955), ('icked', 4956), ('id', 4957), ('idaf', 4958), ('idc', 4959), ('idd', 4960), ('idea', 4961), ('idestroyed', 4962), ('idfkm', 4963), ('idgaf', 4964), ('idi', 4965), ('idid', 4966), ('ididnt', 4967), ('idiiot', 4968), ('idioot', 4969), ('idioot22', 4970), ('idiot', 4971), ('idiota', 4972), ('idiotd', 4973), ('idiotly', 4974), ('idioto', 4975), ('idiots', 4976), ('idiotsz', 4977), ('idite', 4978), ('iditos', 4979), ('idk', 4980), ('idnt', 4981), ('idol', 4982), ('idolaku', 4983), ('idonjt', 4984), ('idont', 4985), ('idot', 4986), ('idots', 4987), ('idrk', 4988), ('ie', 4989), ('if', 4990), ('iget', 4991), ('ignorant', 4992), ('ignore', 4993), ('ignored', 4994), ('igot', 4995), ('igra', 4996), ('igraew', 4997), ('igratj', 4998), ('iguess', 4999), ('ihave', 5000), ('ihhi', 5001), ('ihihih', 5002), ('iidot', 5003), ('iiiiiii', 5004), ('iiiiiiiiis', 5005), ('iimmma', 5006), ('ik', 5007), ('ike', 5008), ('ikkeh', 5009), ('iknow', 5010), ('ikr', 5011), ('il', 5012), ('ilencer', 5013), ('ilidan', 5014), ('ilike', 5015), ('ilknjrhg', 5016), ('ill', 5017), ('illeagle', 5018), ('illiterate', 5019), ('illu', 5020), ('illuminate', 5021), ('illuminati', 5022), ('illus', 5023), ('illusion', 5024), ('ilu', 5025), ('im', 5026), ('ima', 5027), ('imagination', 5028), ('imagine', 5029), ('imba', 5030), ('imbecle', 5031), ('imd', 5032), ('imdeed', 5033), ('imenno', 5034), ('img', 5035), ('imj', 5036), ('imjust', 5037), ('imma', 5038), ('immature', 5039), ('immigration', 5040), ('immop', 5041), ('immortal', 5042), ('immune', 5043), ('immunity', 5044), ('imo', 5045), ('impact', 5046), ('implying', 5047), ('important', 5048), ('imposible', 5049), ('impossible', 5050), ('impressed', 5051), ('impressive', 5052), ('imprisonment', 5053), ('improve', 5054), ('improves', 5055), ('imrpeganted', 5056), ('ims', 5057), ('in', 5058), ('in5k', 5059), ('ina', 5060), ('inb4', 5061), ('inboker', 5062), ('inc', 5063), ('incapable', 5064), ('incest', 5065), ('inch', 5066), ('inches', 5067), ('inchoker', 5068), ('inchs', 5069), ('incoming', 5070), ('incompetent', 5071), ('increase', 5072), ('indee3d', 5073), ('indeed', 5074), ('indian', 5075), ('indifferent', 5076), ('indonesia', 5077), ('inevtable', 5078), ('infantry', 5079), ('infidel', 5080), ('infidels', 5081), ('infiltrating', 5082), ('infinity', 5083), ('inflated', 5084), ('info', 5085), ('infringement', 5086), ('infront', 5087), ('ing', 5088), ('ingame', 5089), ('ingles', 5090), ('inhouse', 5091), ('inicio', 5092), ('inish', 5093), ('init', 5094), ('initiate', 5095), ('initiating', 5096), ('initiation', 5097), ('injoker', 5098), ('innocent', 5099), ('inoker', 5100), ('inplay', 5101), ('ins', 5102), ('insane', 5103), ('insect', 5104), ('inside', 5105), ('inspect', 5106), ('insta', 5107), ('instagram', 5108), ('instakill', 5109), ('install', 5110), ('instaloss', 5111), ('instant', 5112), ('instead', 5113), ('insted', 5114), ('insult', 5115), ('insulte', 5116), ('insulting', 5117), ('int', 5118), ('int1', 5119), ('inteligente', 5120), ('intelligient', 5121), ('intence', 5122), ('intencional', 5123), ('intense', 5124), ('intent', 5125), ('intentional', 5126), ('interested', 5127), ('internal', 5128), ('internet', 5129), ('interrupt', 5130), ('interwebs', 5131), ('into', 5132), ('intresting', 5133), ('intro', 5134), ('introboy', 5135), ('introboys', 5136), ('introgay', 5137), ('inutiles', 5138), ('inv', 5139), ('inveokr', 5140), ('investing', 5141), ('invi', 5142), ('invis', 5143), ('invisible', 5144), ('invit', 5145), ('invite', 5146), ('inviting', 5147), ('invkoer', 5148), ('invkr', 5149), ('invo', 5150), ('invoekr', 5151), ('invok', 5152), ('invoke', 5153), ('invokeeeer', 5154), ('invokeer', 5155), ('invoker', 5156), ('invokers', 5157), ('invuln', 5158), ('io', 5159), ('ioahd', 5160), ('iodiot', 5161), ('iomfg', 5162), ('ion', 5163), ('ios', 5164), ('iq', 5165), ('ir', 5166), ('iria', 5167), ('irl', 5168), ('iron', 5169), ('ironic', 5170), ('irony', 5171), ('irrelievant', 5172), ('is', 5173), ('isee', 5174), ('isi', 5175), ('isin', 5176), ('isis', 5177), ('iskal', 5178), ('isnt', 5179), ('isntant', 5180), ('isntenough', 5181), ('isp', 5182), ('israel', 5183), ('isreal', 5184), ('iss', 5185), ('isso', 5186), ('issu', 5187), ('issue', 5188), ('issues', 5189), ('isukin', 5190), ('it', 5191), ('italy', 5192), ('itch', 5193), ('itchy', 5194), ('itde', 5195), ('item', 5196), ('items', 5197), ('iten', 5198), ('itgma', 5199), ('ithought', 5200), ('itmorning', 5201), ('itms', 5202), ('itnernet', 5203), ('itny', 5204), ('itqrf', 5205), ('its', 5206), ('itscool', 5207), ('itself', 5208), ('itshis', 5209), ('itttt', 5210), ('ittttt', 5211), ('ive', 5212), ('ivno', 5213), ('iw', 5214), ('iwas', 5215), ('iwasnt', 5216), ('iy', 5217), ('iyak', 5218), ('iz', 5219), ('iza', 5220), ('izi', 5221), ('izz', 5222), ('izza', 5223), ('j', 5224), ('j00ked', 5225), ('ja', 5226), ('jaajajajajja', 5227), ('jaajja', 5228), ('jack', 5229), ('jacking', 5230), ('jag', 5231), ('jahahaha', 5232), ('jahahhaa', 5233), ('jahjahahajajaja', 5234), ('jaja', 5235), ('jajahaha', 5236), ('jajahjahaha', 5237), ('jajaj', 5238), ('jajaja', 5239), ('jajajaa', 5240), ('jajajaajajaja', 5241), ('jajajaj', 5242), ('jajajaja', 5243), ('jajajajaa', 5244), ('jajajajaj', 5245), ('jajajajaja', 5246), ('jajajajajaa', 5247), ('jajajajajaaj', 5248), ('jajajajajaja', 5249), ('jajajajja', 5250), ('jajajjaa', 5251), ('jajjaa', 5252), ('jakiro', 5253), ('jakol', 5254), ('jalo', 5255), ('jam', 5256), ('james', 5257), ('jams', 5258), ('jap', 5259), ('japan', 5260), ('jarva', 5261), ('javra', 5262), ('javseen', 5263), ('jay', 5264), ('jdem', 5265), ('je', 5266), ('jebem', 5267), ('jeder', 5268), ('jedi', 5269), ('jeez', 5270), ('jejemon', 5271), ('jembut', 5272), ('jenau', 5273), ('jeniffer', 5274), ('jennifer', 5275), ('jer', 5276), ('jerk', 5277), ('jerking', 5278), ('jerw', 5279), ('jessica', 5280), ('jesus', 5281), ('jew', 5282), ('jewish', 5283), ('jg', 5284), ('jgn', 5285), ('jgug', 5286), ('jguger', 5287), ('jhahaha', 5288), ('jhahahaha', 5289), ('jhave', 5290), ('jhj', 5291), ('jhjosfh', 5292), ('ji', 5293), ('jiahou', 5294), ('jib', 5295), ('jigg', 5296), ('jiji', 5297), ('jijijiji', 5298), ('jimbo', 5299), ('jimmy', 5300), ('jiojojojojo', 5301), ('jippers', 5302), ('jiungling', 5303), ('jizz', 5304), ('jjaja', 5305), ('jjajajaja', 5306), ('jjust', 5307), ('jk', 5308), ('jkajaja', 5309), ('jkakajajajajaj', 5310), ('jkes', 5311), ('jking', 5312), ('jks', 5313), ('job', 5314), ('jobs', 5315), ('jode', 5316), ('joga', 5317), ('jogado', 5318), ('jogar', 5319), ('jogaram', 5320), ('jogo', 5321), ('johan', 5322), ('john', 5323), ('join', 5324), ('joining', 5325), ('jojojo', 5326), ('jojojoj', 5327), ('jojojojo', 5328), ('jojojojoj', 5329), ('jojojojojo', 5330), ('jojojojojoj', 5331), ('jojojojojojo', 5332), ('jojojojojojoj', 5333), ('jojojojojojojo', 5334), ('jojojojojojojojojo', 5335), ('jojoo', 5336), ('joke', 5337), ('jokeing', 5338), ('joker', 5339), ('jokers', 5340), ('jokes', 5341), ('joking', 5342), ('joob', 5343), ('jooked', 5344), ('jooooooooooooooooooooohn', 5345), ('jopjojojojoj', 5346), ('jpg', 5347), ('jr', 5348), ('js', 5349), ('jsagdhjkasgd', 5350), ('jst', 5351), ('jstbreak', 5352), ('jsut', 5353), ('ju', 5354), ('juas', 5355), ('jud', 5356), ('judge', 5357), ('jug', 5358), ('jugaer', 5359), ('jugar', 5360), ('juger', 5361), ('jugg', 5362), ('jugger', 5363), ('juggernaut', 5364), ('juggeru', 5365), ('juggerw', 5366), ('juggn', 5367), ('juggs', 5368), ('jugs', 5369), ('juguer', 5370), ('juijked', 5371), ('juke', 5372), ('juked', 5373), ('jukes', 5374), ('juking', 5375), ('jukt', 5376), ('jummy', 5377), ('jump', 5378), ('jumped', 5379), ('jumping', 5380), ('jung', 5381), ('jungla', 5382), ('jungle', 5383), ('jungler', 5384), ('junglers', 5385), ('jungles', 5386), ('jungling', 5387), ('junior', 5388), ('junkie', 5389), ('junle', 5390), ('jupuk', 5391), ('jurkoff', 5392), ('jus', 5393), ('jussssssssssssst', 5394), ('just', 5395), ('just2kthings', 5396), ('just3kthinggs', 5397), ('justice', 5398), ('jut', 5399), ('juz', 5400), ('jz', 5401), ('k', 5402), ('ka', 5403), ('kaaaaaaaammmmmmmmmmeeeeeeeeeeeeee', 5404), ('kaapa', 5405), ('kaayo', 5406), ('kabobobo', 5407), ('kahh', 5408), ('kak', 5409), ('kaka', 5410), ('kakakak', 5411), ('kala', 5412), ('kale', 5413), ('kalemn', 5414), ('kamazeeee', 5415), ('kamen', 5416), ('kamikaze', 5417), ('kampi', 5418), ('kamu', 5419), ('kangkong', 5420), ('kantot', 5421), ('kapa', 5422), ('kapaa', 5423), ('kapap', 5424), ('kappa', 5425), ('kappapride', 5426), ('kappaross', 5427), ('karam', 5428), ('kardel', 5429), ('karl', 5430), ('karma', 5431), ('karo4e', 5432), ('kasi', 5433), ('kasrma', 5434), ('kat', 5435), ('kate', 5436), ('katka', 5437), ('katkaaa', 5438), ('katkas', 5439), ('katkovich', 5440), ('katoska', 5441), ('katsamba', 5442), ('katushka', 5443), ('kau', 5444), ('kaw', 5445), ('kawawa', 5446), ('kawkawkawkwakaw', 5447), ('kawo', 5448), ('kay', 5449), ('kaya', 5450), ('kayo', 5451), ('kayyyy', 5452), ('kda', 5453), ('kdas', 5454), ('kdddddddddddddd', 5455), ('kden', 5456), ('ke', 5457), ('kee', 5458), ('keep', 5459), ('keeper', 5460), ('keeping', 5461), ('keepo', 5462), ('keeps', 5463), ('kein', 5464), ('kek', 5465), ('kekee', 5466), ('kekke', 5467), ('kekn', 5468), ('keper', 5469), ('keping', 5470), ('kepp', 5471), ('kept', 5472), ('key', 5473), ('keyboard', 5474), ('keybord', 5475), ('kfc', 5476), ('kfucking', 5477), ('kh', 5478), ('kha', 5479), ('khaana', 5480), ('kick', 5481), ('kicked', 5482), ('kid', 5483), ('kidding', 5484), ('kiddo', 5485), ('kidiing', 5486), ('kidn', 5487), ('kids', 5488), ('kidz', 5489), ('kil', 5490), ('kill', 5491), ('killed', 5492), ('killeld', 5493), ('killer', 5494), ('killing', 5495), ('killl', 5496), ('kills', 5497), ('killsl', 5498), ('killsteals', 5499), ('kin', 5500), ('kinards', 5501), ('kind', 5502), ('kinda', 5503), ('kindly', 5504), ('king', 5505), ('kingg', 5506), ('kingina', 5507), ('kinh', 5508), ('kinkaa', 5509), ('kinse', 5510), ('kiolll', 5511), ('kiss', 5512), ('kissing', 5513), ('kit', 5514), ('kitchen', 5515), ('kite', 5516), ('kited', 5517), ('kiter', 5518), ('kiting', 5519), ('kitten', 5520), ('kitty', 5521), ('kjikl', 5522), ('kjkustu', 5523), ('kk', 5524), ('kka', 5525), ('kkg', 5526), ('kkk', 5527), ('kkkk', 5528), ('kkkkk', 5529), ('kkkkkkkkk', 5530), ('kkkkkkkkkkkkk', 5531), ('kkkkkkkkkkkkkkkk', 5532), ('kkkkkkkkkkkkkkkkkk', 5533), ('kkkkkkkkkkkkkkkkkkkkk', 5534), ('kl', 5535), ('klamamaa', 5536), ('klast', 5537), ('klikl', 5538), ('klng', 5539), ('klnown', 5540), ('klog', 5541), ('klool', 5542), ('kms', 5543), ('knew', 5544), ('knght', 5545), ('knife', 5546), ('knight', 5547), ('knnow', 5548), ('kno', 5549), ('knos', 5550), ('know', 5551), ('knowing', 5552), ('known', 5553), ('knows', 5554), ('knw', 5555), ('ko', 5556), ('kod', 5557), ('kogut', 5558), ('kok', 5559), ('kol9y', 5560), ('kolakaoaw', 5561), ('kombat', 5562), ('kompor', 5563), ('konek', 5564), ('kontol', 5565), ('kontolan', 5566), ('kontool', 5567), ('kontr', 5568), ('konw', 5569), ('korea', 5570), ('korean', 5571), ('kos', 5572), ('kotl', 5573), ('kotol', 5574), ('kowqnheui49jqwmke', 5575), ('kpd', 5576), ('kple', 5577), ('kpop', 5578), ('kr', 5579), ('kraaaaaay', 5580), ('kraken', 5581), ('kramer', 5582), ('krben', 5583), ('krch', 5584), ('kreygasm', 5585), ('kriki', 5586), ('kristen', 5587), ('kritami', 5588), ('krits', 5589), ('kriv', 5590), ('krome', 5591), ('ks', 5592), ('ksd', 5593), ('kser', 5594), ('ksing', 5595), ('kso', 5596), ('kstate', 5597), ('kthx', 5598), ('ktukan', 5599), ('ku', 5600), ('kukita', 5601), ('kuma', 5602), ('kunka', 5603), ('kunkaa', 5604), ('kunkha', 5605), ('kunkka', 5606), ('kunkkaaaa', 5607), ('kunkkar', 5608), ('kunnkka', 5609), ('kunt', 5610), ('kunts', 5611), ('kupal', 5612), ('kuro', 5613), ('kurwo', 5614), ('kutn', 5615), ('kuunka', 5616), ('kw', 5617), ('kwkwkw', 5618), ('kwokwowk', 5619), ('kwowk', 5620), ('ky', 5621), ('kycha', 5622), ('kys', 5623), ('kyxy', 5624), ('l', 5625), ('l0l', 5626), ('l2p', 5627), ('la', 5628), ('laah', 5629), ('labas', 5630), ('labert', 5631), ('lachi', 5632), ('lack', 5633), ('lacky', 5634), ('lacras', 5635), ('lacson', 5636), ('lacueeeeek', 5637), ('lad', 5638), ('ladja', 5639), ('lads', 5640), ('lady', 5641), ('lafe', 5642), ('lag', 5643), ('laged', 5644), ('lagg', 5645), ('lagged', 5646), ('laggg', 5647), ('lagggggggg', 5648), ('lagggggggggggg', 5649), ('laggggggggggggg', 5650), ('laggggggggggggggggggggggg', 5651), ('lagggggggggggggggggggggggggggggggggggggggggggggggggg', 5652), ('laggin', 5653), ('lagging', 5654), ('laggs', 5655), ('lagh', 5656), ('lagi', 5657), ('lags', 5658), ('laguhing', 5659), ('lah', 5660), ('lai', 5661), ('laid', 5662), ('laik', 5663), ('lal', 5664), ('lala', 5665), ('lalala', 5666), ('lalalal', 5667), ('lalalala', 5668), ('lalalalal', 5669), ('lalalla', 5670), ('lame', 5671), ('lami', 5672), ('lamo', 5673), ('lan', 5674), ('lanay', 5675), ('lanaya', 5676), ('lanc', 5677), ('lancau', 5678), ('lancer', 5679), ('land', 5680), ('landed', 5681), ('landing', 5682), ('lane', 5683), ('laned', 5684), ('laner', 5685), ('lanes', 5686), ('lang', 5687), ('language', 5688), ('laning', 5689), ('lanjiao', 5690), ('lanka', 5691), ('lanning', 5692), ('lapo', 5693), ('lapsap', 5694), ('laptop', 5695), ('lar', 5696), ('lars', 5697), ('larth', 5698), ('larvas', 5699), ('las', 5700), ('laspelaps', 5701), ('lasst', 5702), ('last', 5703), ('laste', 5704), ('lasted', 5705), ('lasthitting', 5706), ('late', 5707), ('lategame', 5708), ('lately', 5709), ('later', 5710), ('lau', 5711), ('laugh', 5712), ('laughed', 5713), ('laughing', 5714), ('laughs', 5715), ('laught', 5716), ('launched', 5717), ('lawannya', 5718), ('lawl', 5719), ('lawrence', 5720), ('lay', 5721), ('lazy', 5722), ('lb', 5723), ('lc', 5724), ('lcvl', 5725), ('ld', 5726), ('le', 5727), ('leader', 5728), ('leaderhsip', 5729), ('leading', 5730), ('league', 5731), ('leagueoflegends', 5732), ('leak', 5733), ('leap', 5734), ('lear', 5735), ('learn', 5736), ('learned', 5737), ('learner', 5738), ('leart', 5739), ('lease', 5740), ('leash', 5741), ('least', 5742), ('leat', 5743), ('leav', 5744), ('leave', 5745), ('leaved', 5746), ('leaveee', 5747), ('leaveing', 5748), ('leaves', 5749), ('leaving', 5750), ('leddit', 5751), ('lee', 5752), ('leech', 5753), ('leeching', 5754), ('leet', 5755), ('left', 5756), ('leg', 5757), ('legal', 5758), ('legend', 5759), ('legendary', 5760), ('legends', 5761), ('leggo', 5762), ('legio', 5763), ('legion', 5764), ('legionn', 5765), ('legit', 5766), ('leglas', 5767), ('lego', 5768), ('legoin', 5769), ('legolas', 5770), ('leguon', 5771), ('leh', 5772), ('lei', 5773), ('leichfanz', 5774), ('leki', 5775), ('lekt', 5776), ('lel', 5777), ('lel2', 5778), ('lele', 5779), ('lelele', 5780), ('lelelel', 5781), ('lelelelelel', 5782), ('lell', 5783), ('lels', 5784), ('lelz', 5785), ('lemon', 5786), ('lencer', 5787), ('lennon', 5788), ('leon', 5789), ('leong', 5790), ('leonidas', 5791), ('ler', 5792), ('les', 5793), ('lesh', 5794), ('leshrack', 5795), ('leslie', 5796), ('less', 5797), ('lesson', 5798), ('lessons', 5799), ('lesss', 5800), ('lest', 5801), ('lesu', 5802), ('let', 5803), ('letas', 5804), ('lets', 5805), ('letting', 5806), ('letz', 5807), ('level', 5808), ('levels', 5809), ('levi', 5810), ('lewl', 5811), ('lf', 5812), ('lfd', 5813), ('lfdyj', 5814), ('lfmao', 5815), ('lg', 5816), ('lga', 5817), ('lghf', 5818), ('lh', 5819), ('lhawlgal', 5820), ('liaaar', 5821), ('lian', 5822), ('liao', 5823), ('liar', 5824), ('liat', 5825), ('lica', 5826), ('lich', 5827), ('liched', 5828), ('lichg', 5829), ('licj', 5830), ('lick', 5831), ('licky', 5832), ('lie', 5833), ('lied', 5834), ('liek', 5835), ('lier', 5836), ('lies', 5837), ('lieslie', 5838), ('lif', 5839), ('life', 5840), ('lifer', 5841), ('lifes', 5842), ('lifesteal', 5843), ('lifestealer', 5844), ('lift', 5845), ('lifted', 5846), ('ligghhter', 5847), ('lighning', 5848), ('light', 5849), ('lightr', 5850), ('lika', 5851), ('like', 5852), ('liked', 5853), ('likely', 5854), ('likens', 5855), ('likes', 5856), ('lil', 5857), ('lile', 5858), ('limb', 5859), ('lina', 5860), ('linas', 5861), ('linavs', 5862), ('linch', 5863), ('lindo', 5864), ('line', 5865), ('lineup', 5866), ('ling', 5867), ('link', 5868), ('linken', 5869), ('linkens', 5870), ('linking', 5871), ('links', 5872), ('linte', 5873), ('lioch', 5874), ('liol', 5875), ('liomn', 5876), ('lion', 5877), ('liquid', 5878), ('lisaj', 5879), ('list', 5880), ('listen', 5881), ('listening', 5882), ('listo', 5883), ('litaaaar', 5884), ('literally', 5885), ('little', 5886), ('liv', 5887), ('live', 5888), ('lived', 5889), ('lives', 5890), ('livewwr', 5891), ('living', 5892), ('lixo', 5893), ('lixos', 5894), ('lkala', 5895), ('lke', 5896), ('lkixo', 5897), ('lkmao', 5898), ('lkol', 5899), ('ll', 5900), ('llamas', 5901), ('llater', 5902), ('llevaran', 5903), ('llife', 5904), ('llllllllllllll', 5905), ('lllo', 5906), ('lllol', 5907), ('llololol', 5908), ('lloron', 5909), ('llove', 5910), ('lm', 5911), ('lma', 5912), ('lmai', 5913), ('lmao', 5914), ('lmaooo', 5915), ('lmaoooo', 5916), ('lmfao', 5917), ('lmfaofmowemfoawemof', 5918), ('lmfaso', 5919), ('lmoa', 5920), ('lmofa', 5921), ('lmol', 5922), ('lo', 5923), ('lo000iising', 5924), ('loa', 5925), ('load', 5926), ('loading', 5927), ('loan', 5928), ('lob', 5929), ('lobby', 5930), ('lobster', 5931), ('lock', 5932), ('locket', 5933), ('locking', 5934), ('loda', 5935), ('loes', 5936), ('log', 5937), ('loggin', 5938), ('logging', 5939), ('logi', 5940), ('logic', 5941), ('login', 5942), ('logo', 5943), ('loh', 5944), ('loko', 5945), ('lol', 5946), ('lol7', 5947), ('lolc', 5948), ('lold', 5949), ('lolk', 5950), ('loll', 5951), ('lolll', 5952), ('lolllllll', 5953), ('lolllllllll', 5954), ('lollllllllllllllllll', 5955), ('lolo', 5956), ('lolol', 5957), ('lololokol', 5958), ('lololol', 5959), ('lolololol', 5960), ('lololololl', 5961), ('lolololololo', 5962), ('lolqop', 5963), ('lols', 5964), ('lolwut', 5965), ('lolx', 5966), ('lolz', 5967), ('lolzzz', 5968), ('lomaite', 5969), ('lon', 5970), ('lone', 5971), ('lonedruid', 5972), ('lonely', 5973), ('long', 5974), ('longer', 5975), ('longest', 5976), ('loo', 5977), ('look', 5978), ('looked', 5979), ('looking', 5980), ('looks', 5981), ('lool', 5982), ('loomis', 5983), ('loool', 5984), ('looool', 5985), ('looooollll', 5986), ('looooool', 5987), ('loooooolll', 5988), ('loooooool', 5989), ('loooooooool', 5990), ('looooooooooool', 5991), ('loooooooooooooolllllllll', 5992), ('loooooooooooooooool', 5993), ('loooooooooooooooooool', 5994), ('looost', 5995), ('loose', 5996), ('looser', 5997), ('loosing', 5998), ('looy', 5999), ('looya', 6000), ('loozer', 6001), ('lopl', 6002), ('loptop', 6003), ('lorc', 6004), ('lord', 6005), ('los', 6006), ('lose', 6007), ('loseing', 6008), ('loser', 6009), ('losers', 6010), ('loses', 6011), ('losing', 6012), ('loss', 6013), ('losses', 6014), ('lost', 6015), ('loster', 6016), ('losting', 6017), ('lot', 6018), ('lotar', 6019), ('lothar', 6020), ('lothars', 6021), ('loto', 6022), ('lots', 6023), ('lotte', 6024), ('lotus', 6025), ('lou', 6026), ('loudest', 6027), ('louy', 6028), ('lov', 6029), ('love', 6030), ('loved', 6031), ('lovely', 6032), ('lover', 6033), ('loves', 6034), ('loving', 6035), ('low', 6036), ('lowara', 6037), ('lower', 6038), ('lowest', 6039), ('lowl', 6040), ('lowprio', 6041), ('lows', 6042), ('lowskill', 6043), ('lox', 6044), ('lp', 6045), ('ls', 6046), ('lsi', 6047), ('lsoe', 6048), ('lsot', 6049), ('lssing', 6050), ('lt', 6051), ('lti', 6052), ('ltr', 6053), ('lu', 6054), ('lucan', 6055), ('luchshii', 6056), ('lucies', 6057), ('luck', 6058), ('lucker', 6059), ('luckiest', 6060), ('luckman', 6061), ('lucky', 6062), ('luckyman', 6063), ('luckymen', 6064), ('lucu', 6065), ('lukcy', 6066), ('lul', 6067), ('lulz', 6068), ('lumabas', 6069), ('luna', 6070), ('lunatic', 6071), ('lunatics', 6072), ('lupam', 6073), ('lur', 6074), ('luser', 6075), ('lusting', 6076), ('luv', 6077), ('lv', 6078), ('lvl', 6079), ('lvl2', 6080), ('lvl4', 6081), ('lvl7', 6082), ('lvls', 6083), ('lvoe', 6084), ('lycab', 6085), ('lycan', 6086), ('lycane', 6087), ('lyfe', 6088), ('lyin', 6089), ('lying', 6090), ('lz', 6091), ('m', 6092), ('m7', 6093), ('m8', 6094), ('m888', 6095), ('m9', 6096), ('ma', 6097), ('maaaaadd', 6098), ('maap', 6099), ('mabisinakun', 6100), ('maby', 6101), ('macd', 6102), ('machine', 6103), ('machismo', 6104), ('macro', 6105), ('macropyre', 6106), ('mad', 6107), ('madafaka', 6108), ('madafucka', 6109), ('madara', 6110), ('madcuzbad', 6111), ('madda', 6112), ('maddest', 6113), ('made', 6114), ('madness', 6115), ('mads', 6116), ('mael', 6117), ('maen', 6118), ('mag', 6119), ('mage', 6120), ('mages', 6121), ('magi', 6122), ('magic', 6123), ('magina', 6124), ('magn', 6125), ('magnius', 6126), ('magno', 6127), ('magnus', 6128), ('magnuse', 6129), ('mah', 6130), ('mahhhhhhhhhhhhhhhhhhhhhhhh', 6131), ('mahirap', 6132), ('mai', 6133), ('maibe', 6134), ('maidan', 6135), ('maiden', 6136), ('mail', 6137), ('main', 6138), ('mainis', 6139), ('mainly', 6140), ('mairo', 6141), ('maiy', 6142), ('major', 6143), ('majorino', 6144), ('majors', 6145), ('makan', 6146), ('make', 6147), ('makes', 6148), ('making', 6149), ('makunat', 6150), ('malay', 6151), ('malaysia', 6152), ('malaysian', 6153), ('maldito', 6154), ('maledict', 6155), ('malenk', 6156), ('males', 6157), ('malev', 6158), ('maleware', 6159), ('mall', 6160), ('malstrosm', 6161), ('mam', 6162), ('mama', 6163), ('mams', 6164), ('mamu', 6165), ('man', 6166), ('mana', 6167), ('managed', 6168), ('manay', 6169), ('manfight', 6170), ('mang', 6171), ('manga', 6172), ('mango', 6173), ('mangoes', 6174), ('mangs', 6175), ('manner', 6176), ('manners', 6177), ('manos', 6178), ('mans', 6179), ('manta', 6180), ('mantaed', 6181), ('mantan', 6182), ('many', 6183), ('manyu', 6184), ('mao', 6185), ('map', 6186), ('mapa', 6187), ('maphack', 6188), ('mapxak', 6189), ('marahin', 6190), ('marica', 6191), ('mariks', 6192), ('mario', 6193), ('mark', 6194), ('marke', 6195), ('marko', 6196), ('marry', 6197), ('martha', 6198), ('marvel', 6199), ('mas', 6200), ('masakit', 6201), ('masayoshi', 6202), ('masochist', 6203), ('mass', 6204), ('massively', 6205), ('masta', 6206), ('master', 6207), ('mastered', 6208), ('masters', 6209), ('masturbate', 6210), ('masturbating', 6211), ('masturbation', 6212), ('masturvbate', 6213), ('mat4es', 6214), ('mata', 6215), ('matar', 6216), ('match', 6217), ('matched', 6218), ('matches', 6219), ('matchmaking', 6220), ('matchup', 6221), ('mate', 6222), ('mates', 6223), ('math', 6224), ('matrix', 6225), ('matter', 6226), ('matters', 6227), ('maube', 6228), ('max', 6229), ('maxed', 6230), ('maxine', 6231), ('maxing', 6232), ('may', 6233), ('maybe', 6234), ('mayra', 6235), ('mazing', 6236), ('mb', 6237), ('mbi', 6238), ('mby', 6239), ('mc', 6240), ('mcdo', 6241), ('mcdonald', 6242), ('mds', 6243), ('me', 6244), ('me2', 6245), ('mean', 6246), ('meaning', 6247), ('means', 6248), ('meant', 6249), ('meanwhile', 6250), ('measure', 6251), ('meat', 6252), ('meatballs', 6253), ('meca', 6254), ('mech', 6255), ('mechanic', 6256), ('mecry', 6257), ('med', 6258), ('medal', 6259), ('media', 6260), ('medicine', 6261), ('medio', 6262), ('medium', 6263), ('medu', 6264), ('medusa', 6265), ('medusaaaaaa', 6266), ('meduza', 6267), ('meed', 6268), ('meeee', 6269), ('meeeeeeee', 6270), ('meelee', 6271), ('meepo', 6272), ('meeporino', 6273), ('meepos', 6274), ('meepwn', 6275), ('meet', 6276), ('meeting', 6277), ('mega', 6278), ('megas', 6279), ('meh', 6280), ('mehr', 6281), ('mei', 6282), ('meister', 6283), ('mek', 6284), ('meka', 6285), ('mekans', 6286), ('meks', 6287), ('melbourne', 6288), ('meld', 6289), ('mele', 6290), ('melee', 6291), ('melt', 6292), ('mema', 6293), ('memba', 6294), ('members', 6295), ('meme', 6296), ('memeing', 6297), ('memek', 6298), ('memememememem', 6299), ('memes', 6300), ('memorandum', 6301), ('men', 6302), ('mena', 6303), ('mens', 6304), ('ment', 6305), ('mental', 6306), ('mentality', 6307), ('mentally', 6308), ('mention', 6309), ('mentioned', 6310), ('menu', 6311), ('menya', 6312), ('meow', 6313), ('mepo', 6314), ('mepoo', 6315), ('meppen', 6316), ('meppo', 6317), ('mer', 6318), ('meracle', 6319), ('merchant', 6320), ('mercy', 6321), ('mere', 6322), ('merica', 6323), ('merry', 6324), ('mes', 6325), ('mess', 6326), ('message', 6327), ('messed', 6328), ('messi', 6329), ('messing', 6330), ('mestik', 6331), ('met', 6332), ('meta', 6333), ('metaaaaaaaaaaaaaaaaaaaaaaaa', 6334), ('metapicker', 6335), ('metaplayer', 6336), ('metworth', 6337), ('meu', 6338), ('mev', 6339), ('mever', 6340), ('mexican', 6341), ('mexicans', 6342), ('mexico', 6343), ('mf', 6344), ('mfs', 6345), ('mg', 6346), ('mga', 6347), ('mge', 6348), ('mh', 6349), ('mhm', 6350), ('mhmm', 6351), ('mi', 6352), ('mi6kiiiiiiiiiiiiiii', 6353), ('mib', 6354), ('mic', 6355), ('mich', 6356), ('michael', 6357), ('micro', 6358), ('microing', 6359), ('mid', 6360), ('mida', 6361), ('midas', 6362), ('midases', 6363), ('midasom', 6364), ('midd', 6365), ('middddddddddddddddddd', 6366), ('midder', 6367), ('middle', 6368), ('mide', 6369), ('mider', 6370), ('midgame', 6371), ('midl', 6372), ('midlane', 6373), ('midle', 6374), ('mids', 6375), ('midtime', 6376), ('mierda', 6377), ('mierdas', 6378), ('mif', 6379), ('might', 6380), ('mightve', 6381), ('mighty', 6382), ('migo', 6383), ('miis', 6384), ('mike', 6385), ('mikes', 6386), ('mil', 6387), ('mile', 6388), ('miles', 6389), ('mili', 6390), ('milk', 6391), ('million', 6392), ('min', 6393), ('min3', 6394), ('mind', 6395), ('minded', 6396), ('mine', 6397), ('mines', 6398), ('mineski', 6399), ('minets', 6400), ('minglee', 6401), ('mini', 6402), ('minimap', 6403), ('minions', 6404), ('minit', 6405), ('mins', 6406), ('mint', 6407), ('mints', 6408), ('minus', 6409), ('minut', 6410), ('minute', 6411), ('minutes', 6412), ('mir', 6413), ('mira', 6414), ('miracle', 6415), ('miran', 6416), ('mirana', 6417), ('mirna', 6418), ('mis', 6419), ('misclick', 6420), ('misclickded', 6421), ('misha', 6422), ('misklik', 6423), ('misn', 6424), ('misplayed', 6425), ('miss', 6426), ('missclick', 6427), ('missclicked', 6428), ('missed', 6429), ('misses', 6430), ('missile', 6431), ('missing', 6432), ('mission', 6433), ('missis', 6434), ('missle', 6435), ('misss', 6436), ('misstype', 6437), ('mistake', 6438), ('mistakes', 6439), ('mitches', 6440), ('miunte', 6441), ('miurana', 6442), ('mjollnir', 6443), ('mjolnir', 6444), ('mjst', 6445), ('mk', 6446), ('mkb', 6447), ('mkm', 6448), ('mlg', 6449), ('mlgranger', 6450), ('mlk', 6451), ('mm', 6452), ('mmchike', 6453), ('mmm', 6454), ('mmmm', 6455), ('mmmmm', 6456), ('mmmmmmmmm', 6457), ('mmmmmmmmmmmmmmmmmmmmmmmmmmmohmygod', 6458), ('mmmr', 6459), ('mmore', 6460), ('mmr', 6461), ('mmrhunter', 6462), ('mmrmrr', 6463), ('mmrs', 6464), ('mmrt', 6465), ('mmuted', 6466), ('mmy', 6467), ('mn', 6468), ('mnay', 6469), ('mne', 6470), ('mnmay', 6471), ('mnore', 6472), ('mnow', 6473), ('mns', 6474), ('mnt', 6475), ('mnuted', 6476), ('mnvp', 6477), ('mny', 6478), ('mo', 6479), ('moan', 6480), ('moar', 6481), ('mobile', 6482), ('mobkey', 6483), ('mobo', 6484), ('moce', 6485), ('modafuka', 6486), ('mode', 6487), ('modem', 6488), ('mofo', 6489), ('mofoz', 6490), ('mofucker', 6491), ('mog', 6492), ('moha', 6493), ('mokiproblems', 6494), ('mole', 6495), ('moly', 6496), ('mom', 6497), ('moment', 6498), ('moments', 6499), ('mommas', 6500), ('momment', 6501), ('momo', 6502), ('moms', 6503), ('mon', 6504), ('money', 6505), ('moneys', 6506), ('moneyyy', 6507), ('mong', 6508), ('monger', 6509), ('mongol', 6510), ('mongoloi', 6511), ('mongoloid', 6512), ('mongoloids', 6513), ('monkey', 6514), ('monkeys', 6515), ('monkjey', 6516), ('monney', 6517), ('monogolian', 6518), ('month', 6519), ('months', 6520), ('mood', 6521), ('moofta', 6522), ('moon', 6523), ('moonshard', 6524), ('moonwalk', 6525), ('moooooh', 6526), ('mooooooh', 6527), ('moooooooh', 6528), ('moore', 6529), ('mopl', 6530), ('mor', 6531), ('moral', 6532), ('morale', 6533), ('morality', 6534), ('more', 6535), ('more2', 6536), ('moreee', 6537), ('morel', 6538), ('moreplz', 6539), ('morf', 6540), ('morfin', 6541), ('morghulis', 6542), ('mormo', 6543), ('mormon', 6544), ('morning', 6545), ('moron', 6546), ('morons', 6547), ('morp', 6548), ('morph', 6549), ('morphg', 6550), ('morphling', 6551), ('morphlings', 6552), ('morps', 6553), ('morreu', 6554), ('mortal', 6555), ('mortred', 6556), ('morty', 6557), ('most', 6558), ('mostimpotant', 6559), ('mostly', 6560), ('motehr', 6561), ('mother', 6562), ('mothercukeer', 6563), ('motherducker', 6564), ('motherf', 6565), ('motherfucekrew', 6566), ('motherfucker', 6567), ('motherfuckers', 6568), ('motherfucking', 6569), ('motherfuker', 6570), ('motherfuking', 6571), ('motherless', 6572), ('mothers', 6573), ('motion', 6574), ('motivation', 6575), ('motomu', 6576), ('motrher', 6577), ('mought', 6578), ('mouise', 6579), ('mous', 6580), ('mouse', 6581), ('moutch', 6582), ('mouth', 6583), ('move', 6584), ('moved', 6585), ('moves', 6586), ('movespeed', 6587), ('movie', 6588), ('moving', 6589), ('movistar', 6590), ('mppb', 6591), ('mq', 6592), ('mr', 6593), ('mrd', 6594), ('mrda', 6595), ('mrdas', 6596), ('mroe', 6597), ('mrr', 6598), ('mrrr', 6599), ('mrrs', 6600), ('mrs', 6601), ('ms', 6602), ('msg', 6603), ('msgd', 6604), ('msged', 6605), ('msging', 6606), ('mt', 6607), ('mthat', 6608), ('mther', 6609), ('mto', 6610), ('mtoher', 6611), ('mtued', 6612), ('mu', 6613), ('muah', 6614), ('muahagahahahahaah', 6615), ('much', 6616), ('mucho', 6617), ('mud', 6618), ('mudda', 6619), ('muere', 6620), ('muereeeeeeeeeeeee', 6621), ('mueren', 6622), ('muerto', 6623), ('muito', 6624), ('mujm', 6625), ('multiplayer', 6626), ('multiple', 6627), ('mum', 6628), ('mumbarak', 6629), ('mummy', 6630), ('mummys', 6631), ('mums', 6632), ('muna', 6633), ('mundo', 6634), ('munic', 6635), ('munny', 6636), ('murdered', 6637), ('murica', 6638), ('murmuring', 6639), ('murtazaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', 6640), ('musa', 6641), ('mushe', 6642), ('mushi', 6643), ('muslim', 6644), ('musor', 6645), ('musorka', 6646), ('must', 6647), ('musta', 6648), ('mustve', 6649), ('mut', 6650), ('mute', 6651), ('muted', 6652), ('mutelist', 6653), ('mutes', 6654), ('muthafacka', 6655), ('muting', 6656), ('muy', 6657), ('mvp', 6658), ('my', 6659), ('myanmar', 6660), ('myh', 6661), ('mylife', 6662), ('mysefl', 6663), ('myself', 6664), ('myslef', 6665), ('mythical', 6666), ('n', 6667), ('n0t', 6668), ('n1', 6669), ('na', 6670), ('naa', 6671), ('naaaaameeee', 6672), ('naab', 6673), ('naah', 6674), ('naationality', 6675), ('nab', 6676), ('nabbed', 6677), ('nabernaut', 6678), ('nabs', 6679), ('nad', 6680), ('nag', 6681), ('naga', 6682), ('nagbayag', 6683), ('nagelfar', 6684), ('nagga', 6685), ('nah', 6686), ('nahhhh', 6687), ('nahui', 6688), ('nahuj', 6689), ('nahuy', 6690), ('nahyi', 6691), ('nail', 6692), ('nailed', 6693), ('naix', 6694), ('nakaaaal', 6695), ('naked', 6696), ('nalang', 6697), ('naman', 6698), ('namaste', 6699), ('name', 6700), ('named', 6701), ('namekaew', 6702), ('nami', 6703), ('namin', 6704), ('namo', 6705), ('nanashi', 6706), ('nao', 6707), ('nap', 6708), ('napakatanga', 6709), ('napisal', 6710), ('napkin', 6711), ('naps', 6712), ('napusal', 6713), ('narco', 6714), ('narutard', 6715), ('nas', 6716), ('nasad', 6717), ('nasmotrelis', 6718), ('nasty', 6719), ('natalie', 6720), ('nate', 6721), ('national', 6722), ('natural', 6723), ('nature', 6724), ('natures', 6725), ('naughty', 6726), ('navi', 6727), ('naw', 6728), ('naxren', 6729), ('naxui', 6730), ('naxyu', 6731), ('nazi', 6732), ('nb', 6733), ('nbedroom', 6734), ('nbever', 6735), ('nbice', 6736), ('nboob', 6737), ('nbooob', 6738), ('nc', 6739), ('nc1', 6740), ('ncie', 6741), ('ne', 6742), ('neaaaaaaaar', 6743), ('neah', 6744), ('near', 6745), ('nearly', 6746), ('neat', 6747), ('neba', 6748), ('neck', 6749), ('neco', 6750), ('necor', 6751), ('necro', 6752), ('necrolyte', 6753), ('necroo', 6754), ('necrophile', 6755), ('necrophos', 6756), ('necto', 6757), ('nee', 6758), ('need', 6759), ('needed', 6760), ('needs', 6761), ('negative', 6762), ('negger', 6763), ('nego', 6764), ('negro', 6765), ('negros', 6766), ('neh', 6767), ('neiter', 6768), ('neither', 6769), ('nekked', 6770), ('nenado', 6771), ('neo', 6772), ('nerd', 6773), ('nerds', 6774), ('nerf', 6775), ('nervous', 6776), ('ness', 6777), ('nessa', 6778), ('nest', 6779), ('net', 6780), ('network', 6781), ('networth', 6782), ('neutral', 6783), ('neutrals', 6784), ('neuts', 6785), ('neva', 6786), ('never', 6787), ('nevermind', 6788), ('nevermore', 6789), ('nevermoreland', 6790), ('neverr', 6791), ('nevertheless', 6792), ('new', 6793), ('newb', 6794), ('newbie', 6795), ('newbs', 6796), ('nex', 6797), ('nexrt', 6798), ('next', 6799), ('nfr', 6800), ('ng', 6801), ('nganaaaaaa', 6802), ('ngay', 6803), ('ngenge', 6804), ('ngewes', 6805), ('ngiga', 6806), ('ngon', 6807), ('nh', 6808), ('nha', 6809), ('nhanh', 6810), ('nhice', 6811), ('ni', 6812), ('ni4ego', 6813), ('nic2', 6814), ('nica', 6815), ('nice', 6816), ('nice1', 6817), ('nicea', 6818), ('nicedoom', 6819), ('nicee', 6820), ('niceeeeeeee', 6821), ('nicely', 6822), ('niceu', 6823), ('nicht', 6824), ('nickname', 6825), ('nicveee', 6826), ('nid', 6827), ('nien', 6828), ('niet', 6829), ('niga', 6830), ('nigga', 6831), ('nigga1', 6832), ('niggas', 6833), ('niggaz', 6834), ('nigger', 6835), ('niggers', 6836), ('niggu', 6837), ('nigguh', 6838), ('niggumus', 6839), ('night', 6840), ('nightmare', 6841), ('nightmares', 6842), ('nightr', 6843), ('nigsd', 6844), ('nikolaiv', 6845), ('nin', 6846), ('nina', 6847), ('ning', 6848), ('ninja', 6849), ('nippon', 6850), ('nito', 6851), ('nityo', 6852), ('nivel', 6853), ('niyo', 6854), ('njeng', 6855), ('nko', 6856), ('nkow', 6857), ('nlob', 6858), ('nm', 6859), ('nman', 6860), ('nme', 6861), ('nmn', 6862), ('nmooob', 6863), ('nmore', 6864), ('nneed', 6865), ('nnnnnnnnnnnnnnnn', 6866), ('nno', 6867), ('nnono', 6868), ('nnt', 6869), ('no', 6870), ('no1', 6871), ('nob', 6872), ('nobbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', 6873), ('nobo', 6874), ('nobody', 6875), ('nobos', 6876), ('nobss', 6877), ('noce', 6878), ('noes', 6879), ('noi', 6880), ('noice', 6881), ('noioo', 6882), ('noise', 6883), ('noisy', 6884), ('noithign', 6885), ('noiw', 6886), ('nojeira', 6887), ('nol', 6888), ('nolag', 6889), ('nom', 6890), ('noma', 6891), ('nomas', 6892), ('nomnomlion', 6893), ('non', 6894), ('noncontributory', 6895), ('none', 6896), ('nono', 6897), ('nonononono', 6898), ('nonsense', 6899), ('noo', 6900), ('noob', 6901), ('noobbb', 6902), ('noobest', 6903), ('noobi', 6904), ('noobick', 6905), ('noobie', 6906), ('noobies', 6907), ('noobis', 6908), ('noobla', 6909), ('noobnaut', 6910), ('noobs', 6911), ('noobshit', 6912), ('noobsssss', 6913), ('noobwd', 6914), ('noobz', 6915), ('nood', 6916), ('noodle', 6917), ('noone', 6918), ('nooo', 6919), ('nooob', 6920), ('nooobs', 6921), ('noooo', 6922), ('noooob', 6923), ('noooobest', 6924), ('nooooo', 6925), ('nooooob', 6926), ('noooooooo', 6927), ('noooooooooo', 6928), ('noooooooooooo', 6929), ('noooooooooooooooo', 6930), ('nooooooooooooooooooo', 6931), ('nooooooooooooooooooooob', 6932), ('noooooooooooooooooooooooo', 6933), ('noooow', 6934), ('noore', 6935), ('noov', 6936), ('noovs', 6937), ('nop', 6938), ('nope', 6939), ('nor', 6940), ('norm', 6941), ('normal', 6942), ('normalalc', 6943), ('norris', 6944), ('nort', 6945), ('noscope', 6946), ('nose', 6947), ('nosyat', 6948), ('not', 6949), ('notes', 6950), ('nothign', 6951), ('nothin', 6952), ('nothing', 6953), ('notice', 6954), ('noticed', 6955), ('notification', 6956), ('noting', 6957), ('notlikethis', 6958), ('notthing', 6959), ('noty', 6960), ('now', 6961), ('nowaday', 6962), ('nowadays', 6963), ('nowdays', 6964), ('nowww', 6965), ('nowwww', 6966), ('noyt', 6967), ('noza', 6968), ('np', 6969), ('npn', 6970), ('ns', 6971), ('nt', 6972), ('nthing', 6973), ('nto', 6974), ('nty', 6975), ('nu', 6976), ('nub', 6977), ('nubs', 6978), ('nuclear', 6979), ('nucna', 6980), ('nude', 6981), ('nudes', 6982), ('nug', 6983), ('nuh', 6984), ('nuke', 6985), ('nuked', 6986), ('nukeperu', 6987), ('nukes', 6988), ('null', 6989), ('number', 6990), ('nunal', 6991), ('nunca', 6992), ('nus', 6993), ('nut', 6994), ('nuthing', 6995), ('nuts', 6996), ('nutshell', 6997), ('nutz', 6998), ('nvm', 6999), ('nvo', 7000), ('nvoid', 7001), ('nws', 7002), ('nwx', 7003), ('nxt', 7004), ('nya', 7005), ('nyakl', 7006), ('nyc', 7007), ('nyet', 7008), ('nyjno', 7009), ('nylevoy', 7010), ('nyo', 7011), ('nyu', 7012), ('nyx', 7013), ('nyxnynyxynxnyxnyxnyxny', 7014), ('o', 7015), ('o0ne', 7016), ('o0o', 7017), ('o0ur', 7018), ('oa', 7019), ('obama', 7020), ('oblivion', 7021), ('obs', 7022), ('observe', 7023), ('observer', 7024), ('obsessed', 7025), ('obv', 7026), ('obviosly', 7027), ('obvious', 7028), ('obviously', 7029), ('obvius', 7030), ('obviusly', 7031), ('obvusly', 7032), ('ocampo', 7033), ('ocatarine', 7034), ('occasionally', 7035), ('ocmon', 7036), ('oct', 7037), ('octa', 7038), ('octarine', 7039), ('octine', 7040), ('ocugu', 7041), ('od', 7042), ('odd', 7043), ('odin', 7044), ('odio', 7045), ('odn', 7046), ('odnt', 7047), ('odust', 7048), ('oen', 7049), ('of', 7050), ('ofawhore', 7051), ('ofc', 7052), ('ofcc', 7053), ('ofci', 7054), ('off', 7055), ('offalne', 7056), ('offc', 7057), ('offensive', 7058), ('offer', 7059), ('offering', 7060), ('offfffff', 7061), ('offlane', 7062), ('offlaner', 7063), ('offlaners', 7064), ('offline', 7065), ('ofl', 7066), ('ofline', 7067), ('ofmek', 7068), ('often', 7069), ('ogh', 7070), ('oghhh', 7071), ('ogin', 7072), ('oging', 7073), ('ogld', 7074), ('ogre', 7075), ('oh', 7076), ('ohard', 7077), ('ohfoui', 7078), ('ohh', 7079), ('ohhh', 7080), ('ohhhhhh', 7081), ('ohk', 7082), ('ohkay', 7083), ('ohmen', 7084), ('ohmy', 7085), ('ohoy', 7086), ('ohuennee', 7087), ('ohwwwwwwww', 7088), ('oi', 7089), ('oie', 7090), ('oihhh', 7091), ('oiiii', 7092), ('oir', 7093), ('oiut', 7094), ('oj', 7095), ('ojala', 7096), ('ojgba', 7097), ('ojh', 7098), ('ojojojo', 7099), ('ok', 7100), ('ok2', 7101), ('okairi', 7102), ('okaty', 7103), ('okay', 7104), ('okay2', 7105), ('okayu', 7106), ('oke', 7107), ('okeh', 7108), ('okej', 7109), ('okey', 7110), ('okeyy', 7111), ('okeyyy', 7112), ('oki', 7113), ('okidoki', 7114), ('okie', 7115), ('okj', 7116), ('okk', 7117), ('okkkey', 7118), ('okl', 7119), ('okok', 7120), ('oks', 7121), ('oky', 7122), ('ol', 7123), ('olaaa', 7124), ('old', 7125), ('ole', 7126), ('oleg', 7127), ('olll', 7128), ('olo', 7129), ('olol', 7130), ('olso', 7131), ('olympics', 7132), ('om', 7133), ('omfg', 7134), ('omg', 7135), ('omgg', 7136), ('omggg', 7137), ('omggggg', 7138), ('omgggggggggggg', 7139), ('omi', 7140), ('omkg', 7141), ('ommni', 7142), ('omnbi', 7143), ('omni', 7144), ('omniknight', 7145), ('omnislash', 7146), ('omw', 7147), ('on', 7148), ('once', 7149), ('onde', 7150), ('one', 7151), ('oneee', 7152), ('ones', 7153), ('onilne', 7154), ('onkey', 7155), ('onkly', 7156), ('onli', 7157), ('online', 7158), ('only', 7159), ('onngoys', 7160), ('ono', 7161), ('onob', 7162), ('onoob', 7163), ('ont', 7164), ('oo', 7165), ('oob', 7166), ('ooh', 7167), ('oohh', 7168), ('ook', 7169), ('ookay', 7170), ('ooo', 7171), ('oooh', 7172), ('ooohhh', 7173), ('ooohoo', 7174), ('ooon', 7175), ('ooooh', 7176), ('oooooo', 7177), ('oooooon', 7178), ('oooooooo', 7179), ('oooooooooooh', 7180), ('ooooooooooooo', 7181), ('oooooooooooooh', 7182), ('ooooooooooooooooo', 7183), ('ooooooooooooooooooooooom', 7184), ('oooooooor', 7185), ('oooppps', 7186), ('ooops', 7187), ('ooopss', 7188), ('oops', 7189), ('op', 7190), ('open', 7191), ('opening', 7192), ('opieop', 7193), ('opinion', 7194), ('opn', 7195), ('oppenent', 7196), ('oppennenth', 7197), ('opponents', 7198), ('opposite', 7199), ('opps', 7200), ('ops', 7201), ('option', 7202), ('optjer', 7203), ('optus', 7204), ('oq', 7205), ('or', 7206), ('oracle', 7207), ('orange', 7208), ('oraxcle', 7209), ('orayt', 7210), ('orchid', 7211), ('ore', 7212), ('org', 7213), ('organise', 7214), ('orge', 7215), ('orgies', 7216), ('original', 7217), ('orlf', 7218), ('oro', 7219), ('orospu', 7220), ('orryt', 7221), ('ort', 7222), ('orusp', 7223), ('ory', 7224), ('oryou', 7225), ('oryt', 7226), ('os', 7227), ('osama', 7228), ('oseza', 7229), ('osfrog', 7230), ('osing', 7231), ('oso', 7232), ('ot', 7233), ('ota', 7234), ('other', 7235), ('others', 7236), ('otherwise', 7237), ('othing', 7238), ('otjimayu', 7239), ('otsukaresama', 7240), ('otvechay', 7241), ('otw', 7242), ('ou', 7243), ('ouch', 7244), ('oufheouihg', 7245), ('oufhsjghsdg', 7246), ('ought', 7247), ('ougna', 7248), ('oui', 7249), ('our', 7250), ('ours', 7251), ('out', 7252), ('outa', 7253), ('outcarry', 7254), ('outfarming', 7255), ('outlaned', 7256), ('outlevling', 7257), ('outpick', 7258), ('outplay', 7259), ('outplayd', 7260), ('outplayed', 7261), ('outside', 7262), ('outta', 7263), ('ouuu', 7264), ('ouy', 7265), ('over', 7266), ('overexcited', 7267), ('overextending', 7268), ('overheated', 7269), ('overkill', 7270), ('overly', 7271), ('overplay', 7272), ('overrated', 7273), ('overwhelming', 7274), ('ovision', 7275), ('ow', 7276), ('owe', 7277), ('owh', 7278), ('own', 7279), ('owned', 7280), ('ownes', 7281), ('owning', 7282), ('ownit', 7283), ('ownittt', 7284), ('owns', 7285), ('owuld', 7286), ('owww', 7287), ('ox', 7288), ('oxigeno', 7289), ('oy', 7290), ('oyoy', 7291), ('oyoyoy', 7292), ('oyoyoyoyoy', 7293), ('oyoyoyoyoyoyoy', 7294), ('oyu', 7295), ('p', 7296), ('p0ta', 7297), ('p0tang', 7298), ('p5', 7299), ('pa', 7300), ('pablo', 7301), ('pacan', 7302), ('pack', 7303), ('packet', 7304), ('packetloss', 7305), ('pad', 7306), ('pagar', 7307), ('pagaram', 7308), ('pahaha', 7309), ('pahha', 7310), ('paid', 7311), ('pail', 7312), ('pain', 7313), ('paint', 7314), ('pair', 7315), ('pairs', 7316), ('paiseh', 7317), ('pajarraco', 7318), ('pak', 7319), ('paket', 7320), ('paki', 7321), ('pakyu', 7322), ('pal', 7323), ('palky', 7324), ('paly', 7325), ('palyed', 7326), ('panda', 7327), ('pang', 7328), ('panic', 7329), ('panics', 7330), ('paningkamot', 7331), ('pano', 7332), ('panowie', 7333), ('pants', 7334), ('papa', 7335), ('papanya', 7336), ('papech', 7337), ('paper', 7338), ('papi', 7339), ('par', 7340), ('para', 7341), ('parents', 7342), ('park', 7343), ('parni', 7344), ('parody', 7345), ('part', 7346), ('partida', 7347), ('party', 7348), ('partyrock', 7349), ('pasie', 7350), ('pasiive', 7351), ('pass', 7352), ('passed', 7353), ('passing', 7354), ('passion', 7355), ('passive', 7356), ('passives', 7357), ('passsion', 7358), ('past', 7359), ('paste', 7360), ('pasue', 7361), ('pat', 7362), ('patalo', 7363), ('patch', 7364), ('patehtic', 7365), ('paterns', 7366), ('patetic', 7367), ('pathetic', 7368), ('patheticness', 7369), ('pathing', 7370), ('pati', 7371), ('patience', 7372), ('paty', 7373), ('paus', 7374), ('pause', 7375), ('paused', 7376), ('pauses', 7377), ('pausing', 7378), ('pausr', 7379), ('pauzka', 7380), ('pavo', 7381), ('pay', 7382), ('payback', 7383), ('payuse', 7384), ('pc', 7385), ('pciks', 7386), ('pcs', 7387), ('pd', 7388), ('pduga', 7389), ('pduge', 7390), ('pe', 7391), ('peace', 7392), ('pease', 7393), ('pedge', 7394), ('pee', 7395), ('peeeeeee', 7396), ('peeeeeeeeeeeeeeeeeeeeee', 7397), ('peen', 7398), ('peenoise', 7399), ('peenoty', 7400), ('peenoy', 7401), ('peenoys', 7402), ('peeps', 7403), ('pegde', 7404), ('peinose', 7405), ('pelase', 7406), ('pelean', 7407), ('pem', 7408), ('pena', 7409), ('penalty', 7410), ('pendejo', 7411), ('pene', 7412), ('peneoise', 7413), ('penetrate', 7414), ('penetrated', 7415), ('penetration', 7416), ('penis', 7417), ('people', 7418), ('pepe', 7419), ('pepet', 7420), ('peppers', 7421), ('per', 7422), ('perdedor', 7423), ('perdi', 7424), ('pered', 7425), ('perfect', 7426), ('perfectly', 7427), ('performance', 7428), ('perhapse', 7429), ('peri', 7430), ('period', 7431), ('perma', 7432), ('pero', 7433), ('perra', 7434), ('perro', 7435), ('perrra', 7436), ('perruuuuuuu', 7437), ('person', 7438), ('personality', 7439), ('perspective', 7440), ('pertty', 7441), ('peru', 7442), ('peruanetirar', 7443), ('peruano', 7444), ('peruanos', 7445), ('peruasno', 7446), ('peruca', 7447), ('perujvians', 7448), ('peruvian', 7449), ('peruvians', 7450), ('pesti', 7451), ('peter', 7452), ('pff', 7453), ('pfff', 7454), ('pfffttt', 7455), ('pfft', 7456), ('pgg', 7457), ('ph', 7458), ('phah', 7459), ('phandon', 7460), ('phanter', 7461), ('phantom', 7462), ('phase', 7463), ('phaton', 7464), ('phelps', 7465), ('phenix', 7466), ('pheonix', 7467), ('phew', 7468), ('phil', 7469), ('phoenix', 7470), ('phon', 7471), ('phone', 7472), ('phonix', 7473), ('phor', 7474), ('photo', 7475), ('photobooth', 7476), ('photoshoot', 7477), ('phreshin', 7478), ('phub', 7479), ('physical', 7480), ('pi', 7481), ('piast', 7482), ('pic', 7483), ('pice', 7484), ('pick', 7485), ('picka', 7486), ('pickd', 7487), ('picked', 7488), ('picker', 7489), ('pickers', 7490), ('picket', 7491), ('picki', 7492), ('pickin', 7493), ('picking', 7494), ('pickings', 7495), ('picknut', 7496), ('pickphase', 7497), ('picks', 7498), ('pid', 7499), ('pidar', 7500), ('pidaras', 7501), ('pidori', 7502), ('piece', 7503), ('pieces', 7504), ('pig', 7505), ('piggy', 7506), ('pikachu', 7507), ('pikaite', 7508), ('pikat', 7509), ('pikon', 7510), ('pilaet', 7511), ('pilde', 7512), ('pill', 7513), ('pimpo', 7514), ('pinaka', 7515), ('ping', 7516), ('pingas', 7517), ('pinging', 7518), ('pink', 7519), ('pinke', 7520), ('pinky', 7521), ('pinoy', 7522), ('pinoys', 7523), ('pinoyyyyyyyyy', 7524), ('piost', 7525), ('pip', 7526), ('pipe', 7527), ('pirt', 7528), ('piso', 7529), ('pisot', 7530), ('piss', 7531), ('pissed', 7532), ('pissface', 7533), ('pissing', 7534), ('pissy', 7535), ('piste', 7536), ('pit', 7537), ('pitty', 7538), ('pity', 7539), ('piwi', 7540), ('pizda', 7541), ('pizdabo', 7542), ('pizdec', 7543), ('pizza', 7544), ('pjsalt', 7545), ('pl', 7546), ('pla', 7547), ('place', 7548), ('placing', 7549), ('plags', 7550), ('plahing', 7551), ('plain', 7552), ('plan', 7553), ('plane', 7554), ('planet', 7555), ('planing', 7556), ('planned', 7557), ('plans', 7558), ('plant', 7559), ('plantign', 7560), ('planting', 7561), ('plaryer', 7562), ('plase', 7563), ('plata', 7564), ('platemail', 7565), ('plater', 7566), ('platter', 7567), ('plaued', 7568), ('plausible', 7569), ('play', 7570), ('playa', 7571), ('playd', 7572), ('playdoh', 7573), ('playe', 7574), ('played', 7575), ('player', 7576), ('playeres', 7577), ('players', 7578), ('playert', 7579), ('playes', 7580), ('playign', 7581), ('playin', 7582), ('playing', 7583), ('plays', 7584), ('playstlye', 7585), ('pld', 7586), ('pleae', 7587), ('pleas', 7588), ('please', 7589), ('pleaser', 7590), ('pleases', 7591), ('pleasse', 7592), ('pleasure', 7593), ('pleb', 7594), ('plebs', 7595), ('pleeaassse', 7596), ('pleease', 7597), ('pleeeeeeeee', 7598), ('plentiful', 7599), ('plenty', 7600), ('plesae', 7601), ('plis', 7602), ('pliss', 7603), ('pliz', 7604), ('pllease', 7605), ('plot', 7606), ('plox', 7607), ('pls', 7608), ('plss', 7609), ('plsss', 7610), ('plsssss', 7611), ('plsys', 7612), ('plsz', 7613), ('plurals', 7614), ('plus', 7615), ('plx', 7616), ('ply', 7617), ('plyer', 7618), ('plys', 7619), ('plz', 7620), ('plzi', 7621), ('plzz', 7622), ('plzzz', 7623), ('pm', 7624), ('pms', 7625), ('pnkg', 7626), ('pnoy', 7627), ('po', 7628), ('poa', 7629), ('pobre', 7630), ('pocket', 7631), ('pode', 7632), ('podes', 7633), ('podrubil', 7634), ('poetic', 7635), ('poetomu', 7636), ('pogchamp', 7637), ('pogi', 7638), ('pogle', 7639), ('pohui', 7640), ('point', 7641), ('pointless', 7642), ('points', 7643), ('poisin', 7644), ('poison', 7645), ('poitns', 7646), ('pokemon', 7647), ('polaying', 7648), ('polease', 7649), ('policy', 7650), ('polie', 7651), ('polite', 7652), ('pols', 7653), ('pone', 7654), ('pony', 7655), ('poo', 7656), ('poocheredi', 7657), ('pool', 7658), ('poop', 7659), ('poor', 7660), ('poormans', 7661), ('pop', 7662), ('popal', 7663), ('popcorn', 7664), ('por', 7665), ('porblem', 7666), ('poreso', 7667), ('porfavor', 7668), ('porn', 7669), ('pornhub', 7670), ('pornsite', 7671), ('porr', 7672), ('porra', 7673), ('portal', 7674), ('pos', 7675), ('poser', 7676), ('poshel', 7677), ('posion', 7678), ('position', 7679), ('positioning', 7680), ('positive', 7681), ('posla', 7682), ('poslal', 7683), ('possess', 7684), ('possible', 7685), ('possibly', 7686), ('post', 7687), ('postion', 7688), ('pot', 7689), ('pota', 7690), ('potang', 7691), ('potato', 7692), ('potm', 7693), ('potms', 7694), ('potomu', 7695), ('potreatj', 7696), ('potuition', 7697), ('pounce', 7698), ('pouncy', 7699), ('pourri', 7700), ('pouse', 7701), ('poutsa', 7702), ('powel', 7703), ('power', 7704), ('powerball', 7705), ('powerful', 7706), ('powerfull', 7707), ('powershot', 7708), ('powful', 7709), ('poxui', 7710), ('pp', 7711), ('ppd', 7712), ('ppl', 7713), ('pple', 7714), ('ppls', 7715), ('ppor', 7716), ('pq', 7717), ('practicalle', 7718), ('practice', 7719), ('practis', 7720), ('pray', 7721), ('pre', 7722), ('preach', 7723), ('precisa', 7724), ('predic', 7725), ('prediccion', 7726), ('predick', 7727), ('predict', 7728), ('predicted', 7729), ('prediction', 7730), ('preety', 7731), ('prefer', 7732), ('prefere', 7733), ('prefered', 7734), ('preference', 7735), ('premade', 7736), ('premute', 7737), ('premuted', 7738), ('prepare', 7739), ('present', 7740), ('president', 7741), ('press', 7742), ('pressence', 7743), ('pressing', 7744), ('pressure', 7745), ('pretty', 7746), ('previous', 7747), ('prfile', 7748), ('price', 7749), ('prick', 7750), ('pricks', 7751), ('pride', 7752), ('prio', 7753), ('prior', 7754), ('priority', 7755), ('private', 7756), ('pro', 7757), ('prob', 7758), ('probably', 7759), ('probaly', 7760), ('probkme', 7761), ('problem', 7762), ('probleme', 7763), ('problems', 7764), ('probly', 7765), ('probs', 7766), ('probsn', 7767), ('proccesor', 7768), ('produ', 7769), ('produced', 7770), ('profarmer', 7771), ('profeed', 7772), ('profesional', 7773), ('professional', 7774), ('profile', 7775), ('proigrali', 7776), ('prolem', 7777), ('promise', 7778), ('proo', 7779), ('proof', 7780), ('proofs', 7781), ('proper', 7782), ('properly', 7783), ('prophet', 7784), ('prophetation', 7785), ('pros', 7786), ('proseidon', 7787), ('prostit', 7788), ('prosto', 7789), ('proud', 7790), ('prove', 7791), ('proved', 7792), ('proves', 7793), ('provider', 7794), ('provoking', 7795), ('prretty', 7796), ('prt', 7797), ('pru', 7798), ('pry', 7799), ('pseudo', 7800), ('psh', 7801), ('psl', 7802), ('pss', 7803), ('psuh', 7804), ('psycho', 7805), ('pt', 7806), ('ptm', 7807), ('ptmr', 7808), ('pts', 7809), ('ptsd', 7810), ('puase', 7811), ('puases', 7812), ('pub', 7813), ('public', 7814), ('pubs', 7815), ('puc', 7816), ('puch', 7817), ('pucj', 7818), ('puck', 7819), ('pucked', 7820), ('pud', 7821), ('pude', 7822), ('pudeg', 7823), ('pudg', 7824), ('pudga', 7825), ('pudge', 7826), ('pudghe', 7827), ('pudgr', 7828), ('pudsge', 7829), ('puede', 7830), ('pug', 7831), ('pugde', 7832), ('puge', 7833), ('pugna', 7834), ('pugne', 7835), ('pugnoo', 7836), ('pui', 7837), ('puish', 7838), ('puita', 7839), ('puj', 7840), ('pukingina', 7841), ('pul', 7842), ('pull', 7843), ('pulled', 7844), ('pulling', 7845), ('pult', 7846), ('pumped', 7847), ('punch', 7848), ('punish', 7849), ('punishes', 7850), ('punit', 7851), ('punk', 7852), ('pup', 7853), ('puppey', 7854), ('purch', 7855), ('purchase', 7856), ('pure', 7857), ('purely', 7858), ('purge', 7859), ('purges', 7860), ('purp', 7861), ('purple', 7862), ('purpose', 7863), ('push', 7864), ('pushed', 7865), ('pusher', 7866), ('pushes', 7867), ('pushhhhhhh', 7868), ('pushing', 7869), ('pushte', 7870), ('pusi', 7871), ('puss', 7872), ('pussi', 7873), ('pussied', 7874), ('pussies', 7875), ('pusssssysyyysysysysys', 7876), ('pussy', 7877), ('pussyh', 7878), ('pussys', 7879), ('pusyy', 7880), ('put', 7881), ('puta', 7882), ('putang', 7883), ('putangina', 7884), ('putin', 7885), ('puting', 7886), ('putins', 7887), ('puto', 7888), ('putting', 7889), ('pve', 7890), ('pw', 7891), ('pwn', 7892), ('pyzdec', 7893), ('pz', 7894), ('q', 7895), ('qa', 7896), ('qartveli', 7897), ('qb', 7898), ('qesseo', 7899), ('qfhqiuhfouqehfoqwf', 7900), ('qo', 7901), ('qop', 7902), ('qopa', 7903), ('qopp', 7904), ('qops', 7905), ('qoq', 7906), ('qoup', 7907), ('qp', 7908), ('qq', 7909), ('qquesiton', 7910), ('quad', 7911), ('quality', 7912), ('quanto', 7913), ('quas', 7914), ('quaz', 7915), ('que', 7916), ('qued', 7917), ('queda', 7918), ('queen', 7919), ('quellin', 7920), ('quelling', 7921), ('quer', 7922), ('quesiton', 7923), ('question', 7924), ('questionable', 7925), ('queue', 7926), ('queueing', 7927), ('qui', 7928), ('quick', 7929), ('quickcast', 7930), ('quicker', 7931), ('quickly', 7932), ('quickscoped', 7933), ('quien', 7934), ('quieres', 7935), ('quiero', 7936), ('quiet', 7937), ('quik', 7938), ('quilspray', 7939), ('quiras', 7940), ('quit', 7941), ('quite', 7942), ('quiter', 7943), ('quitrr', 7944), ('quits', 7945), ('quitters', 7946), ('quitting', 7947), ('quote', 7948), ('quuuuuiiiiiteerasgx', 7949), ('qw3kje', 7950), ('qwait', 7951), ('qward', 7952), ('qwat', 7953), ('qweeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee', 7954), ('qweqwe', 7955), ('r', 7956), ('r8', 7957), ('ra', 7958), ('race', 7959), ('races', 7960), ('racism', 7961), ('racist', 7962), ('racks', 7963), ('rad', 7964), ('radi', 7965), ('radiance', 7966), ('radiant', 7967), ('radic', 7968), ('radidance', 7969), ('radience', 7970), ('radik', 7971), ('rae', 7972), ('rael', 7973), ('rag', 7974), ('rage', 7975), ('raged', 7976), ('ragequit', 7977), ('rager', 7978), ('rages', 7979), ('ragin', 7980), ('raging', 7981), ('raijin', 7982), ('rain', 7983), ('rainfall', 7984), ('raise', 7985), ('rak', 7986), ('raka', 7987), ('raki', 7988), ('raku', 7989), ('rally', 7990), ('rambo', 7991), ('ramge', 7992), ('ramo', 7993), ('ramp', 7994), ('rampae', 7995), ('rampage', 7996), ('rampagee', 7997), ('rampege', 7998), ('ran', 7999), ('randmon', 8000), ('random', 8001), ('randomed', 8002), ('randomer', 8003), ('randoming', 8004), ('randomn', 8005), ('randoms', 8006), ('ranfom', 8007), ('ranga', 8008), ('range', 8009), ('ranged', 8010), ('ranger', 8011), ('rank', 8012), ('ranked', 8013), ('rap', 8014), ('rape', 8015), ('raped', 8016), ('rapemid', 8017), ('rapes', 8018), ('rapido', 8019), ('rapier', 8020), ('rapiers', 8021), ('rapira', 8022), ('rapiraa', 8023), ('raporting', 8024), ('raptor', 8025), ('raqequited', 8026), ('rare', 8027), ('rarely', 8028), ('rares', 8029), ('rart', 8030), ('rase', 8031), ('raslabone', 8032), ('rason', 8033), ('rasta', 8034), ('rat', 8035), ('rata', 8036), ('ratas', 8037), ('ratazaaaa', 8038), ('rate', 8039), ('rather', 8040), ('rating', 8041), ('rats', 8042), ('ravage', 8043), ('rawr', 8044), ('rax', 8045), ('raxando', 8046), ('raz', 8047), ('raze', 8048), ('razes', 8049), ('razing', 8050), ('razor', 8051), ('rc', 8052), ('rcing', 8053), ('rcn', 8054), ('rdnt', 8055), ('rdy', 8056), ('re', 8057), ('rea', 8058), ('reach', 8059), ('reached', 8060), ('reaching', 8061), ('reaction', 8062), ('reactively', 8063), ('read', 8064), ('reading', 8065), ('ready', 8066), ('real', 8067), ('realise', 8068), ('realist', 8069), ('realistic', 8070), ('reality', 8071), ('realize', 8072), ('realized', 8073), ('reall', 8074), ('really', 8075), ('reallyh', 8076), ('realty', 8077), ('realx', 8078), ('realy', 8079), ('reaped', 8080), ('reaper', 8081), ('rearm', 8082), ('reason', 8083), ('reasonable', 8084), ('reasons', 8085), ('reatd', 8086), ('reatrd', 8087), ('reatrds', 8088), ('reaver', 8089), ('rebooted', 8090), ('rebooting', 8091), ('reborn', 8092), ('rec', 8093), ('recall', 8094), ('receive', 8095), ('recent', 8096), ('recently', 8097), ('recfraction', 8098), ('reci', 8099), ('recing', 8100), ('recion', 8101), ('recipe', 8102), ('recked', 8103), ('recomend', 8104), ('recomenden', 8105), ('recomiendane', 8106), ('recomiendenme', 8107), ('recomiendo', 8108), ('recommend', 8109), ('recon', 8110), ('reconect', 8111), ('reconncet', 8112), ('reconnect', 8113), ('reconnecting', 8114), ('reconnet', 8115), ('recons', 8116), ('reconsider', 8117), ('record', 8118), ('recorded', 8119), ('recorder', 8120), ('recording', 8121), ('recorn', 8122), ('recotnra', 8123), ('recovered', 8124), ('rect', 8125), ('recure', 8126), ('red', 8127), ('reddit', 8128), ('redemption', 8129), ('redhead', 8130), ('rediculous', 8131), ('redtube', 8132), ('reduction', 8133), ('ree', 8134), ('reeeeekt', 8135), ('reeport', 8136), ('ref', 8137), ('reference', 8138), ('refill', 8139), ('refport', 8140), ('refrac', 8141), ('refract', 8142), ('refraction', 8143), ('refresh', 8144), ('refreshed', 8145), ('refresher', 8146), ('refused', 8147), ('refuses', 8148), ('refusing', 8149), ('regalen', 8150), ('regard', 8151), ('regardless', 8152), ('regbnt', 8153), ('rege', 8154), ('regen', 8155), ('register', 8156), ('regret', 8157), ('regrets', 8158), ('reinicio', 8159), ('reinstall', 8160), ('reisen', 8161), ('rek', 8162), ('rekity', 8163), ('rekon', 8164), ('rekt', 8165), ('rektd', 8166), ('rekted', 8167), ('rektkatka', 8168), ('reky', 8169), ('related', 8170), ('relax', 8171), ('relaxxx', 8172), ('relentless', 8173), ('relevant', 8174), ('relic', 8175), ('relly', 8176), ('relocate', 8177), ('relt', 8178), ('remain', 8179), ('remaining', 8180), ('remake', 8181), ('rematch', 8182), ('rembo', 8183), ('remember', 8184), ('remembet', 8185), ('reminant', 8186), ('remind', 8187), ('remmeber', 8188), ('remnant', 8189), ('remotely', 8190), ('remove', 8191), ('removed', 8192), ('remy', 8193), ('rent', 8194), ('reort', 8195), ('rep', 8196), ('repel', 8197), ('replay', 8198), ('replays', 8199), ('repoc', 8200), ('repoirt', 8201), ('repoprt', 8202), ('repor', 8203), ('repord', 8204), ('reporn', 8205), ('reporspec', 8206), ('report', 8207), ('reporta', 8208), ('reportado', 8209), ('reportd', 8210), ('reporte', 8211), ('reportean', 8212), ('reported', 8213), ('reporten', 8214), ('reportenlo', 8215), ('reportenme', 8216), ('reportet', 8217), ('reporting', 8218), ('reportnite', 8219), ('reportnu', 8220), ('reporto', 8221), ('reportr', 8222), ('reports', 8223), ('reportslark', 8224), ('reportt', 8225), ('reportwk', 8226), ('repot', 8227), ('repoted', 8228), ('repotr', 8229), ('repport', 8230), ('representing', 8231), ('reprot', 8232), ('reprpot', 8233), ('reprt', 8234), ('republicans', 8235), ('requested', 8236), ('requim', 8237), ('require', 8238), ('rerport', 8239), ('rerpot', 8240), ('resdtart', 8241), ('reserve', 8242), ('reset', 8243), ('resetin', 8244), ('resist', 8245), ('resmue', 8246), ('respawn', 8247), ('respawned', 8248), ('respawns', 8249), ('respect', 8250), ('response', 8251), ('rest', 8252), ('restart', 8253), ('restarted', 8254), ('restarting', 8255), ('restroom', 8256), ('result', 8257), ('resume', 8258), ('resumed', 8259), ('resuming', 8260), ('retads', 8261), ('retard', 8262), ('retarded', 8263), ('retardedrer', 8264), ('retardedw', 8265), ('retards', 8266), ('retared', 8267), ('rethink', 8268), ('retk', 8269), ('retrad', 8270), ('retrasado', 8271), ('retreath', 8272), ('retribution', 8273), ('return', 8274), ('reume', 8275), ('reuse', 8276), ('revenge', 8277), ('reward', 8278), ('rewporetd', 8279), ('rewtar', 8280), ('rexxar', 8281), ('rez', 8282), ('rezzzzzzzzzzzzzzzzzzzzzzzzzzzz', 8283), ('rfc', 8284), ('rhim', 8285), ('rice', 8286), ('rich', 8287), ('rick', 8288), ('ricken', 8289), ('ricky', 8290), ('rid', 8291), ('ride', 8292), ('ridiculous', 8293), ('right', 8294), ('rightr', 8295), ('riiight', 8296), ('riki', 8297), ('rikii', 8298), ('rikik', 8299), ('rikki', 8300), ('riky', 8301), ('rim', 8302), ('rindo', 8303), ('ring', 8304), ('rip', 8305), ('riperino', 8306), ('ripp', 8307), ('rise', 8308), ('risky', 8309), ('rissoans', 8310), ('ritardati', 8311), ('rite', 8312), ('ritw', 8313), ('river', 8314), ('rizada', 8315), ('rizwan', 8316), ('rjyxtyfz', 8317), ('rkbyrp', 8318), ('rl', 8319), ('rleady', 8320), ('rlly', 8321), ('rly', 8322), ('rlz', 8323), ('rm', 8324), ('rme', 8325), ('rmemeber', 8326), ('rmk', 8327), ('rn', 8328), ('rng', 8329), ('rngesus', 8330), ('rnked', 8331), ('ro', 8332), ('ro3', 8333), ('road', 8334), ('roam', 8335), ('roamer', 8336), ('roaming', 8337), ('roams', 8338), ('robbing', 8339), ('robin', 8340), ('robot', 8341), ('rock', 8342), ('rocked', 8343), ('rocket', 8344), ('rockhead', 8345), ('rocks', 8346), ('rof', 8347), ('rofl', 8348), ('roflajhjakhdjakldnalkdn', 8349), ('roflll', 8350), ('rofllll', 8351), ('roflmao', 8352), ('rofmao', 8353), ('roh', 8354), ('roi', 8355), ('roket', 8356), ('roki', 8357), ('role', 8358), ('roles', 8359), ('rolf', 8360), ('rolfh', 8361), ('roll', 8362), ('rollers', 8363), ('rolling', 8364), ('rolls', 8365), ('romans', 8366), ('rong', 8367), ('roofl', 8368), ('room', 8369), ('root', 8370), ('rooting', 8371), ('ropfl', 8372), ('ropsh', 8373), ('rosh', 8374), ('roshan', 8375), ('roshans', 8376), ('roshd', 8377), ('roshing', 8378), ('rot', 8379), ('rotate', 8380), ('rotated', 8381), ('rotating', 8382), ('rotation', 8383), ('rotations', 8384), ('rough', 8385), ('rought', 8386), ('round', 8387), ('router', 8388), ('row', 8389), ('rp', 8390), ('rpeort', 8391), ('rpeorten', 8392), ('rpeorting', 8393), ('rpeott', 8394), ('rperot', 8395), ('rpg', 8396), ('rpo', 8397), ('rpoflmao', 8398), ('rprobems', 8399), ('rps', 8400), ('rq', 8401), ('rroamer', 8402), ('rs', 8403), ('rsdfkpghsd', 8404), ('rspe', 8405), ('rt', 8406), ('rtc', 8407), ('rtd', 8408), ('rteported', 8409), ('rthe', 8410), ('rto', 8411), ('rtop', 8412), ('rtz', 8413), ('ru', 8414), ('ru9n', 8415), ('rub', 8416), ('rubbick', 8417), ('rubbish', 8418), ('rube', 8419), ('rubic', 8420), ('rubick', 8421), ('rubik', 8422), ('rubikc', 8423), ('rubing', 8424), ('rubish', 8425), ('rubrick', 8426), ('rubs', 8427), ('rude', 8428), ('rudee', 8429), ('rudeness', 8430), ('rugal', 8431), ('ruh', 8432), ('ruim', 8433), ('ruin', 8434), ('ruine', 8435), ('ruined', 8436), ('ruiner', 8437), ('ruinetr', 8438), ('ruining', 8439), ('ruins', 8440), ('rule', 8441), ('rules', 8442), ('ruling', 8443), ('ruller', 8444), ('run', 8445), ('runa', 8446), ('rune', 8447), ('runes', 8448), ('runing', 8449), ('runner', 8450), ('running', 8451), ('runnn', 8452), ('runs', 8453), ('rupees', 8454), ('ruptured', 8455), ('rus', 8456), ('rush', 8457), ('rushing', 8458), ('rusk', 8459), ('ruskaya', 8460), ('ruski', 8461), ('ruskies', 8462), ('ruskis', 8463), ('russ', 8464), ('russia', 8465), ('russian', 8466), ('russians', 8467), ('russiinn', 8468), ('russki', 8469), ('russtard', 8470), ('rustard', 8471), ('rustards', 8472), ('rusty', 8473), ('rweport', 8474), ('rx', 8475), ('ryan', 8476), ('rylai', 8477), ('ryt', 8478), ('rz', 8479), ('s', 8480), ('s4', 8481), ('sa', 8482), ('saad', 8483), ('saatnic', 8484), ('saba', 8485), ('sabe', 8486), ('sabihin', 8487), ('saboid', 8488), ('sabrura', 8489), ('sacan', 8490), ('sacas', 8491), ('sack', 8492), ('sacrifica', 8493), ('sacrifice', 8494), ('sacrifile', 8495), ('sacrifish', 8496), ('sad', 8497), ('sadasdasdasd', 8498), ('sadbois', 8499), ('sadboy', 8500), ('saddlebag', 8501), ('sadest', 8502), ('sadfacearino', 8503), ('sadfaceariono', 8504), ('sadist', 8505), ('sadly', 8506), ('sadness', 8507), ('saduhhsad', 8508), ('saec', 8509), ('saf', 8510), ('safd', 8511), ('safe', 8512), ('safelaen', 8513), ('safelane', 8514), ('safely', 8515), ('sahdow', 8516), ('sai', 8517), ('said', 8518), ('saikiiii', 8519), ('saimung', 8520), ('saiyan', 8521), ('sak', 8522), ('sakai', 8523), ('sake', 8524), ('sakes', 8525), ('saki', 8526), ('sakin', 8527), ('sakit', 8528), ('sakita', 8529), ('salami', 8530), ('sali', 8531), ('salisi', 8532), ('salita', 8533), ('sall', 8534), ('salo', 8535), ('salrk', 8536), ('salt', 8537), ('salty', 8538), ('sam', 8539), ('sama', 8540), ('same', 8541), ('samuel', 8542), ('samuelll', 8543), ('samul', 8544), ('san', 8545), ('sand', 8546), ('sandking', 8547), ('sandstorm', 8548), ('sandstorming', 8549), ('sandy', 8550), ('sange', 8551), ('sanking', 8552), ('sanya', 8553), ('sao', 8554), ('sasgggddd', 8555), ('sassy', 8556), ('sat', 8557), ('satan', 8558), ('satanic', 8559), ('satiesfied', 8560), ('satisfaction', 8561), ('sausage', 8562), ('sauy', 8563), ('save', 8564), ('saved', 8565), ('saving', 8566), ('saw', 8567), ('say', 8568), ('sayang', 8569), ('sayin', 8570), ('saying', 8571), ('sayo', 8572), ('says', 8573), ('sb', 8574), ('sblade', 8575), ('sbv', 8576), ('sc', 8577), ('scare', 8578), ('scared', 8579), ('scary', 8580), ('scepter', 8581), ('schackle', 8582), ('schei', 8583), ('school', 8584), ('schoolboy', 8585), ('schwanz', 8586), ('science', 8587), ('scientist', 8588), ('sclard', 8589), ('sclav', 8590), ('scool', 8591), ('score', 8592), ('scoreboard', 8593), ('scored', 8594), ('scorpion', 8595), ('scott', 8596), ('scouting', 8597), ('scrabbling', 8598), ('scratched', 8599), ('scre', 8600), ('scream', 8601), ('screaming', 8602), ('screen', 8603), ('screenshoot', 8604), ('screw', 8605), ('screwing', 8606), ('scrim', 8607), ('scriper', 8608), ('scripter', 8609), ('scripts', 8610), ('scroll', 8611), ('scrub', 8612), ('scrublord', 8613), ('scrubs', 8614), ('scrun', 8615), ('scum', 8616), ('scurbs', 8617), ('scurred', 8618), ('sd', 8619), ('sda', 8620), ('sdelayu', 8621), ('sds', 8622), ('se', 8623), ('sea', 8624), ('search', 8625), ('searing', 8626), ('seattle', 8627), ('sec', 8628), ('second', 8629), ('seconds', 8630), ('seconecto', 8631), ('secpls', 8632), ('secret', 8633), ('secrets', 8634), ('secretshop', 8635), ('secs', 8636), ('section', 8637), ('secure', 8638), ('secured', 8639), ('secxy', 8640), ('sed', 8641), ('sedih', 8642), ('see', 8643), ('seee', 8644), ('seeing', 8645), ('seeker', 8646), ('seem', 8647), ('seems', 8648), ('seen', 8649), ('seer', 8650), ('seeu', 8651), ('seexy', 8652), ('seg', 8653), ('sek', 8654), ('self', 8655), ('selfish', 8656), ('selg', 8657), ('sell', 8658), ('sem', 8659), ('semana', 8660), ('semantics', 8661), ('semblance', 8662), ('semi', 8663), ('sen', 8664), ('sence', 8665), ('send', 8666), ('sending', 8667), ('sene', 8668), ('senk', 8669), ('sense', 8670), ('sent', 8671), ('sentence', 8672), ('sentires', 8673), ('sentries', 8674), ('sentry', 8675), ('sentrys', 8676), ('sents', 8677), ('sepa', 8678), ('sepctre', 8679), ('sereiously', 8680), ('serenity', 8681), ('sereuse', 8682), ('series', 8683), ('serinity', 8684), ('serious', 8685), ('seriously', 8686), ('seriouslyt', 8687), ('seroiusly', 8688), ('serv', 8689), ('server', 8690), ('servers', 8691), ('serves', 8692), ('service', 8693), ('servidor', 8694), ('serving', 8695), ('set', 8696), ('setnry', 8697), ('setter', 8698), ('setting', 8699), ('settings', 8700), ('settled', 8701), ('several', 8702), ('sevn', 8703), ('sex', 8704), ('sexally', 8705), ('sexi', 8706), ('sexo', 8707), ('sexual', 8708), ('sexy', 8709), ('sexyness', 8710), ('sexyyyy', 8711), ('sexyyyyy', 8712), ('sf', 8713), ('sfs', 8714), ('sg', 8715), ('sget', 8716), ('sgh', 8717), ('sgut', 8718), ('sh', 8719), ('shackel', 8720), ('shackels', 8721), ('shackle', 8722), ('shackled', 8723), ('shackles', 8724), ('shackleshot', 8725), ('shacle', 8726), ('shades', 8727), ('shadow', 8728), ('shadowblade', 8729), ('shadowfiend', 8730), ('shadowfriend666', 8731), ('shag', 8732), ('shake', 8733), ('shaked', 8734), ('shakel', 8735), ('shaker', 8736), ('shakere', 8737), ('shakermon', 8738), ('shaking', 8739), ('shakle', 8740), ('shakled', 8741), ('shall', 8742), ('shallow', 8743), ('shallwe', 8744), ('sham', 8745), ('shaman', 8746), ('shambles', 8747), ('shame', 8748), ('shameless', 8749), ('shape', 8750), ('shapes', 8751), ('shard', 8752), ('shards', 8753), ('share', 8754), ('shareing', 8755), ('sharp', 8756), ('sharpest', 8757), ('shas', 8758), ('shdi', 8759), ('she', 8760), ('sheck', 8761), ('shell', 8762), ('shes', 8763), ('shet', 8764), ('shh', 8765), ('shhh', 8766), ('shhhh', 8767), ('shhht', 8768), ('shhihtter', 8769), ('shhiiittt', 8770), ('shhshhshshshnh', 8771), ('shield', 8772), ('shiet', 8773), ('shift', 8774), ('shiits', 8775), ('shinken', 8776), ('shiny', 8777), ('shirt', 8778), ('shit', 8779), ('shitbag', 8780), ('shite', 8781), ('shitface', 8782), ('shithead', 8783), ('shiti', 8784), ('shitness', 8785), ('shitpoo', 8786), ('shits', 8787), ('shitshaker', 8788), ('shitstain', 8789), ('shitstains', 8790), ('shitt', 8791), ('shittalking', 8792), ('shitters', 8793), ('shittest', 8794), ('shitties', 8795), ('shittt', 8796), ('shitttt', 8797), ('shittttt', 8798), ('shitty', 8799), ('shity', 8800), ('shiva', 8801), ('shjit', 8802), ('shkel', 8803), ('shld', 8804), ('shocking', 8805), ('shockingly', 8806), ('shodnt', 8807), ('shoehorn', 8808), ('shoes', 8809), ('shoiuld', 8810), ('shold', 8811), ('shoot', 8812), ('shop', 8813), ('shoping', 8814), ('shoppin', 8815), ('shorer', 8816), ('short', 8817), ('shorter', 8818), ('shortest', 8819), ('shot', 8820), ('shots', 8821), ('shoukld', 8822), ('should', 8823), ('shoulda', 8824), ('shoulders', 8825), ('shouldnt', 8826), ('shouldve', 8827), ('shout', 8828), ('shoutout', 8829), ('shove', 8830), ('show', 8831), ('showed', 8832), ('shpould', 8833), ('shrapnel', 8834), ('sht', 8835), ('shti', 8836), ('shto', 8837), ('shtorm', 8838), ('shu', 8839), ('shu8t', 8840), ('shud', 8841), ('shuit', 8842), ('shuld', 8843), ('shup', 8844), ('shut', 8845), ('shutting', 8846), ('shutup', 8847), ('shy', 8848), ('si', 8849), ('sia', 8850), ('siap', 8851), ('siark', 8852), ('sick', 8853), ('sickde', 8854), ('sickened', 8855), ('sickening', 8856), ('sickest', 8857), ('side', 8858), ('sided', 8859), ('sideeee', 8860), ('sides', 8861), ('sidruptor', 8862), ('siege', 8863), ('sielencer', 8864), ('sielnecr', 8865), ('sien', 8866), ('siente', 8867), ('sif', 8868), ('sige', 8869), ('sigh', 8870), ('sign', 8871), ('sigue', 8872), ('siht', 8873), ('sii', 8874), ('sikerlerrrrrrrrrrrrrrrrrrrrrrrr', 8875), ('sil', 8876), ('silancer', 8877), ('silence', 8878), ('silenced', 8879), ('silencer', 8880), ('silencerrrr', 8881), ('silencing', 8882), ('silencio', 8883), ('silenser', 8884), ('silent', 8885), ('silento', 8886), ('silly', 8887), ('silncer', 8888), ('silncver', 8889), ('silver', 8890), ('similar', 8891), ('simple', 8892), ('simples', 8893), ('sin', 8894), ('sina', 8895), ('sinadya', 8896), ('since', 8897), ('sing', 8898), ('singapour', 8899), ('singing', 8900), ('single', 8901), ('singlehandely', 8902), ('singsing', 8903), ('sinned', 8904), ('sipport', 8905), ('sir', 8906), ('siraptor', 8907), ('sire', 8908), ('siren', 8909), ('sis', 8910), ('sisihan', 8911), ('sister', 8912), ('sit', 8913), ('site', 8914), ('sites', 8915), ('siting', 8916), ('sitll', 8917), ('sitted', 8918), ('sitting', 8919), ('situation', 8920), ('six', 8921), ('size', 8922), ('sj', 8923), ('sjit', 8924), ('sjot', 8925), ('sjwsahashbas', 8926), ('sk', 8927), ('skadi', 8928), ('skali', 8929), ('skatin', 8930), ('skdjbfksjdbfk', 8931), ('skewer', 8932), ('skil', 8933), ('skill', 8934), ('skilled', 8935), ('skills', 8936), ('skillshot', 8937), ('skillz', 8938), ('skils', 8939), ('skrub', 8940), ('skulls', 8941), ('sky', 8942), ('skyp', 8943), ('skype', 8944), ('skyrath', 8945), ('slack', 8946), ('slacko', 8947), ('slada', 8948), ('sladar', 8949), ('sladdar', 8950), ('slakrk', 8951), ('slam', 8952), ('slank', 8953), ('slant', 8954), ('slanty', 8955), ('slar', 8956), ('slaradar', 8957), ('slarda', 8958), ('slardar', 8959), ('slardara', 8960), ('slardars', 8961), ('slardasr', 8962), ('slarder', 8963), ('slardera', 8964), ('slards', 8965), ('slark', 8966), ('slarka', 8967), ('slarke', 8968), ('slarkino', 8969), ('slarkj', 8970), ('slarky', 8971), ('slarsadder', 8972), ('slash', 8973), ('slaty', 8974), ('slaughtered', 8975), ('slave', 8976), ('slay', 8977), ('slayer', 8978), ('slayers', 8979), ('sleep', 8980), ('sleeping', 8981), ('sleepzzz', 8982), ('slil', 8983), ('slipped', 8984), ('slivaetsya', 8985), ('slojno', 8986), ('slot', 8987), ('slota', 8988), ('sloth', 8989), ('slots', 8990), ('slova', 8991), ('slovakia', 8992), ('slow', 8993), ('slowing', 8994), ('slown', 8995), ('slrk', 8996), ('slus', 8997), ('slut', 8998), ('sluts', 8999), ('slutty', 9000), ('slvrdota', 9001), ('sm', 9002), ('sm1', 9003), ('smack', 9004), ('smacked', 9005), ('small', 9006), ('smar', 9007), ('smart', 9008), ('smartass', 9009), ('smash', 9010), ('smaterz', 9011), ('sme', 9012), ('smell', 9013), ('smg', 9014), ('smh', 9015), ('smhall', 9016), ('smht', 9017), ('smile', 9018), ('smk', 9019), ('smoke', 9020), ('smoked', 9021), ('smoking', 9022), ('smooth', 9023), ('sms', 9024), ('smth', 9025), ('smurf', 9026), ('smurfs', 9027), ('snaiper', 9028), ('snake', 9029), ('snap', 9030), ('snatch', 9031), ('sneaky', 9032), ('snickers', 9033), ('snieper', 9034), ('snierp', 9035), ('snip', 9036), ('snipa', 9037), ('snipe', 9038), ('sniped', 9039), ('sniper', 9040), ('snipers', 9041), ('snitch', 9042), ('snow', 9043), ('snowangel', 9044), ('snowball', 9045), ('snowballing', 9046), ('sny', 9047), ('so', 9048), ('sobaka', 9049), ('soboy', 9050), ('sobrala', 9051), ('socre', 9052), ('sod', 9053), ('soda', 9054), ('sodium', 9055), ('soft', 9056), ('softly', 9057), ('soght', 9058), ('soh', 9059), ('sohai', 9060), ('sohuld', 9061), ('soi', 9062), ('sok', 9063), ('solar', 9064), ('sold', 9065), ('solid', 9066), ('solo', 9067), ('soloing', 9068), ('soloooooooooooooooooooooooooooooooooooooooooooooo', 9069), ('soloq', 9070), ('solos', 9071), ('solyanovo', 9072), ('somali', 9073), ('somalis', 9074), ('some', 9075), ('some1', 9076), ('somebody', 9077), ('somee1', 9078), ('somehow', 9079), ('someone', 9080), ('something', 9081), ('somethings', 9082), ('sometimes', 9083), ('somevont', 9084), ('somewhere', 9085), ('somthing', 9086), ('somtiems', 9087), ('son', 9088), ('sone', 9089), ('song', 9090), ('sonic', 9091), ('sons', 9092), ('soo', 9093), ('sook', 9094), ('sooking', 9095), ('soon', 9096), ('soontm', 9097), ('sooo', 9098), ('soookaaa', 9099), ('sooon', 9100), ('soooo', 9101), ('sooooo', 9102), ('soooooooo', 9103), ('soooooooooooooooo', 9104), ('soooooooooooooooobad', 9105), ('soooooooooooooooooooo', 9106), ('sooooooooooooooooooooooooooo', 9107), ('sop', 9108), ('sopresa', 9109), ('sore', 9110), ('sorr', 9111), ('sorru', 9112), ('sorry', 9113), ('sorta', 9114), ('sorting', 9115), ('sory', 9116), ('soryr', 9117), ('sos', 9118), ('sosater', 9119), ('sosator', 9120), ('sosi', 9121), ('sosnesh', 9122), ('soul', 9123), ('sould', 9124), ('souls', 9125), ('sound', 9126), ('sounded', 9127), ('sounds', 9128), ('soup', 9129), ('south', 9130), ('soy', 9131), ('soz', 9132), ('sozdanie', 9133), ('sp', 9134), ('spa', 9135), ('space', 9136), ('spacecow', 9137), ('spaghetti', 9138), ('spam', 9139), ('spammed', 9140), ('spammer', 9141), ('spamming', 9142), ('spanish', 9143), ('spare', 9144), ('spared', 9145), ('spark', 9146), ('sparkly', 9147), ('sparta', 9148), ('spasibo', 9149), ('spastic', 9150), ('spawn', 9151), ('spawned', 9152), ('spawns', 9153), ('spaz', 9154), ('spce', 9155), ('speack', 9156), ('speak', 9157), ('speaking', 9158), ('speaks', 9159), ('spec', 9160), ('speccccccc', 9161), ('special', 9162), ('specially', 9163), ('spectr', 9164), ('spectra', 9165), ('spectre', 9166), ('speed', 9167), ('spell', 9168), ('spelled', 9169), ('spelling', 9170), ('spelll', 9171), ('spells', 9172), ('spend', 9173), ('spender', 9174), ('spends', 9175), ('spent', 9176), ('sperm', 9177), ('spiccy', 9178), ('spics', 9179), ('spicy', 9180), ('spider', 9181), ('spiderman', 9182), ('spieleeeeen', 9183), ('spik', 9184), ('spike', 9185), ('spiked', 9186), ('spikes', 9187), ('spin', 9188), ('spiral', 9189), ('spirit', 9190), ('spisn', 9191), ('spit', 9192), ('split', 9193), ('spoil', 9194), ('spoiler', 9195), ('spoiling', 9196), ('spoill', 9197), ('spoils', 9198), ('spokesman', 9199), ('spooked', 9200), ('spooky', 9201), ('sport', 9202), ('spot', 9203), ('spoted', 9204), ('spoterd', 9205), ('spotted', 9206), ('spouky', 9207), ('spray', 9208), ('spree', 9209), ('spro', 9210), ('sprout', 9211), ('sps', 9212), ('spy', 9213), ('spykes', 9214), ('sqeezy', 9215), ('squad', 9216), ('square', 9217), ('squeaker', 9218), ('squishy', 9219), ('sr', 9220), ('sreated', 9221), ('srg', 9222), ('sri', 9223), ('srlsy', 9224), ('srry', 9225), ('srs', 9226), ('srsly', 9227), ('sry', 9228), ('sryt', 9229), ('ss', 9230), ('ssentry', 9231), ('sshh', 9232), ('sso', 9233), ('ssory', 9234), ('sss', 9235), ('ssss', 9236), ('st', 9237), ('stab', 9238), ('stabil', 9239), ('stable', 9240), ('stack', 9241), ('stacked', 9242), ('stacks', 9243), ('stacktrash', 9244), ('stading', 9245), ('stafcks', 9246), ('staff', 9247), ('stage', 9248), ('stages', 9249), ('stahp', 9250), ('stains', 9251), ('staks', 9252), ('stalker', 9253), ('stalking', 9254), ('stall', 9255), ('stamos', 9256), ('stand', 9257), ('standard', 9258), ('standin', 9259), ('standing', 9260), ('star', 9261), ('staralsya', 9262), ('starfall', 9263), ('stars', 9264), ('start', 9265), ('started', 9266), ('starting', 9267), ('starts', 9268), ('starving', 9269), ('stats', 9270), ('statss', 9271), ('statues', 9272), ('stay', 9273), ('stayd', 9274), ('staying', 9275), ('stays', 9276), ('std', 9277), ('steal', 9278), ('stealed', 9279), ('stealer', 9280), ('stealing', 9281), ('stealmid', 9282), ('steam', 9283), ('steamroll', 9284), ('steel', 9285), ('step', 9286), ('stepd', 9287), ('stewped', 9288), ('stfu', 9289), ('stfuuu', 9290), ('stfuy', 9291), ('sth', 9292), ('sthap', 9293), ('sthe', 9294), ('stick', 9295), ('stil', 9296), ('still', 9297), ('stinked', 9298), ('stinky', 9299), ('stitch', 9300), ('stlark', 9301), ('stole', 9302), ('stomp', 9303), ('stomped', 9304), ('stone', 9305), ('stonf', 9306), ('stong', 9307), ('stoods', 9308), ('stooping', 9309), ('stop', 9310), ('stopd', 9311), ('stoping', 9312), ('stopm', 9313), ('stopped', 9314), ('store', 9315), ('storm', 9316), ('storma', 9317), ('storng', 9318), ('story', 9319), ('stoy', 9320), ('stoyat', 9321), ('stpd', 9322), ('stpuid', 9323), ('str', 9324), ('str8', 9325), ('straight', 9326), ('stral', 9327), ('strange', 9328), ('strangers', 9329), ('strat', 9330), ('strategy', 9331), ('strats', 9332), ('streak', 9333), ('streaks', 9334), ('stream', 9335), ('streamers', 9336), ('streaming', 9337), ('stree', 9338), ('streka', 9339), ('stress', 9340), ('stressed', 9341), ('strg', 9342), ('striggers', 9343), ('strike', 9344), ('strikes', 9345), ('stroking', 9346), ('strom', 9347), ('stromn', 9348), ('stron', 9349), ('strong', 9350), ('stronger', 9351), ('stronk', 9352), ('stronmg', 9353), ('struggled', 9354), ('struggling', 9355), ('stryong', 9356), ('stuck', 9357), ('stud', 9358), ('stuff', 9359), ('stum', 9360), ('stump', 9361), ('stun', 9362), ('stuna', 9363), ('stunned', 9364), ('stunning', 9365), ('stunrange', 9366), ('stuns', 9367), ('stupid', 9368), ('stupids', 9369), ('stupiud', 9370), ('style', 9371), ('su', 9372), ('su4ok', 9373), ('sua', 9374), ('submit', 9375), ('succes', 9376), ('succesds', 9377), ('successful', 9378), ('such', 9379), ('suchj', 9380), ('suchtryhard', 9381), ('suck', 9382), ('sucka', 9383), ('sucked', 9384), ('sucker', 9385), ('suckers', 9386), ('sucking', 9387), ('suckmid', 9388), ('suckoff', 9389), ('sucks', 9390), ('suda', 9391), ('suddenly', 9392), ('sudoku', 9393), ('sue', 9394), ('suffer', 9395), ('suffered', 9396), ('suffering', 9397), ('suffers', 9398), ('sugar', 9399), ('suggust', 9400), ('suicede', 9401), ('suicidal', 9402), ('suicide', 9403), ('suiciding', 9404), ('suitable', 9405), ('suits', 9406), ('suka', 9407), ('suki', 9408), ('sulod', 9409), ('sum', 9410), ('sumail', 9411), ('sumial', 9412), ('summail', 9413), ('sumwer', 9414), ('sun', 9415), ('sunder', 9416), ('sunset', 9417), ('sunsfan', 9418), ('sunstrike', 9419), ('suoper', 9420), ('sup', 9421), ('supamida', 9422), ('super', 9423), ('supercreeps', 9424), ('suphd', 9425), ('supoer', 9426), ('suport', 9427), ('suporte', 9428), ('suporting', 9429), ('suports', 9430), ('supp', 9431), ('supplies', 9432), ('suppoort', 9433), ('suppoprt', 9434), ('suppor', 9435), ('support', 9436), ('supportado', 9437), ('supported', 9438), ('supporting', 9439), ('supports', 9440), ('suppose', 9441), ('supposed', 9442), ('suppp', 9443), ('supprt', 9444), ('supps', 9445), ('suprise', 9446), ('suprised', 9447), ('sups', 9448), ('suqooa', 9449), ('sur', 9450), ('sure', 9451), ('surely', 9452), ('sureness', 9453), ('surgery', 9454), ('surivive', 9455), ('surprise', 9456), ('surprised', 9457), ('surpriseee', 9458), ('surprising', 9459), ('surrender', 9460), ('surrneder', 9461), ('surte', 9462), ('survived', 9463), ('sus', 9464), ('suspen', 9465), ('susto', 9466), ('sut', 9467), ('sutpid', 9468), ('sutralian', 9469), ('suuport', 9470), ('sux', 9471), ('sven', 9472), ('svenb', 9473), ('svinya', 9474), ('swaggy', 9475), ('swan', 9476), ('swap', 9477), ('swapping', 9478), ('swear', 9479), ('sweet', 9480), ('swimm', 9481), ('swin', 9482), ('swing', 9483), ('swisp', 9484), ('switch', 9485), ('switched', 9486), ('switggity', 9487), ('swooty', 9488), ('sword', 9489), ('sworn', 9490), ('swpt', 9491), ('swup', 9492), ('sy', 9493), ('sydney', 9494), ('syhit', 9495), ('syka', 9496), ('sylla', 9497), ('syllabear', 9498), ('syndrome', 9499), ('synergy', 9500), ('syr', 9501), ('syrum', 9502), ('system', 9503), ('sz', 9504), ('t', 9505), ('t.', 9506), ('t0', 9507), ('t1', 9508), ('t2', 9509), ('t3', 9510), ('t4', 9511), ('ta', 9512), ('tab', 9513), ('tabbed', 9514), ('tabla', 9515), ('tables', 9516), ('tac', 9517), ('tackle', 9518), ('tacnies', 9519), ('taco', 9520), ('tacos', 9521), ('tactic', 9522), ('tactical', 9523), ('tactics', 9524), ('tae', 9525), ('taem', 9526), ('taewm', 9527), ('tag', 9528), ('tagal', 9529), ('tahaha', 9530), ('tahimik', 9531), ('taht', 9532), ('tahts', 9533), ('taichi', 9534), ('taired', 9535), ('tak', 9536), ('takde', 9537), ('take', 9538), ('taken', 9539), ('takes', 9540), ('taking', 9541), ('takle', 9542), ('takoe', 9543), ('takogo', 9544), ('talaga', 9545), ('talent', 9546), ('talento', 9547), ('tales', 9548), ('talga', 9549), ('taling', 9550), ('talino', 9551), ('talk', 9552), ('talked', 9553), ('talker', 9554), ('talkin', 9555), ('talking', 9556), ('talkinga', 9557), ('talks', 9558), ('talkshits', 9559), ('talktalktalk', 9560), ('talo', 9561), ('talong', 9562), ('tam', 9563), ('tambien', 9564), ('tame', 9565), ('tammates', 9566), ('tampar', 9567), ('tamrae', 9568), ('tan', 9569), ('tancheeyuan', 9570), ('tang', 9571), ('tanga', 9572), ('tangina', 9573), ('tanginamo', 9574), ('tanginmao', 9575), ('tango', 9576), ('tangos', 9577), ('tank', 9578), ('tanked', 9579), ('tanking', 9580), ('tanks', 9581), ('tanky', 9582), ('tanto', 9583), ('tap', 9584), ('tapped', 9585), ('tard', 9586), ('tards', 9587), ('target', 9588), ('targeting', 9589), ('tarzan', 9590), ('tas', 9591), ('taste', 9592), ('tastes', 9593), ('tasty', 9594), ('tat', 9595), ('tatctis', 9596), ('tau', 9597), ('taunt', 9598), ('taxi', 9599), ('tay', 9600), ('tb', 9601), ('tbd', 9602), ('tbelieve', 9603), ('tbh', 9604), ('tc', 9605), ('tch', 9606), ('tches', 9607), ('td', 9608), ('tdat', 9609), ('te', 9610), ('tea', 9611), ('teach', 9612), ('teaching', 9613), ('team', 9614), ('team8', 9615), ('teamate', 9616), ('teamates', 9617), ('teamchat', 9618), ('teamed', 9619), ('teamfight', 9620), ('teamfighting', 9621), ('teamfights', 9622), ('teamgame', 9623), ('teamkilled', 9624), ('teamm8', 9625), ('teammate', 9626), ('teammates', 9627), ('teamplay', 9628), ('teams', 9629), ('teamwipe', 9630), ('teamwork', 9631), ('tear', 9632), ('tearing', 9633), ('tears', 9634), ('tebya', 9635), ('tech', 9636), ('techhies', 9637), ('techies', 9638), ('techis', 9639), ('tecla', 9640), ('tee', 9641), ('teeam', 9642), ('teehee', 9643), ('teeny', 9644), ('teh', 9645), ('tehn', 9646), ('tehnku', 9647), ('tekkies', 9648), ('tel', 9649), ('tele', 9650), ('teliin', 9651), ('tell', 9652), ('tellin', 9653), ('telling', 9654), ('tells', 9655), ('telstra', 9656), ('tem', 9657), ('tema', 9658), ('temammate', 9659), ('templar', 9660), ('templer', 9661), ('ten', 9662), ('teneg', 9663), ('tenemos', 9664), ('tengene', 9665), ('tengo', 9666), ('tenks', 9667), ('tenkz', 9668), ('tentando', 9669), ('teq', 9670), ('tequila', 9671), ('terash', 9672), ('terminen', 9673), ('terrible', 9674), ('terribles', 9675), ('terroblade', 9676), ('terror', 9677), ('terrorblade', 9678), ('terussss', 9679), ('tesam', 9680), ('test', 9681), ('testing', 9682), ('tetris', 9683), ('tevas', 9684), ('text', 9685), ('tf', 9686), ('tfw', 9687), ('tgate', 9688), ('tghrowers', 9689), ('tgx', 9690), ('th', 9691), ('tha', 9692), ('thaha', 9693), ('thai', 9694), ('thailand', 9695), ('thais', 9696), ('than', 9697), ('thanh', 9698), ('thank', 9699), ('thankfully', 9700), ('thanking', 9701), ('thanks', 9702), ('thankyou', 9703), ('thankyouuu', 9704), ('thas', 9705), ('that', 9706), ('thatga', 9707), ('thatm', 9708), ('thats', 9709), ('thatsw', 9710), ('thatswhy', 9711), ('thay', 9712), ('thc', 9713), ('thd', 9714), ('the', 9715), ('thebest', 9716), ('thee', 9717), ('theee', 9718), ('theese', 9719), ('thefeedisrea', 9720), ('theh', 9721), ('their', 9722), ('theirselves', 9723), ('thejm', 9724), ('them', 9725), ('themetric', 9726), ('themoreyouknow', 9727), ('then', 9728), ('there', 9729), ('therefore', 9730), ('therei', 9731), ('therer', 9732), ('theres', 9733), ('therikavidalama', 9734), ('these', 9735), ('thesee', 9736), ('they', 9737), ('theyre', 9738), ('thge', 9739), ('thgis', 9740), ('thief', 9741), ('thigns', 9742), ('thin', 9743), ('thing', 9744), ('things', 9745), ('think', 9746), ('thinke', 9747), ('thinker', 9748), ('thinkg', 9749), ('thinking', 9750), ('thinks', 9751), ('thinsk', 9752), ('third', 9753), ('thirst', 9754), ('thirsty', 9755), ('this', 9756), ('thisgame', 9757), ('thisis', 9758), ('thiss', 9759), ('thissf', 9760), ('thist', 9761), ('thistime', 9762), ('thje', 9763), ('thjis', 9764), ('thk', 9765), ('thnak', 9766), ('thnis', 9767), ('thnk', 9768), ('thnqq', 9769), ('thnx', 9770), ('tho', 9771), ('thoght', 9772), ('thoiught', 9773), ('tholugh', 9774), ('thomas', 9775), ('thooo', 9776), ('thos', 9777), ('those', 9778), ('thot', 9779), ('thou', 9780), ('though', 9781), ('thought', 9782), ('thoughts', 9783), ('thousand', 9784), ('thrashes', 9785), ('three', 9786), ('threow', 9787), ('threw', 9788), ('thrid', 9789), ('throne', 9790), ('througfh', 9791), ('through', 9792), ('throw', 9793), ('thrower', 9794), ('throwing', 9795), ('throws', 9796), ('thru', 9797), ('thrwos', 9798), ('ths', 9799), ('thsi', 9800), ('tht', 9801), ('thta', 9802), ('thua', 9803), ('thug', 9804), ('thught', 9805), ('thuis', 9806), ('thunderstorms', 9807), ('thursday', 9808), ('thus', 9809), ('thusfar', 9810), ('thuskar', 9811), ('thwn', 9812), ('thx', 9813), ('thy', 9814), ('thye', 9815), ('thz', 9816), ('thzis', 9817), ('ti', 9818), ('ti5', 9819), ('ti6', 9820), ('tickect', 9821), ('ticket', 9822), ('ticklish', 9823), ('tide', 9824), ('tider', 9825), ('tides', 9826), ('tidew', 9827), ('tidner', 9828), ('tie', 9829), ('tiene', 9830), ('tienen', 9831), ('tienes', 9832), ('tier', 9833), ('tifa', 9834), ('tiger', 9835), ('tight', 9836), ('tihnk', 9837), ('tik', 9838), ('tiki', 9839), ('til', 9840), ('till', 9841), ('tilt', 9842), ('tilted', 9843), ('tilting', 9844), ('tim', 9845), ('tima', 9846), ('timber', 9847), ('timbersaw', 9848), ('time', 9849), ('timed', 9850), ('timee', 9851), ('timeing', 9852), ('timer', 9853), ('timers', 9854), ('times', 9855), ('timing', 9856), ('tin', 9857), ('tinekr', 9858), ('ting', 9859), ('tinhker', 9860), ('tini', 9861), ('tink', 9862), ('tinke', 9863), ('tinker', 9864), ('tinny', 9865), ('tiny', 9866), ('tinyyyy', 9867), ('tiome', 9868), ('tip', 9869), ('tipical', 9870), ('tipo', 9871), ('tipu', 9872), ('tira', 9873), ('tirame', 9874), ('tired', 9875), ('tirei', 9876), ('tis', 9877), ('tissue', 9878), ('tit', 9879), ('titan', 9880), ('title', 9881), ('titling', 9882), ('tits', 9883), ('titty', 9884), ('tiwala', 9885), ('tiz', 9886), ('tj', 9887), ('tjat', 9888), ('tjhanks', 9889), ('tjhat', 9890), ('tke', 9891), ('tks', 9892), ('tlaking', 9893), ('tm', 9894), ('tmoher', 9895), ('tmr', 9896), ('tn', 9897), ('tnc', 9898), ('tng', 9899), ('tnker', 9900), ('tnks', 9901), ('tntnt', 9902), ('tnx', 9903), ('to', 9904), ('toa', 9905), ('toaday', 9906), ('tob', 9907), ('tobbe', 9908), ('tobias', 9909), ('tocan', 9910), ('tocar', 9911), ('toco', 9912), ('today', 9913), ('todl', 9914), ('todo', 9915), ('toewr', 9916), ('togehtr', 9917), ('together', 9918), ('toggle', 9919), ('toh', 9920), ('toilet', 9921), ('toimbstone', 9922), ('tok', 9923), ('tol', 9924), ('told', 9925), ('tolko', 9926), ('tolol', 9927), ('tom', 9928), ('tomb', 9929), ('tombstone', 9930), ('tommorow', 9931), ('tomorrow', 9932), ('tonight', 9933), ('tonker', 9934), ('tony', 9935), ('tonyh', 9936), ('too', 9937), ('tooeasy', 9938), ('took', 9939), ('tool', 9940), ('toon', 9941), ('tooo', 9942), ('tooooooooooooo', 9943), ('tooos', 9944), ('toos', 9945), ('top', 9946), ('topgame', 9947), ('toplane', 9948), ('topo', 9949), ('topor', 9950), ('tops', 9951), ('toque', 9952), ('torando', 9953), ('torll', 9954), ('tornade', 9955), ('tornadio', 9956), ('tornado', 9957), ('torso', 9958), ('torture', 9959), ('toss', 9960), ('tossed', 9961), ('tossing', 9962), ('tot', 9963), ('total', 9964), ('totally', 9965), ('totaly', 9966), ('totem', 9967), ('touch', 9968), ('touchdown', 9969), ('touched', 9970), ('touching', 9971), ('tough', 9972), ('tout', 9973), ('tow', 9974), ('towards', 9975), ('tower', 9976), ('towers', 9977), ('towets', 9978), ('town', 9979), ('toxi', 9980), ('toxic', 9981), ('toxics', 9982), ('toyhpj', 9983), ('tp', 9984), ('tped', 9985), ('tping', 9986), ('tps', 9987), ('tpying', 9988), ('tq', 9989), ('tqtq', 9990), ('tr', 9991), ('traash', 9992), ('trable', 9993), ('track', 9994), ('tracked', 9995), ('tracks', 9996), ('trad', 9997), ('trade', 9998), ('trademark', 9999), ('trading', 10000), ('trah', 10001), ('trahs', 10002), ('train', 10003), ('traitor', 10004), ('tramp', 10005), ('tranq', 10006), ('tranqs', 10007), ('tranquil', 10008), ('transfer', 10009), ('transh', 10010), ('transition', 10011), ('trap', 10012), ('trapp', 10013), ('trapping', 10014), ('trash', 10015), ('trashbag', 10016), ('trashcan', 10017), ('trashg', 10018), ('trashlords', 10019), ('trashtalk', 10020), ('trashtalking', 10021), ('trasktalker', 10022), ('travel', 10023), ('travels', 10024), ('trawsh', 10025), ('trax', 10026), ('traxes', 10027), ('traxex', 10028), ('treads', 10029), ('tream', 10030), ('treant', 10031), ('treants', 10032), ('treash', 10033), ('treasues', 10034), ('treatn', 10035), ('treats', 10036), ('tred', 10037), ('trede', 10038), ('tree', 10039), ('treeants', 10040), ('treee', 10041), ('trees', 10042), ('trench', 10043), ('trend', 10044), ('trentooooo', 10045), ('tress', 10046), ('tri', 10047), ('tricked', 10048), ('tricks', 10049), ('tricky', 10050), ('tried', 10051), ('tries', 10052), ('trigger', 10053), ('trihard', 10054), ('trilane', 10055), ('trilaned', 10056), ('trilaning', 10057), ('triple', 10058), ('trippers', 10059), ('trist', 10060), ('trk1j325rj123', 10061), ('trol', 10062), ('troll', 10063), ('trolled', 10064), ('trolling', 10065), ('trolls', 10066), ('tron', 10067), ('trone', 10068), ('trow', 10069), ('trsah', 10070), ('tru', 10071), ('true', 10072), ('trump', 10073), ('trust', 10074), ('truth', 10075), ('trx', 10076), ('try', 10077), ('tryahrds', 10078), ('tryed', 10079), ('tryhard', 10080), ('tryharding', 10081), ('tryhards', 10082), ('tryharidng', 10083), ('trying', 10084), ('tryna', 10085), ('ts', 10086), ('tsambahero', 10087), ('tsokotomotoko', 10088), ('tsunami', 10089), ('tsuyuyuy', 10090), ('tt', 10091), ('tteam', 10092), ('ttimes', 10093), ('ttrash', 10094), ('ttuskk', 10095), ('tu', 10096), ('tube', 10097), ('tudsun', 10098), ('tuesday', 10099), ('tuesdays', 10100), ('tuftn', 10101), ('tuiny', 10102), ('tumaba', 10103), ('tumbaron', 10104), ('tune', 10105), ('tunnel', 10106), ('turk', 10107), ('turn', 10108), ('turned', 10109), ('turns', 10110), ('tursk', 10111), ('tus', 10112), ('tusk', 10113), ('tuskar', 10114), ('tusker', 10115), ('tuskker', 10116), ('tusklar', 10117), ('tusks', 10118), ('tusky', 10119), ('tussle', 10120), ('tutututut', 10121), ('tutututututut', 10122), ('tv', 10123), ('twas', 10124), ('twat', 10125), ('twatted', 10126), ('twf', 10127), ('twice', 10128), ('twin', 10129), ('twirling', 10130), ('twist', 10131), ('twisted', 10132), ('twitch', 10133), ('twitter', 10134), ('two', 10135), ('twot', 10136), ('twq', 10137), ('twr', 10138), ('twrs', 10139), ('tx', 10140), ('ty', 10141), ('tyes', 10142), ('tyhrue', 10143), ('tyiny', 10144), ('tyou', 10145), ('type', 10146), ('typgin', 10147), ('typical', 10148), ('typing', 10149), ('typoe', 10150), ('tyrazor', 10151), ('tyty', 10152), ('tyu', 10153), ('tyy', 10154), ('u', 10155), ('u2', 10156), ('uask', 10157), ('uck', 10158), ('ucnt', 10159), ('ud', 10160), ('udah', 10161), ('uderstood', 10162), ('udh', 10163), ('udid', 10164), ('udy', 10165), ('uebishe', 10166), ('ueless', 10167), ('uf', 10168), ('ufcvk', 10169), ('ufhm', 10170), ('ug', 10171), ('ugay', 10172), ('ugh', 10173), ('ughh', 10174), ('ugly', 10175), ('uguys', 10176), ('uh', 10177), ('uheuhee', 10178), ('uhhh', 10179), ('uhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh', 10180), ('ui', 10181), ('uick', 10182), ('uifhiuwehfouihwef', 10183), ('uilt', 10184), ('uis', 10185), ('uit', 10186), ('ukltimate', 10187), ('ul', 10188), ('uless', 10189), ('ulit', 10190), ('ull', 10191), ('ulol', 10192), ('ult', 10193), ('ulted', 10194), ('ulti', 10195), ('ultie', 10196), ('ultied', 10197), ('ulties', 10198), ('ultiiiii', 10199), ('ultimate', 10200), ('ultimates', 10201), ('ulting', 10202), ('ultis', 10203), ('ulto', 10204), ('ultra', 10205), ('ultrakill', 10206), ('ults', 10207), ('ulty', 10208), ('ulul', 10209), ('um', 10210), ('umad', 10211), ('umanner', 10212), ('umaru', 10213), ('umeju', 10214), ('umr', 10215), ('umum', 10216), ('un', 10217), ('una', 10218), ('unapuse', 10219), ('unaware', 10220), ('unbalanced', 10221), ('unbeliaveblay', 10222), ('unbelievebale', 10223), ('unbelivebale', 10224), ('uncle', 10225), ('unclickable', 10226), ('und', 10227), ('undaut', 10228), ('under', 10229), ('understand', 10230), ('understands', 10231), ('undertaker', 10232), ('undiyinh', 10233), ('undiyng', 10234), ('undy', 10235), ('undying', 10236), ('undyingpicker', 10237), ('unerstand', 10238), ('unfair', 10239), ('unfortunate', 10240), ('unfortunately', 10241), ('ungas', 10242), ('uni', 10243), ('unifi', 10244), ('uninstall', 10245), ('uninstalld', 10246), ('unite', 10247), ('unity', 10248), ('univeirse', 10249), ('universe', 10250), ('unkillable', 10251), ('unknown', 10252), ('unless', 10253), ('unlike', 10254), ('unlimited', 10255), ('unlres', 10256), ('unluck', 10257), ('unlucky', 10258), ('unmanner', 10259), ('unmuted', 10260), ('unn', 10261), ('unneeded', 10262), ('unnistal', 10263), ('uno', 10264), ('unp', 10265), ('unpause', 10266), ('unpaused', 10267), ('unpauser', 10268), ('unpausers', 10269), ('unpauses', 10270), ('unpausing', 10271), ('unpauzka', 10272), ('unpaybale', 10273), ('unplayabel', 10274), ('unpoused', 10275), ('unpuase', 10276), ('unpy', 10277), ('unranked', 10278), ('unreal', 10279), ('unskill', 10280), ('unskilled', 10281), ('unstable', 10282), ('unstopdbl', 10283), ('until', 10284), ('untol', 10285), ('untouchable', 10286), ('unwinnable', 10287), ('uou', 10288), ('up', 10289), ('update', 10290), ('updates', 10291), ('upgrade', 10292), ('upgraded', 10293), ('uphill', 10294), ('upon', 10295), ('upports', 10296), ('ups', 10297), ('upset', 10298), ('uptime', 10299), ('ur', 10300), ('ure', 10301), ('urejsut', 10302), ('urge', 10303), ('urgent', 10304), ('urn', 10305), ('urod', 10306), ('urs', 10307), ('ursa', 10308), ('ursaz', 10309), ('urself', 10310), ('urselves', 10311), ('urt', 10312), ('us', 10313), ('usa', 10314), ('usage', 10315), ('usallt', 10316), ('usap', 10317), ('usck', 10318), ('use', 10319), ('used', 10320), ('useful', 10321), ('usefull', 10322), ('uselees', 10323), ('useles', 10324), ('useless', 10325), ('uselses', 10326), ('uselss', 10327), ('uselsss', 10328), ('user', 10329), ('uses', 10330), ('ushel', 10331), ('using', 10332), ('usky', 10333), ('usles', 10334), ('usless', 10335), ('ussles', 10336), ('ust', 10337), ('ustedes', 10338), ('usual', 10339), ('usually', 10340), ('ut', 10341), ('uter', 10342), ('utmost', 10343), ('utterly', 10344), ('utv', 10345), ('uu', 10346), ('uuseless', 10347), ('uuuuuu', 10348), ('uve', 10349), ('uwant', 10350), ('uwhere', 10351), ('uwon', 10352), ('uxaxa', 10353), ('uy', 10354), ('uya', 10355), ('uydi', 10356), ('uyes', 10357), ('uys', 10358), ('uyy', 10359), ('uz', 10360), ('uze', 10361), ('v', 10362), ('v1', 10363), ('va', 10364), ('vac', 10365), ('vacum', 10366), ('vacuume', 10367), ('vagi', 10368), ('vagina', 10369), ('vaginitis', 10370), ('vahahaha', 10371), ('vai', 10372), ('vajayjay', 10373), ('valar', 10374), ('valcano', 10375), ('vale', 10376), ('value', 10377), ('valve', 10378), ('vamatem', 10379), ('vanguard', 10380), ('vas', 10381), ('vbane', 10382), ('vbefore', 10383), ('vbl', 10384), ('vbs', 10385), ('vby', 10386), ('vc', 10387), ('vcs', 10388), ('vd', 10389), ('vejam', 10390), ('vendetta', 10391), ('venge', 10392), ('veno', 10393), ('venom', 10394), ('vepe', 10395), ('verde', 10396), ('veri', 10397), ('versteh', 10398), ('very', 10399), ('veryyy', 10400), ('ves', 10401), ('vete', 10402), ('vf', 10403), ('vfvrf', 10404), ('vg', 10405), ('vi', 10406), ('victory', 10407), ('videogame', 10408), ('viebite', 10409), ('viebu', 10410), ('vieja', 10411), ('vietnam', 10412), ('vigilante', 10413), ('villain', 10414), ('vinaaaaaaaaa', 10415), ('vinegar', 10416), ('vinme', 10417), ('vinter', 10418), ('violado', 10419), ('violent', 10420), ('vip', 10421), ('viper', 10422), ('vipper', 10423), ('virgin', 10424), ('virginia', 10425), ('virgins', 10426), ('virodok', 10427), ('visage', 10428), ('vision', 10429), ('viva', 10430), ('viweres', 10431), ('vk', 10432), ('vl', 10433), ('vlad', 10434), ('vlads', 10435), ('vlve', 10436), ('vod', 10437), ('vodka', 10438), ('vodkahead', 10439), ('voer', 10440), ('voice', 10441), ('void', 10442), ('voided', 10443), ('voids', 10444), ('voker', 10445), ('volteo', 10446), ('voltis', 10447), ('volume', 10448), ('volvo', 10449), ('volvooooooooooooooooooooooooooo', 10450), ('vooral', 10451), ('vote', 10452), ('voting', 10453), ('vovlo', 10454), ('vp', 10455), ('vpn', 10456), ('vroom', 10457), ('vs', 10458), ('vs1', 10459), ('vs5', 10460), ('vse', 10461), ('vseh', 10462), ('vsem', 10463), ('vsf', 10464), ('vtnc', 10465), ('vueki', 10466), ('vuelva', 10467), ('vuz', 10468), ('vwwgg', 10469), ('vy', 10470), ('w', 10471), ('w0w', 10472), ('w33', 10473), ('w3w', 10474), ('w8', 10475), ('w8ing', 10476), ('w9', 10477), ('wa', 10478), ('waaa', 10479), ('waaaaaaa', 10480), ('waaah', 10481), ('waaahahahahaaha', 10482), ('waaat', 10483), ('waahahaha', 10484), ('waaw', 10485), ('wacth', 10486), ('wadap', 10487), ('waddafaaaak', 10488), ('waddup', 10489), ('wadhawdujaw', 10490), ('wae', 10491), ('wag', 10492), ('wagaga', 10493), ('wagger', 10494), ('wagon', 10495), ('wah', 10496), ('wahah', 10497), ('wahaha', 10498), ('wahahaha', 10499), ('wahahahah', 10500), ('wahahahhahaa', 10501), ('wahahgah', 10502), ('wahahha', 10503), ('wahahhah', 10504), ('wahahhaha', 10505), ('wahha', 10506), ('waht', 10507), ('wahts', 10508), ('wai', 10509), ('waiat', 10510), ('waifu', 10511), ('waiitng', 10512), ('wail', 10513), ('wait', 10514), ('waitcan', 10515), ('waite', 10516), ('waited', 10517), ('waiting', 10518), ('waits', 10519), ('waiy', 10520), ('waja', 10521), ('wak', 10522), ('wakaka', 10523), ('wakas', 10524), ('wake', 10525), ('wakle', 10526), ('wala', 10527), ('walamak', 10528), ('walang', 10529), ('walao', 10530), ('waldo', 10531), ('walk', 10532), ('walked', 10533), ('walking', 10534), ('wall', 10535), ('wallhacks', 10536), ('walllls', 10537), ('walrus', 10538), ('wan', 10539), ('wana', 10540), ('wand', 10541), ('wanna', 10542), ('wannabe', 10543), ('wannabve', 10544), ('want', 10545), ('wanted', 10546), ('wanting', 10547), ('wanto', 10548), ('wants', 10549), ('wao', 10550), ('waoit', 10551), ('waot', 10552), ('waow', 10553), ('war', 10554), ('ward', 10555), ('warddd', 10556), ('wardea', 10557), ('warded', 10558), ('warder', 10559), ('warding', 10560), ('wardinmg', 10561), ('wards', 10562), ('wardz', 10563), ('wared', 10564), ('warlock', 10565), ('warlok', 10566), ('warrior', 10567), ('wars', 10568), ('warsd', 10569), ('warudo', 10570), ('warum', 10571), ('was', 10572), ('wasfucking', 10573), ('wash', 10574), ('washington', 10575), ('washroom', 10576), ('wasn', 10577), ('wasnt', 10578), ('wass', 10579), ('waste', 10580), ('wasted', 10581), ('wasteour', 10582), ('wasting', 10583), ('wastn', 10584), ('wat', 10585), ('watafak', 10586), ('watashiwaa', 10587), ('watch', 10588), ('watched', 10589), ('watchin', 10590), ('watching', 10591), ('wated', 10592), ('water', 10593), ('watermelon', 10594), ('wating', 10595), ('wats', 10596), ('watta', 10597), ('wauiting', 10598), ('waut', 10599), ('wave', 10600), ('waved', 10601), ('waves', 10602), ('waw', 10603), ('wawa', 10604), ('way', 10605), ('waya', 10606), ('ways', 10607), ('wazzup', 10608), ('wb', 10609), ('wbahahha', 10610), ('wbghat', 10611), ('wbu', 10612), ('wc', 10613), ('wc3', 10614), ('wd', 10615), ('wdc', 10616), ('we', 10617), ('weaboo', 10618), ('weak', 10619), ('weakest', 10620), ('weapon', 10621), ('wear', 10622), ('wearing', 10623), ('weather', 10624), ('weave', 10625), ('weaver', 10626), ('weavere', 10627), ('web', 10628), ('website', 10629), ('wed', 10630), ('weeb', 10631), ('weee', 10632), ('weeee', 10633), ('week', 10634), ('weeks', 10635), ('weh', 10636), ('wehat', 10637), ('wehn', 10638), ('wehw', 10639), ('weigh', 10640), ('weird', 10641), ('weirdest', 10642), ('wejiahbekjebqkjedbqkjd', 10643), ('welcome', 10644), ('welcum', 10645), ('well', 10646), ('welll', 10647), ('wellplayed', 10648), ('welp', 10649), ('wen', 10650), ('went', 10651), ('weooh', 10652), ('wep', 10653), ('wepe', 10654), ('wer', 10655), ('were', 10656), ('werent', 10657), ('werk', 10658), ('werth', 10659), ('west', 10660), ('westeurope', 10661), ('wet', 10662), ('wevaer', 10663), ('weve', 10664), ('wew', 10665), ('wewe', 10666), ('wewerw', 10667), ('wewewew', 10668), ('wewo', 10669), ('wewt', 10670), ('wex', 10671), ('wez', 10672), ('wfepi', 10673), ('wh', 10674), ('wha', 10675), ('whaaaaaaaaaaaat', 10676), ('whaaaaaaat', 10677), ('whahah', 10678), ('whahaha', 10679), ('whahahaha', 10680), ('whahahahaha', 10681), ('whahahha', 10682), ('whant', 10683), ('what', 10684), ('whata', 10685), ('whatever', 10686), ('whats', 10687), ('whattagame', 10688), ('whattttttttttttttttttttttttttt', 10689), ('whatup', 10690), ('whatver', 10691), ('whaty', 10692), ('when', 10693), ('whenever', 10694), ('where', 10695), ('whereare', 10696), ('whered', 10697), ('wheree', 10698), ('whereeee', 10699), ('wheres', 10700), ('whhy', 10701), ('which', 10702), ('whiel', 10703), ('while', 10704), ('whine', 10705), ('whiners', 10706), ('whingeing', 10707), ('whingey', 10708), ('whining', 10709), ('whiningf', 10710), ('whiny', 10711), ('whipe', 10712), ('whirlpool', 10713), ('whisp', 10714), ('whit', 10715), ('white', 10716), ('who', 10717), ('whoa', 10718), ('whocares', 10719), ('whoever', 10720), ('whole', 10721), ('wholle', 10722), ('whom', 10723), ('whoooooooooooo', 10724), ('whooow', 10725), ('whoop', 10726), ('whoops', 10727), ('whore', 10728), ('whores', 10729), ('whos', 10730), ('whose', 10731), ('whould', 10732), ('whow', 10733), ('whrre', 10734), ('wht', 10735), ('whthapnd', 10736), ('whurzen', 10737), ('whut', 10738), ('whuuuuuuut', 10739), ('whwere', 10740), ('why', 10741), ('whys', 10742), ('whyy', 10743), ('whyyy', 10744), ('whyyyy', 10745), ('whyyyyyyyyyy', 10746), ('whyyyyyyyyyyy', 10747), ('whyyyyyyyyyyyyy', 10748), ('whyyyyyyyyyyyyyyyyyyyyyyyy', 10749), ('wi', 10750), ('wiat', 10751), ('wicp', 10752), ('wif', 10753), ('wife', 10754), ('wifi', 10755), ('wiht', 10756), ('wihtout', 10757), ('wii', 10758), ('wil', 10759), ('wild', 10760), ('wildkin', 10761), ('will', 10762), ('willen', 10763), ('willing', 10764), ('willw', 10765), ('wim', 10766), ('win', 10767), ('wind', 10768), ('winde', 10769), ('windows', 10770), ('windranger', 10771), ('windranner', 10772), ('windrun', 10773), ('windrunner', 10774), ('wings', 10775), ('winnable', 10776), ('winner', 10777), ('winnign', 10778), ('winning', 10779), ('winrate', 10780), ('winrun', 10781), ('wins', 10782), ('winsgaes', 10783), ('winte', 10784), ('wintendo', 10785), ('winter', 10786), ('wioth', 10787), ('wipe', 10788), ('wipeout', 10789), ('wipi', 10790), ('wipo', 10791), ('wipping', 10792), ('wirdo', 10793), ('wirht', 10794), ('wirth', 10795), ('wisdom', 10796), ('wise', 10797), ('wish', 10798), ('wishes', 10799), ('wisp', 10800), ('wispa', 10801), ('wisped', 10802), ('wispo', 10803), ('wit', 10804), ('witch', 10805), ('with', 10806), ('withj', 10807), ('without', 10808), ('witing', 10809), ('wits', 10810), ('wivern', 10811), ('wiw', 10812), ('wizard', 10813), ('wjahaha', 10814), ('wjat', 10815), ('wjhat', 10816), ('wjo', 10817), ('wjore', 10818), ('wk', 10819), ('wkdopqwkdqwp', 10820), ('wkowkw', 10821), ('wkwk', 10822), ('wkwkw', 10823), ('wkwkwk', 10824), ('wkwkwkkw', 10825), ('wkwkwkwkw', 10826), ('wkwkwkwkwk', 10827), ('wkwwk', 10828), ('wlangbawas', 10829), ('wlel', 10830), ('wll', 10831), ('wlwlwlwllww', 10832), ('wnna', 10833), ('wo', 10834), ('woa', 10835), ('woaaah', 10836), ('woah', 10837), ('woaw', 10838), ('wodden', 10839), ('woden', 10840), ('wodota', 10841), ('wods', 10842), ('wohooo', 10843), ('wohoooooooooo', 10844), ('woierjewkhfiwepojoiwehfijwer', 10845), ('wok', 10846), ('wole', 10847), ('wolf', 10848), ('wolfie', 10849), ('wollongong', 10850), ('wolrd', 10851), ('woman', 10852), ('wombo', 10853), ('won', 10854), ('wond', 10855), ('wonder', 10856), ('wondering', 10857), ('wonderland', 10858), ('wont', 10859), ('woo', 10860), ('woobshe', 10861), ('wood', 10862), ('woodden', 10863), ('wooden', 10864), ('woods', 10865), ('wooho', 10866), ('woohoo', 10867), ('woohooooo', 10868), ('wooo', 10869), ('wooohoo', 10870), ('woooo', 10871), ('wooooo', 10872), ('woooooooaw', 10873), ('woooooooo', 10874), ('woooooooooooo', 10875), ('woooooooooooooooooo', 10876), ('wooooooooooooow', 10877), ('wooooow', 10878), ('woooop', 10879), ('woooow', 10880), ('woop', 10881), ('woops', 10882), ('woow', 10883), ('wop', 10884), ('word', 10885), ('words', 10886), ('work', 10887), ('worked', 10888), ('workin', 10889), ('working', 10890), ('works', 10891), ('world', 10892), ('wornggg', 10893), ('worries', 10894), ('worry', 10895), ('worrying', 10896), ('worse', 10897), ('worst', 10898), ('worsth', 10899), ('wort', 10900), ('worth', 10901), ('worthit', 10902), ('worthless', 10903), ('worthy', 10904), ('wot', 10905), ('wotah', 10906), ('woth', 10907), ('wotn', 10908), ('would', 10909), ('woulda', 10910), ('wouldfk', 10911), ('wouldndt', 10912), ('wouldnt', 10913), ('wouldve', 10914), ('wounds', 10915), ('wow', 10916), ('wowo', 10917), ('wowow', 10918), ('wowowo', 10919), ('wowowoowoww', 10920), ('wowowowo', 10921), ('wowowowow', 10922), ('woww', 10923), ('wowww', 10924), ('wp', 10925), ('wp2', 10926), ('wpe', 10927), ('wpee', 10928), ('wpgg', 10929), ('wpijwpeifg', 10930), ('wpo', 10931), ('wpould', 10932), ('wpp', 10933), ('wpwp', 10934), ('wpwpw', 10935), ('wqewi', 10936), ('wqill', 10937), ('wr', 10938), ('wrap', 10939), ('wrath', 10940), ('wrd', 10941), ('wreck', 10942), ('wrecked', 10943), ('wrecking', 10944), ('wreking', 10945), ('wrekt', 10946), ('wrestling', 10947), ('wrg', 10948), ('write', 10949), ('writed', 10950), ('writing', 10951), ('writting', 10952), ('wrking', 10953), ('wrng', 10954), ('wrok', 10955), ('wron', 10956), ('wrond', 10957), ('wrong', 10958), ('wrongggg', 10959), ('wrote', 10960), ('wrrou', 10961), ('wrs', 10962), ('wrtf', 10963), ('wrwrw', 10964), ('wsilencer', 10965), ('wsip', 10966), ('wsp', 10967), ('wstf', 10968), ('wstyd', 10969), ('wt', 10970), ('wtb', 10971), ('wtdf', 10972), ('wtf', 10973), ('wtfast', 10974), ('wtff', 10975), ('wtfff', 10976), ('wtffff', 10977), ('wtfg', 10978), ('wtfuck', 10979), ('wtg', 10980), ('wtgf', 10981), ('wth', 10982), ('wtrf', 10983), ('wtrgf', 10984), ('wtslow', 10985), ('wtv', 10986), ('wu', 10987), ('wub', 10988), ('wud', 10989), ('wudlve', 10990), ('wudup', 10991), ('wuhahsduasd', 10992), ('wuith', 10993), ('wuitters', 10994), ('wun', 10995), ('wut', 10996), ('wutever', 10997), ('wutface', 10998), ('wuzzy', 10999), ('ww', 11000), ('ww3w', 11001), ('wwat', 11002), ('wwe', 11003), ('wwpwwpwpwp', 11004), ('www', 11005), ('wwwwwwwwwwiiiiiiiiiiiiiiiiii', 11006), ('wy', 11007), ('wyvern', 11008), ('wyverns', 11009), ('wywern', 11010), ('wz', 11011), ('x', 11012), ('xa', 11013), ('xaaxx', 11014), ('xanh', 11015), ('xart', 11016), ('xarub', 11017), ('xaxa', 11018), ('xaxaaxax', 11019), ('xaxaxa', 11020), ('xaxaxax', 11021), ('xaxaxaxa', 11022), ('xaxaxaxax', 11023), ('xaxaxaxaxaxa', 11024), ('xaxaxaxaxaxaxax', 11025), ('xaxaxaxaxaxaxaxax', 11026), ('xaxaxaxaxaxaxaxaxa', 11027), ('xboct', 11028), ('xd', 11029), ('xdd', 11030), ('xddd', 11031), ('xdddd', 11032), ('xddddd', 11033), ('xddddddd', 11034), ('xdddddddd', 11035), ('xdddddddddd', 11036), ('xdddddddddddd', 11037), ('xdddddddddddddddddd', 11038), ('xdddddddddddddddddddddddd', 11039), ('xddddddddddddddddddddddddd', 11040), ('xdddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddd', 11041), ('xdlmfao', 11042), ('xdx', 11043), ('xdxd', 11044), ('xdxdxd', 11045), ('xhamster', 11046), ('xo', 11047), ('xonshaotmare', 11048), ('xp', 11049), ('xpm', 11050), ('xsend', 11051), ('xu', 11052), ('xuan', 11053), ('xx', 11054), ('xxddd', 11055), ('xyilo', 11056), ('xzxaaxaxaxaxaxaxa', 11057), ('y', 11058), ('y3a', 11059), ('ya', 11060), ('yaaaaaaaaaa', 11061), ('yaah', 11062), ('yaeh', 11063), ('yah', 11064), ('yahh', 11065), ('yall', 11066), ('yalll', 11067), ('yalnext', 11068), ('yamam', 11069), ('yan', 11070), ('yap', 11071), ('yaphets', 11072), ('yas', 11073), ('yasha', 11074), ('yasno', 11075), ('yawa', 11076), ('yawn', 11077), ('yay', 11078), ('yaya', 11079), ('yayaya', 11080), ('yayayayaa', 11081), ('yaysayaay', 11082), ('ye', 11083), ('yea', 11084), ('yeaaa', 11085), ('yeaaaah', 11086), ('yeaahhhhhhhhh', 11087), ('yeah', 11088), ('yeah2', 11089), ('yeahb', 11090), ('yeahhh', 11091), ('yeahhhhhhhhhh', 11092), ('yeahj', 11093), ('yeajj', 11094), ('yeanhj', 11095), ('yeap', 11096), ('year', 11097), ('years', 11098), ('yebok', 11099), ('yee', 11100), ('yeee', 11101), ('yeeeeeeeeeeeeeeeep', 11102), ('yeeha', 11103), ('yeezys', 11104), ('yeh', 11105), ('yeha', 11106), ('yektdsq', 11107), ('yeling', 11108), ('yell', 11109), ('yelling', 11110), ('yelloqw', 11111), ('yellow', 11112), ('yep', 11113), ('yer', 11114), ('yes', 11115), ('yesss', 11116), ('yessssssssssssss', 11117), ('yessssssssssssssssss', 11118), ('yesterday', 11119), ('yet', 11120), ('yeup', 11121), ('yewa', 11122), ('yey', 11123), ('yeyah', 11124), ('yeye', 11125), ('yf', 11126), ('yh', 11127), ('yie', 11128), ('yiyi', 11129), ('yje', 11130), ('yjhv', 11131), ('ylmao', 11132), ('ynf', 11133), ('yo', 11134), ('yolo', 11135), ('yolol', 11136), ('yoo', 11137), ('yooohoo', 11138), ('yoor', 11139), ('yoru', 11140), ('yoruself', 11141), ('yoshi', 11142), ('you', 11143), ('youc', 11144), ('youi', 11145), ('youjizz', 11146), ('youknow', 11147), ('youl', 11148), ('youll', 11149), ('youmadbro', 11150), ('young', 11151), ('youporn', 11152), ('your', 11153), ('youre', 11154), ('yours', 11155), ('yourself', 11156), ('yourselves', 11157), ('youtube', 11158), ('youve', 11159), ('youy', 11160), ('yozu', 11161), ('yqakmsd', 11162), ('yr', 11163), ('yrs', 11164), ('yt', 11165), ('ytepyepyepp', 11166), ('ytolyu', 11167), ('ytou', 11168), ('yu', 11169), ('yuep', 11170), ('yuked', 11171), ('yuki', 11172), ('yum', 11173), ('yummy', 11174), ('yun', 11175), ('yung', 11176), ('yuo', 11177), ('yuor', 11178), ('yuoyu', 11179), ('yup', 11180), ('yurnero', 11181), ('yusss', 11182), ('yuuo', 11183), ('ywa', 11184), ('yxcv', 11185), ('yy', 11186), ('yyy', 11187), ('yyyyyyy', 11188), ('z', 11189), ('za', 11190), ('zaaaaaaaaaaaaaaaaaaaaaa', 11191), ('zaebal', 11192), ('zaglod', 11193), ('zail', 11194), ('zakonjelas', 11195), ('zaoszczedzony', 11196), ('zaruinil', 11197), ('zayehal', 11198), ('zbs', 11199), ('ze', 11200), ('zel', 11201), ('zenokaia', 11202), ('zenokaiais', 11203), ('zero', 11204), ('zeros', 11205), ('zeuiz', 11206), ('zeus', 11207), ('zeusss', 11208), ('zeyus', 11209), ('zez', 11210), ('zhdu', 11211), ('zica', 11212), ('ziga', 11213), ('zl', 11214), ('zmuffinman', 11215), ('znaete', 11216), ('zombi', 11217), ('zombie', 11218), ('zone', 11219), ('zones', 11220), ('zoo', 11221), ('zoos', 11222), ('zoro', 11223), ('zuers', 11224), ('zues', 11225), ('zz', 11226), ('zzz', 11227), ('zzzz', 11228), ('zzzzz', 11229), ('zzzzzzz', 11230), ('zzzzzzzzz', 11231), ('zzzzzzzzzz', 11232), ('zzzzzzzzzzz', 11233), ('zzzzzzzzzzzz', 11234), ('zzzzzzzzzzzzzzz', 11235), ('zzzzzzzzzzzzzzzzzzzz', 11236), ('zzzzzzzzzzzzzzzzzzzzzzzz', 11237), ('zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz', 11238), ('|', 11239)])\n","11240\n"]}],"source":["print(word_dict.items())\n","print(len(word_dict))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1653909401986,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"TYN2x40R84uz","outputId":"d28ebc5e-2cf8-470e-93d1-1969793e7714"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  import sys\n"]}],"source":["emb_table = []\n","\n","w2v_sg_emb_dim=w2v_sg_model.vector_size\n","\n","for i, word in enumerate(word_list):\n","    if word in w2v_sg_model:\n","        emb_table.append(w2v_sg_model[word])\n","    else:\n","        # The pretrained glove twitter does not contain the embeddings for the [PAD] and [UNKNOWN] tokens we defined\n","        # Here, we just use all 0 for both [PAD] and [UNKNOWN] tokens for simplicity\n","        emb_table.append([0]*w2v_sg_emb_dim)\n","w2v_sg_emb_table = np.array(emb_table).astype('float')\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":434,"status":"ok","timestamp":1653827036579,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"FQ2zSBjLjmIC","outputId":"8aef926a-9f57-424d-ba7a-4e77aacfbdaf"},"outputs":[{"name":"stdout","output_type":"stream","text":["The shape of input embedding table is:  (11240, 200)\n"]}],"source":["print('The shape of input embedding table is: ', w2v_sg_emb_table.shape)"]},{"cell_type":"markdown","source":["###FastText"],"metadata":{"id":"8Z35BCZbRvvB"}},{"cell_type":"code","source":["w2v_ft_model = FastText(tokensized_sents_full, size=200, window=5, min_count=1, workers=4, sg=1)\n","\n","emb_table = []\n","\n","w2v_ft_emb_dim=w2v_ft_model.vector_size\n","\n","for i, word in enumerate(word_list):\n","    if word in w2v_ft_model:\n","        emb_table.append(w2v_ft_model[word])\n","    else:\n","        # The pretrained glove twitter does not contain the embeddings for the [PAD] and [UNKNOWN] tokens we defined\n","        # Here, we just use all 0 for both [PAD] and [UNKNOWN] tokens for simplicity\n","        emb_table.append([0]*w2v_ft_emb_dim)\n","w2v_ft_emb_table = np.array(emb_table).astype('float')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BjCcOW86RzV_","executionInfo":{"status":"ok","timestamp":1654070858649,"user_tz":-600,"elapsed":6269,"user":{"displayName":"roger wang","userId":"04313800435014207614"}},"outputId":"db15a01a-1d47-4eac-f508-5d00075170df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  if __name__ == '__main__':\n"]}]},{"cell_type":"markdown","metadata":{"id":"uoHJTpfEHvAo"},"source":["##Domain Feature Embedding\n","e.g. Your own new feature embedding to solve this in-game chat word slot filling(tagging)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":912,"status":"ok","timestamp":1654073323109,"user":{"displayName":"roger wang","userId":"04313800435014207614"},"user_tz":-600},"id":"Iow_9bi6iTUw","outputId":"e1334ba3-e192-409e-b258-db0782f9aaca"},"outputs":[{"output_type":"stream","name":"stdout","text":["5435\n"]}],"source":["import json\n","\n","with open('/content/drive/MyDrive/COMP5046-NLP-AS2/Data/domain_docs.json','r') as f:\n","  data=json.load(f)\n","\n","domain_docs=(data['data'])\n","print(len(domain_docs))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_EOgOMRNiTRm"},"outputs":[],"source":["#train a skip-gram W2V model\n","# training corpus = domain_docs + full tokenized sentences\n","domain_sg_model = Word2Vec(domain_docs+tokensized_sents_full, size=200, window=5, min_count=1, workers=4, sg=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":480,"status":"ok","timestamp":1654073334076,"user":{"displayName":"roger wang","userId":"04313800435014207614"},"user_tz":-600},"id":"wc9y_BfDiTOM","outputId":"4cca694a-1923-4e2e-939b-9ebe7c649e80"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  import sys\n"]}],"source":["emb_table = []\n","\n","domain_sg_emb_dim=domain_sg_model.vector_size\n","\n","for i, word in enumerate(word_list):\n","    if word in domain_sg_model:\n","        emb_table.append(domain_sg_model[word])\n","    else:\n","        # The pretrained glove twitter does not contain the embeddings for the [PAD] and [UNKNOWN] tokens we defined\n","        # Here, we just use all 0 for both [PAD] and [UNKNOWN] tokens for simplicity\n","        emb_table.append([0]*domain_sg_emb_dim)\n","domain_sg_emb_table = np.array(emb_table).astype('float')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":307,"status":"ok","timestamp":1653889258220,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"02mx1N4SiTLL","outputId":"27bb881f-fba9-4082-e3dd-eec10e9fb703"},"outputs":[{"output_type":"stream","name":"stdout","text":["(11240, 200)\n"]}],"source":["print(domain_sg_emb_table.shape)"]},{"cell_type":"markdown","metadata":{"id":"aCHxubQ3yAe8"},"source":["### Concatenate POS +W2V"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"924EzG2PyVQ3"},"outputs":[],"source":["#this concatenate is from the embedding table level - POS table + w2v table\n","w2v_POS_emb_table=np.concatenate((w2v_sg_emb_table,POS_embedding_matrix),axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1653827046529,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"Bbhr9oLC0dyr","outputId":"b38e0aff-45c9-419a-f43f-64343a3825db"},"outputs":[{"data":{"text/plain":["(11240, 217)"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["w2v_POS_emb_table.shape"]},{"cell_type":"markdown","source":["### Concatenate POS + FastText"],"metadata":{"id":"OaeKQO1iccL9"}},{"cell_type":"code","source":["ft_POS_emb_table=np.concatenate((w2v_ft_emb_table,POS_embedding_matrix),axis=1)"],"metadata":{"id":"eWsRffM7ciH7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xRLwLRyriTFx"},"source":["### Concatenate POS + domain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1653909411926,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"hUMoE-WKiXO1","outputId":"b5dec85d-b149-4568-c0be-363adc892569"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(11240, 217)"]},"metadata":{},"execution_count":20}],"source":["POS_domain_emb_table=np.concatenate((POS_embedding_matrix,domain_sg_emb_table),axis=1)\n","POS_domain_emb_table.shape"]},{"cell_type":"markdown","metadata":{"id":"WlP9sywKJEFo"},"source":["### Concatenate POS + w2v + domain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1653909413328,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"6Djwt-WXiSmM","outputId":"4eb82d6b-e420-4cb6-ce5c-26788bd105d4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(11240, 417)"]},"metadata":{},"execution_count":21}],"source":["w2v_POS_domain_emb_table=np.concatenate((w2v_POS_emb_table,domain_sg_emb_table),axis=1)\n","w2v_POS_domain_emb_table.shape"]},{"cell_type":"markdown","source":["### Concatenate POS + ft + domain"],"metadata":{"id":"ok_LmdMqcAy_"}},{"cell_type":"code","source":["ft_POS_domain_emb_table=np.concatenate((ft_POS_emb_table,domain_sg_emb_table),axis=1)\n","ft_POS_domain_emb_table.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KV1-NOt9cAaP","executionInfo":{"status":"ok","timestamp":1654073339263,"user_tz":-600,"elapsed":282,"user":{"displayName":"roger wang","userId":"04313800435014207614"}},"outputId":"cd9ded84-415b-4be7-c100-5ca2a0bf04af"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(11240, 417)"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"rubQ2dOI_r_K"},"source":["#Helper function to make the code more readable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q4zPg-64_zKD"},"outputs":[],"source":["def argmax(vec):\n","    # return the argmax as a python int\n","    _, idx = torch.max(vec, 1)\n","    return idx.item()\n","\n","\n","def prepare_sequence(seq, to_ix):\n","    idxs = [to_ix[w] for w in seq]\n","    return torch.tensor(idxs, dtype=torch.long)\n","\n","\n","# Compute log sum exp in a numerically stable way for the forward algorithm\n","def log_sum_exp(vec):\n","    max_score = vec[0, argmax(vec)]\n","    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n","    return max_score + \\\n","        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n","\n","#cal_acc \n","def cal_acc(model, input_index, output_index):\n","    ground_truth = []\n","    predicted = []\n","    for i,idxs in enumerate(input_index):\n","        ground_truth += output_index[i]\n","        score, pred = model(torch.tensor(idxs, dtype=torch.long).to(device))\n","        predicted += pred\n","    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n","    return predicted, ground_truth, accuracy"]},{"cell_type":"markdown","metadata":{"id":"hbIAPo-1Bc0e"},"source":["# Baseline Model-Bi-LSTM with CRF\n","Copy the code from previous notbook lab"]},{"cell_type":"markdown","metadata":{"id":"RHFOksBw_4WD"},"source":["##Build baseline Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x_GLj3FC_307"},"outputs":[],"source":["# construct baseline model\n","class BiLSTM_CRF(nn.Module):\n","\n","    def __init__(self, vocab_size, tag_to_ix,embedding_dim, hidden_dim):\n","        super(BiLSTM_CRF, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.vocab_size = vocab_size\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","\n","        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n","        \n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n","                            num_layers=1, bidirectional=True)\n","\n","        # Maps the output of the LSTM into tag space.\n","        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n","\n","        # Matrix of transition parameters.  Entry i,j is the score of\n","        # transitioning *to* i *from* j.\n","        self.transitions = nn.Parameter(\n","            torch.randn(self.tagset_size, self.tagset_size))\n","\n","        # These two statements enforce the constraint that we never transfer\n","        # to the start tag and we never transfer from the stop tag\n","        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n","        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n","\n","        self.hidden = self.init_hidden()\n","\n","    def init_hidden(self):\n","        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n","                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n","\n","    def _forward_alg(self, feats):\n","        # Do the forward algorithm to compute the partition function\n","        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n","        # START_TAG has all of the score.\n","        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n","\n","        # Wrap in a variable so that we will get automatic backprop\n","        forward_var = init_alphas\n","\n","        # Iterate through the sentence\n","        for feat in feats:\n","            alphas_t = []  # The forward tensors at this timestep\n","            for next_tag in range(self.tagset_size):\n","                # broadcast the emission score: it is the same regardless of\n","                # the previous tag\n","                emit_score = feat[next_tag].view(\n","                    1, -1).expand(1, self.tagset_size)\n","                # the ith entry of trans_score is the score of transitioning to\n","                # next_tag from i\n","                trans_score = self.transitions[next_tag].view(1, -1)\n","                # The ith entry of next_tag_var is the value for the\n","                # edge (i -> next_tag) before we do log-sum-exp\n","                next_tag_var = forward_var + trans_score + emit_score\n","                # The forward variable for this tag is log-sum-exp of all the\n","                # scores.\n","                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n","            forward_var = torch.cat(alphas_t).view(1, -1)\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        alpha = log_sum_exp(terminal_var)\n","        return alpha\n","\n","    def _get_lstm_features(self, sentence):\n","        self.hidden = self.init_hidden()\n","        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n","        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n","        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n","        lstm_feats = self.hidden2tag(lstm_out)\n","        return lstm_feats\n","\n","    def _score_sentence(self, feats, tags):\n","        # Gives the score of a provided tag sequence\n","        score = torch.zeros(1).to(device)\n","        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n","        for i, feat in enumerate(feats):\n","            score = score + \\\n","                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n","        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n","        return score\n","\n","    def _viterbi_decode(self, feats):\n","        backpointers = []\n","\n","        # Initialize the viterbi variables in log space\n","        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n","        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n","\n","        # forward_var at step i holds the viterbi variables for step i-1\n","        forward_var = init_vvars\n","        for feat in feats:\n","            bptrs_t = []  # holds the backpointers for this step\n","            viterbivars_t = []  # holds the viterbi variables for this step\n","\n","            for next_tag in range(self.tagset_size):\n","                # next_tag_var[i] holds the viterbi variable for tag i at the\n","                # previous step, plus the score of transitioning\n","                # from tag i to next_tag.\n","                # We don't include the emission scores here because the max\n","                # does not depend on them (we add them in below)\n","                next_tag_var = forward_var + self.transitions[next_tag]\n","                best_tag_id = argmax(next_tag_var)\n","                bptrs_t.append(best_tag_id)\n","                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n","            # Now add in the emission scores, and assign forward_var to the set\n","            # of viterbi variables we just computed\n","            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n","            backpointers.append(bptrs_t)\n","\n","        # Transition to STOP_TAG\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        best_tag_id = argmax(terminal_var)\n","        path_score = terminal_var[0][best_tag_id]\n","\n","        # Follow the back pointers to decode the best path.\n","        best_path = [best_tag_id]\n","        for bptrs_t in reversed(backpointers):\n","            best_tag_id = bptrs_t[best_tag_id]\n","            best_path.append(best_tag_id)\n","        # Pop off the start tag (we dont want to return that to the caller)\n","        start = best_path.pop()\n","        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n","        best_path.reverse()\n","        return path_score, best_path\n","\n","    def neg_log_likelihood(self, sentence, tags):\n","        feats = self._get_lstm_features(sentence)\n","        forward_score = self._forward_alg(feats)\n","        gold_score = self._score_sentence(feats, tags)\n","        return forward_score - gold_score\n","\n","    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n","        # Get the emission scores from the BiLSTM\n","        lstm_feats = self._get_lstm_features(sentence)\n","\n","        # Find the best path, given the features.\n","        score, tag_seq = self._viterbi_decode(lstm_feats)\n","        return score, tag_seq"]},{"cell_type":"markdown","metadata":{"id":"OYL1200qi45O"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vN2UjE7l_rbD"},"outputs":[],"source":["# use w2v_sg_emb_table as embedding table & num layer = 1\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","HIDDEN_DIM = 50\n","EMBEDDING_DIM=200\n","\n","model = BiLSTM_CRF(len(word_dict), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM).to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1098330,"status":"ok","timestamp":1653810886764,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"Qk7XCnrcWD8g","outputId":"b8d44907-8a07-402d-9d8f-79523b187c50"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch:1, Training loss: 21611.06, train acc: 0.9877, val loss: 2477.36, val acc: 0.9843, time: 549.16s\n","Epoch:2, Training loss: 4805.30, train acc: 0.9951, val loss: 1749.16, val acc: 0.9906, time: 548.83s\n"]}],"source":["# Training\n","import datetime\n","\n","for epoch in range(2):  \n","    time1 = datetime.datetime.now()\n","    train_loss = 0\n","\n","    model.train()\n","    for i, idxs in enumerate(train_input_index):\n","        tags_index = train_output_index[i]\n","\n","        # Step 1. Remember that Pytorch accumulates gradients.\n","        # We need to clear them out before each instance\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is,\n","        # turn them into Tensors of word indices.\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","\n","        # Step 3. Run our forward pass.\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        # calling optimizer.step()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss+=loss.item()\n","\n","    model.eval()\n","    # Call the cal_acc functions you implemented as required\n","    _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n","    _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n","\n","    val_loss = 0\n","    for i, idxs in enumerate(val_input_index):\n","        tags_index = val_output_index[i]\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","        val_loss+=loss.item()\n","    time2 = datetime.datetime.now()\n","\n","    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n"]},{"cell_type":"markdown","metadata":{"id":"zUZqLtn6H-DN"},"source":["##Save and load model"]},{"cell_type":"code","source":["torch.save(model,'/content/drive/MyDrive/COMP5046-NLP-AS2/Models/baseline_CRF_bilstm_model_1.pt')"],"metadata":{"id":"4jjtJlarfEq9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AN5uVk-uZMq-"},"outputs":[],"source":["baseline_CRF_bilstm_model_1=torch.load('/content/drive/MyDrive/COMP5046-NLP-AS2/Models/baseline_CRF_bilstm_model_1.pt')"]},{"cell_type":"markdown","metadata":{"id":"QrvSyVn6aFSS"},"source":["##Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":326,"status":"ok","timestamp":1653798627043,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"GOYlgHBpUmIy","outputId":"4e892cf2-b295-402c-b353-e2c19092b2a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["(tensor(90.9989, device='cuda:0'), [4, 5, 4, 3, 2, 4, 5, 2])\n"]}],"source":["# Check predictions after training e.g tokensized_training_docs[8]\n","with torch.no_grad():\n","    precheck_sent = prepare_sequence(tokensized_training_docs[8], word_dict).to(device)\n","    precheck_tags = torch.tensor([tag_to_ix[t] for t in tokensized_training_label_docs[8]], dtype=torch.long).to(device)\n","    print(model(precheck_sent))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":303,"status":"ok","timestamp":1653798629928,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"ytZ9KFWaYNHq","outputId":"e2964d61-7e22-4ad6-efca-c5524f8b8e90"},"outputs":[{"name":"stdout","output_type":"stream","text":["[4, 5, 4, 3, 2, 4, 5, 2]\n","['S', 'SEPA', 'S', 'P', 'O', 'S', 'SEPA', 'O']\n"]}],"source":["print(train_output_index[8])\n","print(tokensized_training_label_docs[8])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mrj4R7qSEB9R"},"outputs":[],"source":["baseline_CRF_bilstm_model_1.eval()\n","predict=[]\n","true_label =[]\n","for sentence, tags in zip(tokensized_validation_docs,tokensized_validation_label_docs):\n","  sentence_in = prepare_sequence(sentence, word_dict).to(device)\n","  targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long).to(device)\n","  _, y_pred=baseline_CRF_bilstm_model_1(sentence_in)\n","  predict.extend(y_pred)\n","  true_label.extend(targets.cpu().numpy().tolist())\n","\n","f1, f1_T_label, f1_S_label, f1_P_label, f1_O_label, f1_D_label, f1_C_label=calculate_f1(predict, true_label)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":316,"status":"ok","timestamp":1653909510611,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"Lr-w6xb-EX6j","outputId":"73aab275-b482-4e1e-fb80-59c8d590c4da"},"outputs":[{"output_type":"stream","name":"stdout","text":["╒═══════════════════════════════╤════════════╕\n","│ baseline_CRF_bilstm_model_1   │   F1 Score │\n","╞═══════════════════════════════╪════════════╡\n","│ F1                            │   0.990466 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (T)                        │   0.947583 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (S)                        │   0.986454 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (P)                        │   0.995935 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (O)                        │   0.996313 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (D)                        │   0.871859 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (C)                        │   0.964046 │\n","╘═══════════════════════════════╧════════════╛\n"]}],"source":["Eval_Model_Name = []\n","Eval_result =[]\n","\n","#Manually put the result in the table as the below might not be run again.\n","#create table\n","table= [[\"F1\",f1],\n","        [\"F1 (T)\",f1_T_label],\n","        [\"F1 (S)\",f1_S_label],\n","        [\"F1 (P)\",f1_P_label],\n","        [\"F1 (O)\",f1_O_label],\n","        [\"F1 (D)\",f1_D_label],\n","        [\"F1 (C)\",f1_C_label]\n","        ]\n","col_names=[\"baseline_CRF_bilstm_model_1\",\"F1 Score\"]\n","\n","Eval_Model_Name.append(\"baseline_CRF_bilstm_model_1\")\n","Eval_result.append([el[1] for el in table])\n","\n","print(tabulate(table, headers=col_names,tablefmt='fancy_grid'))"]},{"cell_type":"markdown","metadata":{"id":"_ZaB-4POammh"},"source":["# Model - Bi-LSTM CRF - Word2Vector"]},{"cell_type":"markdown","metadata":{"id":"tdDxAyVdoB7N"},"source":["##Build Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ny1aqZlBGT0S"},"outputs":[],"source":["#add embedding matrix as the initial weights of nn.Embedding\n","\n","class BiLSTM_CRF(nn.Module):\n","\n","    def __init__(self, vocab_size, tag_to_ix,embedding_dim, hidden_dim,emb_table,num_layers):\n","        super(BiLSTM_CRF, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.vocab_size = vocab_size\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","        self.num_layers = num_layers\n","\n","        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n","\n","        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n","        #self.word_embeds.weight.data.copy_(torch.from_numpy(w2v_sg_emb_dim)) \n","        self.word_embeds.weight.data.copy_(torch.from_numpy(np.asarray(emb_table))) # need to replace w2v_sg_emb_dim to other embedding table\n","        \n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n","                            num_layers=num_layers, bidirectional=True)\n","\n","\n","\n","        # Maps the output of the LSTM into tag space.\n","        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n","\n","        # Matrix of transition parameters.  Entry i,j is the score of\n","        # transitioning *to* i *from* j.\n","        self.transitions = nn.Parameter(\n","            torch.randn(self.tagset_size, self.tagset_size))\n","\n","        # These two statements enforce the constraint that we never transfer\n","        # to the start tag and we never transfer from the stop tag\n","        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n","        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n","\n","        self.hidden = self.init_hidden()\n","\n","    def init_hidden(self):\n","        return (torch.randn(2*num_layers, 1, self.hidden_dim // 2).to(device),\n","                torch.randn(2*num_layers, 1, self.hidden_dim // 2).to(device))\n","\n","    def _forward_alg(self, feats):\n","        # Do the forward algorithm to compute the partition function\n","        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n","        # START_TAG has all of the score.\n","        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n","\n","        # Wrap in a variable so that we will get automatic backprop\n","        forward_var = init_alphas\n","\n","        # Iterate through the sentence\n","        for feat in feats:\n","            alphas_t = []  # The forward tensors at this timestep\n","            for next_tag in range(self.tagset_size):\n","                # broadcast the emission score: it is the same regardless of\n","                # the previous tag\n","                emit_score = feat[next_tag].view(\n","                    1, -1).expand(1, self.tagset_size)\n","                # the ith entry of trans_score is the score of transitioning to\n","                # next_tag from i\n","                trans_score = self.transitions[next_tag].view(1, -1)\n","                # The ith entry of next_tag_var is the value for the\n","                # edge (i -> next_tag) before we do log-sum-exp\n","                next_tag_var = forward_var + trans_score + emit_score\n","                # The forward variable for this tag is log-sum-exp of all the\n","                # scores.\n","                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n","            forward_var = torch.cat(alphas_t).view(1, -1)\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        alpha = log_sum_exp(terminal_var)\n","        return alpha\n","\n","    def _get_lstm_features(self, sentence):\n","        self.hidden = self.init_hidden()\n","        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n","        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n","        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n","        lstm_feats = self.hidden2tag(lstm_out)\n","        return lstm_feats\n","\n","    def _score_sentence(self, feats, tags):\n","        # Gives the score of a provided tag sequence\n","        score = torch.zeros(1).to(device)\n","        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n","        for i, feat in enumerate(feats):\n","            score = score + \\\n","                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n","        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n","        return score\n","\n","    def _viterbi_decode(self, feats):\n","        backpointers = []\n","\n","        # Initialize the viterbi variables in log space\n","        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n","        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n","\n","        # forward_var at step i holds the viterbi variables for step i-1\n","        forward_var = init_vvars\n","        for feat in feats:\n","            bptrs_t = []  # holds the backpointers for this step\n","            viterbivars_t = []  # holds the viterbi variables for this step\n","\n","            for next_tag in range(self.tagset_size):\n","                # next_tag_var[i] holds the viterbi variable for tag i at the\n","                # previous step, plus the score of transitioning\n","                # from tag i to next_tag.\n","                # We don't include the emission scores here because the max\n","                # does not depend on them (we add them in below)\n","                next_tag_var = forward_var + self.transitions[next_tag]\n","                best_tag_id = argmax(next_tag_var)\n","                bptrs_t.append(best_tag_id)\n","                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n","            # Now add in the emission scores, and assign forward_var to the set\n","            # of viterbi variables we just computed\n","            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n","            backpointers.append(bptrs_t)\n","\n","        # Transition to STOP_TAG\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        best_tag_id = argmax(terminal_var)\n","        path_score = terminal_var[0][best_tag_id]\n","\n","        # Follow the back pointers to decode the best path.\n","        best_path = [best_tag_id]\n","        for bptrs_t in reversed(backpointers):\n","            best_tag_id = bptrs_t[best_tag_id]\n","            best_path.append(best_tag_id)\n","        # Pop off the start tag (we dont want to return that to the caller)\n","        start = best_path.pop()\n","        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n","        best_path.reverse()\n","        return path_score, best_path\n","\n","    def neg_log_likelihood(self, sentence, tags):\n","        feats = self._get_lstm_features(sentence)\n","        forward_score = self._forward_alg(feats)\n","        gold_score = self._score_sentence(feats, tags)\n","        return forward_score - gold_score\n","\n","    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n","        # Get the emission scores from the BiLSTM\n","        lstm_feats = self._get_lstm_features(sentence)\n","\n","        # Find the best path, given the features.\n","        score, tag_seq = self._viterbi_decode(lstm_feats)\n","        return score, tag_seq"]},{"cell_type":"markdown","metadata":{"id":"cgBD2oEhPE3I"},"source":["##Training"]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","HIDDEN_DIM = 50\n","EMBEDDING_TAB = w2v_sg_emb_table\n","EMBEDDING_DIM=EMBEDDING_TAB.shape[1]\n","num_layers=1\n","\n","\n","model = BiLSTM_CRF(len(word_dict), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,EMBEDDING_TAB,num_layers).to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"],"metadata":{"id":"bScBz0Wef0Yp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1840822,"status":"ok","timestamp":1653887231230,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"XlJrSnkJapmZ","outputId":"eb24fd15-fd8b-4b92-d688-57fc79371ff4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch:1, Training loss: 22524.07, train acc: 0.9948, val loss: 1381.17, val acc: 0.9926, time: 938.46s\n","Epoch:2, Training loss: 1802.19, train acc: 0.9990, val loss: 1011.95, val acc: 0.9955, time: 902.26s\n"]}],"source":["import datetime\n","\n","for epoch in range(2):  \n","    time1 = datetime.datetime.now()\n","    train_loss = 0\n","\n","    model.train()\n","    for i, idxs in enumerate(train_input_index):\n","        tags_index = train_output_index[i]\n","\n","        # Step 1. Remember that Pytorch accumulates gradients.\n","        # We need to clear them out before each instance\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is,\n","        # turn them into Tensors of word indices.\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","\n","        # Step 3. Run our forward pass.\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        # calling optimizer.step()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss+=loss.item()\n","\n","    model.eval()\n","    # Call the cal_acc functions you implemented as required\n","    _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n","    _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n","\n","    val_loss = 0\n","    for i, idxs in enumerate(val_input_index):\n","        tags_index = val_output_index[i]\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","        val_loss+=loss.item()\n","    time2 = datetime.datetime.now()\n","\n","    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WMdp6Sn-PHGw"},"source":["##Save and Load model"]},{"cell_type":"code","source":["torch.save(model,'/content/drive/MyDrive/COMP5046-NLP-AS2/Models/w2v_CRF_bilstm_model.pt')"],"metadata":{"id":"-GGsu9nPfkvg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gDAhdQBTlSs_"},"outputs":[],"source":["w2v_CRF_bilstm_model=torch.load('/content/drive/MyDrive/COMP5046-NLP-AS2/Models/w2v_CRF_bilstm_model.pt')"]},{"cell_type":"markdown","metadata":{"id":"LA3cSN_CPJax"},"source":["##Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26307,"status":"ok","timestamp":1653909563430,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"eBOJgPqpFJRV","outputId":"bf185057-2819-400b-a9a7-1b7ee101a390"},"outputs":[{"output_type":"stream","name":"stdout","text":["╒════════════════════════╤════════════╕\n","│ w2v_CRF_bilstm_model   │   F1 Score │\n","╞════════════════════════╪════════════╡\n","│ F1                     │   0.995473 │\n","├────────────────────────┼────────────┤\n","│ F1 (T)                 │   0.965963 │\n","├────────────────────────┼────────────┤\n","│ F1 (S)                 │   0.992775 │\n","├────────────────────────┼────────────┤\n","│ F1 (P)                 │   0.997713 │\n","├────────────────────────┼────────────┤\n","│ F1 (O)                 │   1        │\n","├────────────────────────┼────────────┤\n","│ F1 (D)                 │   0.937186 │\n","├────────────────────────┼────────────┤\n","│ F1 (C)                 │   0.973796 │\n","╘════════════════════════╧════════════╛\n"]}],"source":["w2v_CRF_bilstm_model.eval()\n","predict=[]\n","true_label =[]\n","for sentence, tags in zip(tokensized_validation_docs,tokensized_validation_label_docs):\n","\n","  sentence_in = prepare_sequence(sentence, word_dict).to(device)\n","  targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long).to(device)\n","  _, y_pred=w2v_CRF_bilstm_model(sentence_in)\n","  predict.extend(y_pred)\n","  true_label.extend(targets.cpu().numpy().tolist())\n","\n","f1, f1_T_label, f1_S_label, f1_P_label, f1_O_label, f1_D_label, f1_C_label=calculate_f1(predict, true_label)\n","\n","#Manually put the result in the table as the below might not be run again.\n","\n","#create table\n","table= [[\"F1\",f1],\n","        [\"F1 (T)\",f1_T_label],\n","        [\"F1 (S)\",f1_S_label],\n","        [\"F1 (P)\",f1_P_label],\n","        [\"F1 (O)\",f1_O_label],\n","        [\"F1 (D)\",f1_D_label],\n","        [\"F1 (C)\",f1_C_label]]\n","col_names=[\"w2v_CRF_bilstm_model\",\"F1 Score\"]\n","\n","Eval_Model_Name.append(\"w2v_CRF_bilstm_model\")\n","Eval_result.append([el[1] for el in table])\n","\n","print(tabulate(table, headers=col_names,tablefmt='fancy_grid'))"]},{"cell_type":"markdown","source":["#Model-BI-LSTM-FastText"],"metadata":{"id":"ja6uP6aESMmS"}},{"cell_type":"markdown","source":["##Build Model"],"metadata":{"id":"oDWeoSe0SRql"}},{"cell_type":"code","source":["#add embedding matrix as the initial weights of nn.Embedding\n","\n","class BiLSTM_CRF_FT(nn.Module):\n","\n","    def __init__(self, vocab_size, tag_to_ix,embedding_dim, hidden_dim,emb_table,num_layers):\n","        super(BiLSTM_CRF_FT, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.vocab_size = vocab_size\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","        self.num_layers = num_layers\n","\n","        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n","\n","        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n","        #self.word_embeds.weight.data.copy_(torch.from_numpy(w2v_sg_emb_dim)) \n","        self.word_embeds.weight.data.copy_(torch.from_numpy(np.asarray(emb_table))) # need to replace w2v_sg_emb_dim to other embedding table\n","        \n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n","                            num_layers=num_layers, bidirectional=True)\n","\n","\n","\n","        # Maps the output of the LSTM into tag space.\n","        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n","\n","        # Matrix of transition parameters.  Entry i,j is the score of\n","        # transitioning *to* i *from* j.\n","        self.transitions = nn.Parameter(\n","            torch.randn(self.tagset_size, self.tagset_size))\n","\n","        # These two statements enforce the constraint that we never transfer\n","        # to the start tag and we never transfer from the stop tag\n","        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n","        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n","\n","        self.hidden = self.init_hidden()\n","\n","    def init_hidden(self):\n","        return (torch.randn(2*num_layers, 1, self.hidden_dim // 2).to(device),\n","                torch.randn(2*num_layers, 1, self.hidden_dim // 2).to(device))\n","\n","    def _forward_alg(self, feats):\n","        # Do the forward algorithm to compute the partition function\n","        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n","        # START_TAG has all of the score.\n","        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n","\n","        # Wrap in a variable so that we will get automatic backprop\n","        forward_var = init_alphas\n","\n","        # Iterate through the sentence\n","        for feat in feats:\n","            alphas_t = []  # The forward tensors at this timestep\n","            for next_tag in range(self.tagset_size):\n","                # broadcast the emission score: it is the same regardless of\n","                # the previous tag\n","                emit_score = feat[next_tag].view(\n","                    1, -1).expand(1, self.tagset_size)\n","                # the ith entry of trans_score is the score of transitioning to\n","                # next_tag from i\n","                trans_score = self.transitions[next_tag].view(1, -1)\n","                # The ith entry of next_tag_var is the value for the\n","                # edge (i -> next_tag) before we do log-sum-exp\n","                next_tag_var = forward_var + trans_score + emit_score\n","                # The forward variable for this tag is log-sum-exp of all the\n","                # scores.\n","                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n","            forward_var = torch.cat(alphas_t).view(1, -1)\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        alpha = log_sum_exp(terminal_var)\n","        return alpha\n","\n","    def _get_lstm_features(self, sentence):\n","        self.hidden = self.init_hidden()\n","        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n","        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n","        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n","        lstm_feats = self.hidden2tag(lstm_out)\n","        return lstm_feats\n","\n","    def _score_sentence(self, feats, tags):\n","        # Gives the score of a provided tag sequence\n","        score = torch.zeros(1).to(device)\n","        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n","        for i, feat in enumerate(feats):\n","            score = score + \\\n","                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n","        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n","        return score\n","\n","    def _viterbi_decode(self, feats):\n","        backpointers = []\n","\n","        # Initialize the viterbi variables in log space\n","        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n","        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n","\n","        # forward_var at step i holds the viterbi variables for step i-1\n","        forward_var = init_vvars\n","        for feat in feats:\n","            bptrs_t = []  # holds the backpointers for this step\n","            viterbivars_t = []  # holds the viterbi variables for this step\n","\n","            for next_tag in range(self.tagset_size):\n","                # next_tag_var[i] holds the viterbi variable for tag i at the\n","                # previous step, plus the score of transitioning\n","                # from tag i to next_tag.\n","                # We don't include the emission scores here because the max\n","                # does not depend on them (we add them in below)\n","                next_tag_var = forward_var + self.transitions[next_tag]\n","                best_tag_id = argmax(next_tag_var)\n","                bptrs_t.append(best_tag_id)\n","                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n","            # Now add in the emission scores, and assign forward_var to the set\n","            # of viterbi variables we just computed\n","            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n","            backpointers.append(bptrs_t)\n","\n","        # Transition to STOP_TAG\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        best_tag_id = argmax(terminal_var)\n","        path_score = terminal_var[0][best_tag_id]\n","\n","        # Follow the back pointers to decode the best path.\n","        best_path = [best_tag_id]\n","        for bptrs_t in reversed(backpointers):\n","            best_tag_id = bptrs_t[best_tag_id]\n","            best_path.append(best_tag_id)\n","        # Pop off the start tag (we dont want to return that to the caller)\n","        start = best_path.pop()\n","        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n","        best_path.reverse()\n","        return path_score, best_path\n","\n","    def neg_log_likelihood(self, sentence, tags):\n","        feats = self._get_lstm_features(sentence)\n","        forward_score = self._forward_alg(feats)\n","        gold_score = self._score_sentence(feats, tags)\n","        return forward_score - gold_score\n","\n","    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n","        # Get the emission scores from the BiLSTM\n","        lstm_feats = self._get_lstm_features(sentence)\n","\n","        # Find the best path, given the features.\n","        score, tag_seq = self._viterbi_decode(lstm_feats)\n","        return score, tag_seq"],"metadata":{"id":"htdhqz-LSgNf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Training"],"metadata":{"id":"QAT2t_ETSWAx"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","HIDDEN_DIM = 50\n","EMBEDDING_TAB = w2v_ft_emb_table\n","EMBEDDING_DIM=EMBEDDING_TAB.shape[1]\n","num_layers=1\n","\n","\n","model = BiLSTM_CRF_FT(len(word_dict), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,EMBEDDING_TAB,num_layers).to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"],"metadata":{"id":"XyFNgiebSl9h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datetime\n","\n","for epoch in range(2):  \n","    time1 = datetime.datetime.now()\n","    train_loss = 0\n","\n","    model.train()\n","    for i, idxs in enumerate(train_input_index):\n","        tags_index = train_output_index[i]\n","\n","        # Step 1. Remember that Pytorch accumulates gradients.\n","        # We need to clear them out before each instance\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is,\n","        # turn them into Tensors of word indices.\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","\n","        # Step 3. Run our forward pass.\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        # calling optimizer.step()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss+=loss.item()\n","\n","    model.eval()\n","    # Call the cal_acc functions you implemented as required\n","    _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n","    _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n","\n","    val_loss = 0\n","    for i, idxs in enumerate(val_input_index):\n","        tags_index = val_output_index[i]\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","        val_loss+=loss.item()\n","    time2 = datetime.datetime.now()\n","\n","    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B3PtmvLWT4TW","executionInfo":{"status":"ok","timestamp":1654072041830,"user_tz":-600,"elapsed":1083584,"user":{"displayName":"roger wang","userId":"04313800435014207614"}},"outputId":"bfa1cdef-286a-46e5-cd55-608c3fbe2705"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:1, Training loss: 21021.59, train acc: 0.9956, val loss: 1322.26, val acc: 0.9923, time: 541.56s\n","Epoch:2, Training loss: 1551.57, train acc: 0.9994, val loss: 992.94, val acc: 0.9953, time: 541.75s\n"]}]},{"cell_type":"markdown","source":["##Save and load model"],"metadata":{"id":"eoFGdJZXSY2y"}},{"cell_type":"code","source":["torch.save(model,'/content/drive/MyDrive/COMP5046-NLP-AS2/Models/ft_CRF_bilstm_model.pt')\n","ft_CRF_bilstm_model=torch.load('/content/drive/MyDrive/COMP5046-NLP-AS2/Models/ft_CRF_bilstm_model.pt')"],"metadata":{"id":"AxCYkpMfSv0J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Evaluation"],"metadata":{"id":"OAc-7aaGSctS"}},{"cell_type":"code","source":["ft_CRF_bilstm_model.eval()\n","predict=[]\n","true_label =[]\n","for sentence, tags in zip(tokensized_validation_docs,tokensized_validation_label_docs):\n","\n","  sentence_in = prepare_sequence(sentence, word_dict).to(device)\n","  targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long).to(device)\n","  _, y_pred=ft_CRF_bilstm_model(sentence_in)\n","  predict.extend(y_pred)\n","  true_label.extend(targets.cpu().numpy().tolist())\n","\n","f1, f1_T_label, f1_S_label, f1_P_label, f1_O_label, f1_D_label, f1_C_label=calculate_f1(predict, true_label)\n","\n","#Manually put the result in the table as the below might not be run again.\n","\n","#create table\n","table= [[\"F1\",f1],\n","        [\"F1 (T)\",f1_T_label],\n","        [\"F1 (S)\",f1_S_label],\n","        [\"F1 (P)\",f1_P_label],\n","        [\"F1 (O)\",f1_O_label],\n","        [\"F1 (D)\",f1_D_label],\n","        [\"F1 (C)\",f1_C_label]]\n","col_names=[\"ft_CRF_bilstm_model\",\"F1 Score\"]\n","\n","Eval_Model_Name.append(\"ft_CRF_bilstm_model\")\n","Eval_result.append([el[1] for el in table])\n","\n","print(tabulate(table, headers=col_names,tablefmt='fancy_grid'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":242},"id":"1tvUVClDS0sD","executionInfo":{"status":"error","timestamp":1654072068029,"user_tz":-600,"elapsed":24756,"user":{"displayName":"roger wang","userId":"04313800435014207614"}},"outputId":"3b3224d5-c223-4aa2-8023-298a604c6157"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-9ccff82228ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mcol_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ft_CRF_bilstm_model\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"F1 Score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mEval_Model_Name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ft_CRF_bilstm_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mEval_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Eval_Model_Name' is not defined"]}]},{"cell_type":"code","source":["ft_CRF_bilstm_model.eval()\n","predict=[]\n","true_label =[]\n","for sentence, tags in zip(tokensized_validation_docs,tokensized_validation_label_docs):\n","\n","\n","  sentence_in = prepare_sequence(sentence, word_dict).to(device)\n","  targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long).to(device)\n","  _, y_pred=ft_CRF_bilstm_model(sentence_in)\n","  predict.extend(y_pred)\n","  true_label.extend(targets.cpu().numpy().tolist())\n","\n","f1, f1_T_label, f1_S_label, f1_P_label, f1_O_label, f1_D_label, f1_C_label=calculate_f1(predict, true_label)\n","\n","table_2= [[\"F1\",f1],\n","        [\"F1 (T)\",f1_T_label],\n","        [\"F1 (S)\",f1_S_label],\n","        [\"F1 (P)\",f1_P_label],\n","        [\"F1 (O)\",f1_O_label],\n","        [\"F1 (D)\",f1_D_label],\n","        [\"F1 (C)\",f1_C_label]\n","    \n","        ]\n","col_names=[\"F1 Labels\",\"F1 Score\"]\n","\n","print(tabulate(table_2, headers=col_names,tablefmt='fancy_grid'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jNsqK0lQbGnq","executionInfo":{"status":"ok","timestamp":1654072954414,"user_tz":-600,"elapsed":30351,"user":{"displayName":"roger wang","userId":"04313800435014207614"}},"outputId":"78e12bbc-e790-4a8a-c2fc-06079497db9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["╒═════════════╤════════════╕\n","│ F1 Labels   │   F1 Score │\n","╞═════════════╪════════════╡\n","│ F1          │   0.995323 │\n","├─────────────┼────────────┤\n","│ F1 (T)      │   0.968686 │\n","├─────────────┼────────────┤\n","│ F1 (S)      │   0.992173 │\n","├─────────────┼────────────┤\n","│ F1 (P)      │   0.99873  │\n","├─────────────┼────────────┤\n","│ F1 (O)      │   0.998947 │\n","├─────────────┼────────────┤\n","│ F1 (D)      │   0.947236 │\n","├─────────────┼────────────┤\n","│ F1 (C)      │   0.976843 │\n","╘═════════════╧════════════╛\n"]}]},{"cell_type":"markdown","metadata":{"id":"2xMbsAX-a9ok"},"source":["#Model - Bi-LSTM CRF - POS tag only"]},{"cell_type":"markdown","metadata":{"id":"z8OMkfW_eVhE"},"source":["####Training\n"]},{"cell_type":"code","source":["#POS_embedding_matrix\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","HIDDEN_DIM = 50\n","EMBEDDING_TAB=POS_embedding_matrix\n","EMBEDDING_DIM=EMBEDDING_TAB.shape[1]\n","num_layers=1\n","\n","#need to replace the embedding table e.g. POS_embedding_matrix\n","model = BiLSTM_CRF(len(word_dict), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,EMBEDDING_TAB,num_layers).to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"],"metadata":{"id":"kF99IunEgDg5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"ub9z4wuueYM4","outputId":"94538924-613c-4cf6-957f-f69382f738ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch:1, Training loss: 33818.76, train acc: 0.9884, val loss: 2223.29, val acc: 0.9864, time: 884.12s\n","Epoch:2, Training loss: 3329.37, train acc: 0.9978, val loss: 1280.10, val acc: 0.9944, time: 880.68s\n"]}],"source":["import datetime\n","\n","for epoch in range(2):  \n","    time1 = datetime.datetime.now()\n","    train_loss = 0\n","\n","    model.train()\n","    for i, idxs in enumerate(train_input_index):\n","        tags_index = train_output_index[i]\n","\n","        # Step 1. Remember that Pytorch accumulates gradients.\n","        # We need to clear them out before each instance\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is,\n","        # turn them into Tensors of word indices.\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","\n","        # Step 3. Run our forward pass.\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        # calling optimizer.step()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss+=loss.item()\n","\n","    model.eval()\n","    # Call the cal_acc functions you implemented as required\n","    _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n","    _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n","\n","    val_loss = 0\n","    for i, idxs in enumerate(val_input_index):\n","        tags_index = val_output_index[i]\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","        val_loss+=loss.item()\n","    time2 = datetime.datetime.now()\n","\n","    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n"]},{"cell_type":"markdown","metadata":{"id":"ZNMUFylgezVn"},"source":["####Save and Load model"]},{"cell_type":"code","source":["torch.save(model,'/content/drive/MyDrive/COMP5046-NLP-AS2/Models/ft_CRF_bilstm_model.pt')"],"metadata":{"id":"4HUcDX6wgAou"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X7Cln0cxe1cW"},"outputs":[],"source":["ft_CRF_bilstm_model=torch.load('/content/drive/MyDrive/COMP5046-NLP-AS2/Models/ft_CRF_bilstm_model.pt')"]},{"cell_type":"markdown","metadata":{"id":"ImEadytHeq8C"},"source":["####Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":242},"id":"4tj_NXSmg5Yd","executionInfo":{"status":"error","timestamp":1654072767756,"user_tz":-600,"elapsed":33441,"user":{"displayName":"roger wang","userId":"04313800435014207614"}},"outputId":"a389542e-9d8f-4d1d-962f-2e11d43080bd"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-95ba811d1093>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mcol_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ft_CRF_bilstm_model\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"F1 Score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mEval_Model_Name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ft_CRF_bilstm_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mEval_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Eval_Model_Name' is not defined"]}],"source":["ft_CRF_bilstm_model.eval()\n","predict=[]\n","true_label =[]\n","\n","for sentence, tags in zip(tokensized_validation_docs,tokensized_validation_label_docs):\n","\n","  sentence_in = prepare_sequence(sentence, word_dict).to(device)\n","  targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long).to(device)\n","  _, y_pred=ft_CRF_bilstm_model(sentence_in)\n","  predict.extend(y_pred)\n","  true_label.extend(targets.cpu().numpy().tolist())\n","\n","f1, f1_T_label, f1_S_label, f1_P_label, f1_O_label, f1_D_label, f1_C_label=calculate_f1(predict, true_label)\n","\n","#Manually put the result in the table as the below might not be run again.\n","\n","#create table\n","table= [[\"F1\",f1],\n","        [\"F1 (T)\",f1_T_label],\n","        [\"F1 (S)\",f1_S_label],\n","        [\"F1 (P)\",f1_P_label],\n","        [\"F1 (O)\",f1_O_label],\n","        [\"F1 (D)\",f1_D_label],\n","        [\"F1 (C)\",f1_C_label]]\n","col_names=[\"ft_CRF_bilstm_model\",\"F1 Score\"]\n","\n","Eval_Model_Name.append(\"ft_CRF_bilstm_model\")\n","Eval_result.append([el[1] for el in table])\n","\n","print(tabulate(table, headers=col_names,tablefmt='fancy_grid'))"]},{"cell_type":"markdown","metadata":{"id":"oAtHzntcbdZm"},"source":["#Model - Bi-LSTM CRF - Word2Vector + POS"]},{"cell_type":"markdown","metadata":{"id":"N-9Qv3H1bkcd"},"source":["##Training"]},{"cell_type":"code","source":["#w2v_POS_emb_table\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","HIDDEN_DIM = 50\n","EMBEDDING_TAB=w2v_POS_emb_table\n","EMBEDDING_DIM=EMBEDDING_TAB.shape[1]\n","num_layers=1\n","\n","model = BiLSTM_CRF(len(word_dict), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,EMBEDDING_TAB,num_layers).to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"],"metadata":{"id":"vB4MPHlggKSr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1099249,"status":"ok","timestamp":1653816341067,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"C67Lvpdb0I0s","outputId":"efadb438-1e24-45c7-b897-3a5ba940b929"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch:1, Training loss: 19862.46, train acc: 0.9951, val loss: 1247.18, val acc: 0.9930, time: 549.24s\n","Epoch:2, Training loss: 1784.31, train acc: 0.9990, val loss: 950.06, val acc: 0.9954, time: 549.91s\n"]}],"source":["import datetime\n","\n","for epoch in range(2):  \n","    time1 = datetime.datetime.now()\n","    train_loss = 0\n","\n","    model.train()\n","    for i, idxs in enumerate(train_input_index):\n","        tags_index = train_output_index[i]\n","\n","        # Step 1. Remember that Pytorch accumulates gradients.\n","        # We need to clear them out before each instance\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is,\n","        # turn them into Tensors of word indices.\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","\n","        # Step 3. Run our forward pass.\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        # calling optimizer.step()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss+=loss.item()\n","\n","    model.eval()\n","    # Call the cal_acc functions you implemented as required\n","    _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n","    _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n","\n","    val_loss = 0\n","    for i, idxs in enumerate(val_input_index):\n","        tags_index = val_output_index[i]\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","        val_loss+=loss.item()\n","    time2 = datetime.datetime.now()\n","\n","    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n"]},{"cell_type":"markdown","metadata":{"id":"a-RjegwPiazV"},"source":["## Save and Load Model"]},{"cell_type":"code","source":["torch.save(model,'/content/drive/MyDrive/COMP5046-NLP-AS2/Models/POS_w2v_CRF_bilstm_model.pt')"],"metadata":{"id":"dfuMqxMGgP-N"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQhZrE8U6fiZ"},"outputs":[],"source":["POS_w2v_CRF_bilstm_model=torch.load('/content/drive/MyDrive/COMP5046-NLP-AS2/Models/POS_w2v_CRF_bilstm_model.pt')"]},{"cell_type":"markdown","metadata":{"id":"sogYjLtk6xZb"},"source":["##Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kCy2tOWUgVMO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653909614645,"user_tz":-600,"elapsed":24868,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"}},"outputId":"9acd7cb5-961a-43be-cda4-05ba86082f73"},"outputs":[{"output_type":"stream","name":"stdout","text":["╒════════════════════════════╤════════════╕\n","│ POS_w2v_CRF_bilstm_model   │   F1 Score │\n","╞════════════════════════════╪════════════╡\n","│ F1                         │   0.994483 │\n","├────────────────────────────┼────────────┤\n","│ F1 (T)                     │   0.957114 │\n","├────────────────────────────┼────────────┤\n","│ F1 (S)                     │   0.987959 │\n","├────────────────────────────┼────────────┤\n","│ F1 (P)                     │   0.99873  │\n","├────────────────────────────┼────────────┤\n","│ F1 (O)                     │   0.999684 │\n","├────────────────────────────┼────────────┤\n","│ F1 (D)                     │   0.937186 │\n","├────────────────────────────┼────────────┤\n","│ F1 (C)                     │   0.972578 │\n","╘════════════════════════════╧════════════╛\n"]}],"source":["POS_w2v_CRF_bilstm_model.eval()\n","predict=[]\n","true_label =[]\n","for sentence, tags in zip(tokensized_validation_docs,tokensized_validation_label_docs):\n","\n","  sentence_in = prepare_sequence(sentence, word_dict).to(device)\n","  targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long).to(device)\n","  _, y_pred=POS_w2v_CRF_bilstm_model(sentence_in)\n","  predict.extend(y_pred)\n","  true_label.extend(targets.cpu().numpy().tolist())\n","\n","f1, f1_T_label, f1_S_label, f1_P_label, f1_O_label, f1_D_label, f1_C_label=calculate_f1(predict, true_label)\n","\n","#Manually put the result in the table as the below might not be run again.\n","\n","#create table\n","table= [[\"F1\",f1],\n","        [\"F1 (T)\",f1_T_label],\n","        [\"F1 (S)\",f1_S_label],\n","        [\"F1 (P)\",f1_P_label],\n","        [\"F1 (O)\",f1_O_label],\n","        [\"F1 (D)\",f1_D_label],\n","        [\"F1 (C)\",f1_C_label]]\n","\n","col_names=[\"POS_w2v_CRF_bilstm_model\",\"F1 Score\"]\n","\n","Eval_Model_Name.append(\"POS_w2v_CRF_bilstm_model\")\n","Eval_result.append([el[1] for el in table])\n","\n","print(tabulate(table, headers=col_names,tablefmt='fancy_grid'))"]},{"cell_type":"markdown","metadata":{"id":"qrE2ipCSj1FI"},"source":["# Model - Bi-LSTM CRF - POS + Domain "]},{"cell_type":"markdown","metadata":{"id":"DXwk-2uQj41t"},"source":["## Training"]},{"cell_type":"code","source":["#POS_domain_emb_table\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","HIDDEN_DIM = 50\n","EMBEDDING_TAB=POS_domain_emb_table\n","EMBEDDING_DIM=EMBEDDING_TAB.shape[1]\n","num_layers=1\n","\n","model = BiLSTM_CRF(len(word_dict), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,EMBEDDING_TAB,num_layers).to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"],"metadata":{"id":"Z7EPdtBCghWg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1763558,"status":"ok","timestamp":1653875869991,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"7uIGXA-rj72Y","outputId":"087edc2f-caf7-4ef4-fb67-c26a5025128e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch:1, Training loss: 20550.45, train acc: 0.9936, val loss: 1364.74, val acc: 0.9921, time: 873.85s\n","Epoch:2, Training loss: 1951.37, train acc: 0.9983, val loss: 955.89, val acc: 0.9953, time: 878.60s\n"]}],"source":["import datetime\n","\n","for epoch in range(2):  \n","    time1 = datetime.datetime.now()\n","    train_loss = 0\n","\n","    model.train()\n","    for i, idxs in enumerate(train_input_index):\n","        tags_index = train_output_index[i]\n","\n","        # Step 1. Remember that Pytorch accumulates gradients.\n","        # We need to clear them out before each instance\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is,\n","        # turn them into Tensors of word indices.\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","\n","        # Step 3. Run our forward pass.\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        # calling optimizer.step()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss+=loss.item()\n","\n","    model.eval()\n","    # Call the cal_acc functions you implemented as required\n","    _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n","    _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n","\n","    val_loss = 0\n","    for i, idxs in enumerate(val_input_index):\n","        tags_index = val_output_index[i]\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","        val_loss+=loss.item()\n","    time2 = datetime.datetime.now()\n","\n","    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))"]},{"cell_type":"markdown","metadata":{"id":"JqbwHEQOj8i-"},"source":["## Save and Load Model"]},{"cell_type":"code","source":["torch.save(model,'/content/drive/MyDrive/COMP5046-NLP-AS2/Models/POS_domain_CRF_bilstm_model.pt')"],"metadata":{"id":"MRzTWivugk-L"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wNp1gfThj_xU"},"outputs":[],"source":["POS_domain_CRF_bilstm_model=torch.load('/content/drive/MyDrive/COMP5046-NLP-AS2/Models/POS_domain_CRF_bilstm_model.pt')"]},{"cell_type":"markdown","metadata":{"id":"j8AVAawdkAZA"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x86pApAkkCVz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653909640023,"user_tz":-600,"elapsed":24887,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"}},"outputId":"a2d888a5-45cd-4fd3-f471-66b80f5c1180"},"outputs":[{"output_type":"stream","name":"stdout","text":["╒═══════════════════════════════╤════════════╕\n","│ POS_domain_CRF_bilstm_model   │   F1 Score │\n","╞═══════════════════════════════╪════════════╡\n","│ F1                            │   0.995233 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (T)                        │   0.967325 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (S)                        │   0.991872 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (P)                        │   0.998222 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (O)                        │   0.999842 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (D)                        │   0.917085 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (C)                        │   0.975015 │\n","╘═══════════════════════════════╧════════════╛\n"]}],"source":["POS_domain_CRF_bilstm_model.eval()\n","predict=[]\n","true_label =[]\n","for sentence, tags in zip(tokensized_validation_docs,tokensized_validation_label_docs):\n","\n","  sentence_in = prepare_sequence(sentence, word_dict).to(device)\n","  targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long).to(device)\n","  _, y_pred=POS_domain_CRF_bilstm_model(sentence_in)\n","  predict.extend(y_pred)\n","  true_label.extend(targets.cpu().numpy().tolist())\n","\n","f1, f1_T_label, f1_S_label, f1_P_label, f1_O_label, f1_D_label, f1_C_label=calculate_f1(predict, true_label)\n","\n","#Manually put the result in the table as the below might not be run again.\n","\n","#create table\n","table= [[\"F1\",f1],\n","        [\"F1 (T)\",f1_T_label],\n","        [\"F1 (S)\",f1_S_label],\n","        [\"F1 (P)\",f1_P_label],\n","        [\"F1 (O)\",f1_O_label],\n","        [\"F1 (D)\",f1_D_label],\n","        [\"F1 (C)\",f1_C_label]]\n","\n","col_names=[\"POS_domain_CRF_bilstm_model\",\"F1 Score\"]\n","\n","Eval_Model_Name.append(\"POS_domain_CRF_bilstm_model\")\n","Eval_result.append([el[1] for el in table])\n","\n","print(tabulate(table, headers=col_names,tablefmt='fancy_grid'))"]},{"cell_type":"markdown","metadata":{"id":"SEnYOcAHhcoc"},"source":["# Model - Bi-LSTM CRF - Word2Vector + POS + Domain "]},{"cell_type":"markdown","metadata":{"id":"qMciA97wht1U"},"source":["## Training"]},{"cell_type":"code","source":["#w2v_POS_domain_emb_table\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","HIDDEN_DIM = 50\n","EMBEDDING_TAB=w2v_POS_domain_emb_table\n","EMBEDDING_DIM=EMBEDDING_TAB.shape[1]\n","num_layers=1\n","\n","model = BiLSTM_CRF(len(word_dict), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,EMBEDDING_TAB,num_layers).to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"],"metadata":{"id":"nNXIjxPBg-b2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1170854,"status":"ok","timestamp":1653891129783,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"41jI1mWGh0Pl","outputId":"f5056916-50c7-442c-c484-5cc6a63ab737"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:1, Training loss: 17877.41, train acc: 0.9953, val loss: 1241.62, val acc: 0.9936, time: 600.43s\n","Epoch:2, Training loss: 1818.68, train acc: 0.9990, val loss: 874.65, val acc: 0.9956, time: 570.08s\n"]}],"source":["import datetime\n","\n","for epoch in range(2):  \n","    time1 = datetime.datetime.now()\n","    train_loss = 0\n","\n","    model.train()\n","    for i, idxs in enumerate(train_input_index):\n","        tags_index = train_output_index[i]\n","\n","        # Step 1. Remember that Pytorch accumulates gradients.\n","        # We need to clear them out before each instance\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is,\n","        # turn them into Tensors of word indices.\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","\n","        # Step 3. Run our forward pass.\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        # calling optimizer.step()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss+=loss.item()\n","\n","    model.eval()\n","    # Call the cal_acc functions you implemented as required\n","    _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n","    _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n","\n","    val_loss = 0\n","    for i, idxs in enumerate(val_input_index):\n","        tags_index = val_output_index[i]\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","        val_loss+=loss.item()\n","    time2 = datetime.datetime.now()\n","\n","    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))"]},{"cell_type":"markdown","metadata":{"id":"MHr3pCV3hxnF"},"source":["## Save and Load Model"]},{"cell_type":"code","source":["torch.save(model,'/content/drive/MyDrive/COMP5046-NLP-AS2/Models/POS_w2v_domain_CRF_bilstm_model.pt')"],"metadata":{"id":"_ImHN1fzhEG8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpue1W92h023"},"outputs":[],"source":["POS_w2v_domain_CRF_bilstm_model=torch.load('/content/drive/MyDrive/COMP5046-NLP-AS2/Models/POS_w2v_domain_CRF_bilstm_model.pt')"]},{"cell_type":"markdown","metadata":{"id":"Ur8M0gWmh1iN"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Iqdzdpcie5H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653909665970,"user_tz":-600,"elapsed":25470,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"}},"outputId":"a83b57d8-15fd-405f-e544-b4b92c378981"},"outputs":[{"output_type":"stream","name":"stdout","text":["╒═══════════════════════════════════╤════════════╕\n","│ POS_w2v_domain_CRF_bilstm_model   │   F1 Score │\n","╞═══════════════════════════════════╪════════════╡\n","│ F1                                │   0.995593 │\n","├───────────────────────────────────┼────────────┤\n","│ F1 (T)                            │   0.965963 │\n","├───────────────────────────────────┼────────────┤\n","│ F1 (S)                            │   0.992173 │\n","├───────────────────────────────────┼────────────┤\n","│ F1 (P)                            │   0.99873  │\n","├───────────────────────────────────┼────────────┤\n","│ F1 (O)                            │   0.999895 │\n","├───────────────────────────────────┼────────────┤\n","│ F1 (D)                            │   0.947236 │\n","├───────────────────────────────────┼────────────┤\n","│ F1 (C)                            │   0.973796 │\n","╘═══════════════════════════════════╧════════════╛\n"]}],"source":["POS_w2v_domain_CRF_bilstm_model.eval()\n","predict=[]\n","true_label =[]\n","for sentence, tags in zip(tokensized_validation_docs,tokensized_validation_label_docs):\n","\n","  sentence_in = prepare_sequence(sentence, word_dict).to(device)\n","  targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long).to(device)\n","  _, y_pred=POS_w2v_domain_CRF_bilstm_model(sentence_in)\n","  predict.extend(y_pred)\n","  true_label.extend(targets.cpu().numpy().tolist())\n","\n","f1, f1_T_label, f1_S_label, f1_P_label, f1_O_label, f1_D_label, f1_C_label=calculate_f1(predict, true_label)\n","\n","#Manually put the result in the table as the below might not be run again.\n","\n","#create table\n","table= [[\"F1\",f1],\n","        [\"F1 (T)\",f1_T_label],\n","        [\"F1 (S)\",f1_S_label],\n","        [\"F1 (P)\",f1_P_label],\n","        [\"F1 (O)\",f1_O_label],\n","        [\"F1 (D)\",f1_D_label],\n","        [\"F1 (C)\",f1_C_label]]\n","col_names=[\"POS_w2v_domain_CRF_bilstm_model\",\"F1 Score\"]\n","\n","Eval_Model_Name.append(\"POS_w2v_domain_CRF_bilstm_model\")\n","Eval_result.append([el[1] for el in table])\n","\n","print(tabulate(table, headers=col_names,tablefmt='fancy_grid'))"]},{"cell_type":"markdown","source":["# Comparison Table _Word Embedding"],"metadata":{"id":"xIL_we0iuF8F"}},{"cell_type":"code","source":["result_df = pd.DataFrame(columns=['F1', 'F1 (T)', 'F1 (S)','F1 (P)','F1 (O)','F1 (D)','F1 (C) '])\n","for i in range(len(Eval_result)):\n","    result_df.loc[i] = Eval_result[i]\n","\n","result_df.index = Eval_Model_Name\n","result_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"iHDeLF-NmjNd","executionInfo":{"status":"ok","timestamp":1653909671328,"user_tz":-600,"elapsed":341,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"}},"outputId":"7b09298d-7e71-4f33-c58f-3fccc5bc8c3f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                       F1    F1 (T)    F1 (S)    F1 (P)  \\\n","baseline_CRF_bilstm_model_1      0.990466  0.947583  0.986454  0.995935   \n","w2v_CRF_bilstm_model             0.995473  0.965963  0.992775  0.997713   \n","POS_only_CRF_bilstm_model        0.994304  0.956433  0.987959  0.998222   \n","POS_w2v_CRF_bilstm_model         0.994483  0.957114  0.987959  0.998730   \n","POS_domain_CRF_bilstm_model      0.995233  0.967325  0.991872  0.998222   \n","POS_w2v_domain_CRF_bilstm_model  0.995593  0.965963  0.992173  0.998730   \n","\n","                                   F1 (O)    F1 (D)   F1 (C)   \n","baseline_CRF_bilstm_model_1      0.996313  0.871859  0.964046  \n","w2v_CRF_bilstm_model             1.000000  0.937186  0.973796  \n","POS_only_CRF_bilstm_model        0.999737  0.927136  0.972578  \n","POS_w2v_CRF_bilstm_model         0.999684  0.937186  0.972578  \n","POS_domain_CRF_bilstm_model      0.999842  0.917085  0.975015  \n","POS_w2v_domain_CRF_bilstm_model  0.999895  0.947236  0.973796  "],"text/html":["\n","  <div id=\"df-886dd653-300a-48c6-98cb-bd5eddff0077\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>F1</th>\n","      <th>F1 (T)</th>\n","      <th>F1 (S)</th>\n","      <th>F1 (P)</th>\n","      <th>F1 (O)</th>\n","      <th>F1 (D)</th>\n","      <th>F1 (C)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>baseline_CRF_bilstm_model_1</th>\n","      <td>0.990466</td>\n","      <td>0.947583</td>\n","      <td>0.986454</td>\n","      <td>0.995935</td>\n","      <td>0.996313</td>\n","      <td>0.871859</td>\n","      <td>0.964046</td>\n","    </tr>\n","    <tr>\n","      <th>w2v_CRF_bilstm_model</th>\n","      <td>0.995473</td>\n","      <td>0.965963</td>\n","      <td>0.992775</td>\n","      <td>0.997713</td>\n","      <td>1.000000</td>\n","      <td>0.937186</td>\n","      <td>0.973796</td>\n","    </tr>\n","    <tr>\n","      <th>POS_only_CRF_bilstm_model</th>\n","      <td>0.994304</td>\n","      <td>0.956433</td>\n","      <td>0.987959</td>\n","      <td>0.998222</td>\n","      <td>0.999737</td>\n","      <td>0.927136</td>\n","      <td>0.972578</td>\n","    </tr>\n","    <tr>\n","      <th>POS_w2v_CRF_bilstm_model</th>\n","      <td>0.994483</td>\n","      <td>0.957114</td>\n","      <td>0.987959</td>\n","      <td>0.998730</td>\n","      <td>0.999684</td>\n","      <td>0.937186</td>\n","      <td>0.972578</td>\n","    </tr>\n","    <tr>\n","      <th>POS_domain_CRF_bilstm_model</th>\n","      <td>0.995233</td>\n","      <td>0.967325</td>\n","      <td>0.991872</td>\n","      <td>0.998222</td>\n","      <td>0.999842</td>\n","      <td>0.917085</td>\n","      <td>0.975015</td>\n","    </tr>\n","    <tr>\n","      <th>POS_w2v_domain_CRF_bilstm_model</th>\n","      <td>0.995593</td>\n","      <td>0.965963</td>\n","      <td>0.992173</td>\n","      <td>0.998730</td>\n","      <td>0.999895</td>\n","      <td>0.947236</td>\n","      <td>0.973796</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-886dd653-300a-48c6-98cb-bd5eddff0077')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-886dd653-300a-48c6-98cb-bd5eddff0077 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-886dd653-300a-48c6-98cb-bd5eddff0077');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"LQvQGPaKket2"},"source":["# Model - Number of layers \n","\n","At least 2 different numbers for stacked layers "]},{"cell_type":"markdown","metadata":{"id":"0FmKXdn-kzQ7"},"source":["## Training"]},{"cell_type":"code","source":["#use the best embedding method from above\n","\n","#w2v_POS_domain_emb_table\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","HIDDEN_DIM = 50\n","EMBEDDING_TAB=w2v_POS_domain_emb_table\n","EMBEDDING_DIM=EMBEDDING_TAB.shape[1]\n","num_layers=2\n","\n","model = BiLSTM_CRF(len(word_dict), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,EMBEDDING_TAB,num_layers).to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"],"metadata":{"id":"KAiV2vWJhpR1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1142572,"status":"ok","timestamp":1653817546139,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"NTW6cByto-Vf","outputId":"d2da2b93-24e7-465f-d0b3-3facacf7e7f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch:1, Training loss: 29862.28, train acc: 0.9921, val loss: 1559.75, val acc: 0.9895, time: 570.52s\n","Epoch:2, Training loss: 2563.38, train acc: 0.9973, val loss: 1068.95, val acc: 0.9943, time: 571.82s\n"]}],"source":["import datetime\n","\n","for epoch in range(2):  \n","    time1 = datetime.datetime.now()\n","    train_loss = 0\n","\n","    model.train()\n","    for i, idxs in enumerate(train_input_index):\n","        tags_index = train_output_index[i]\n","\n","        # Step 1. Remember that Pytorch accumulates gradients.\n","        # We need to clear them out before each instance\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is,\n","        # turn them into Tensors of word indices.\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","\n","        # Step 3. Run our forward pass.\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        # calling optimizer.step()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss+=loss.item()\n","\n","    model.eval()\n","    # Call the cal_acc functions you implemented as required\n","    _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n","    _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n","\n","    val_loss = 0\n","    for i, idxs in enumerate(val_input_index):\n","        tags_index = val_output_index[i]\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","        val_loss+=loss.item()\n","    time2 = datetime.datetime.now()\n","\n","    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))"]},{"cell_type":"markdown","metadata":{"id":"xZUmC-8Dk6Ek"},"source":["## Save and Load Model"]},{"cell_type":"code","source":["torch.save(model,'/content/drive/MyDrive/COMP5046-NLP-AS2/Models/twolayer_CRF_bilstm_model.pt')"],"metadata":{"id":"nafn0Gishs9K"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bw_XAtV2pQk0"},"outputs":[],"source":["twolayer_CRF_bilstm_model=torch.load('/content/drive/MyDrive/COMP5046-NLP-AS2/Models/twolayer_CRF_bilstm_model.pt')"]},{"cell_type":"markdown","metadata":{"id":"_UbViu9yk8_d"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KfXOy7IWpZFv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653909723658,"user_tz":-600,"elapsed":26464,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"}},"outputId":"869f77a4-5c51-400b-a24e-d09d85db1cdd"},"outputs":[{"output_type":"stream","name":"stdout","text":["╒═════════════════════════════╤════════════╕\n","│ twolayer_CRF_bilstm_model   │   F1 Score │\n","╞═════════════════════════════╪════════════╡\n","│ F1                          │   0.994483 │\n","├─────────────────────────────┼────────────┤\n","│ F1 (T)                      │   0.960517 │\n","├─────────────────────────────┼────────────┤\n","│ F1 (S)                      │   0.990066 │\n","├─────────────────────────────┼────────────┤\n","│ F1 (P)                      │   0.998984 │\n","├─────────────────────────────┼────────────┤\n","│ F1 (O)                      │   0.999737 │\n","├─────────────────────────────┼────────────┤\n","│ F1 (D)                      │   0.896985 │\n","├─────────────────────────────┼────────────┤\n","│ F1 (C)                      │   0.973796 │\n","╘═════════════════════════════╧════════════╛\n"]}],"source":["twolayer_CRF_bilstm_model.eval()\n","predict=[]\n","true_label =[]\n","for sentence, tags in zip(tokensized_validation_docs,tokensized_validation_label_docs):\n","\n","  sentence_in = prepare_sequence(sentence, word_dict).to(device)\n","  targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long).to(device)\n","  _, y_pred=twolayer_CRF_bilstm_model(sentence_in)\n","  predict.extend(y_pred)\n","  true_label.extend(targets.cpu().numpy().tolist())\n","\n","f1, f1_T_label, f1_S_label, f1_P_label, f1_O_label, f1_D_label, f1_C_label=calculate_f1(predict, true_label)\n","\n","#Manually put the result in the table as the below might not be run again.\n","\n","#create table\n","table= [[\"F1\",f1],\n","        [\"F1 (T)\",f1_T_label],\n","        [\"F1 (S)\",f1_S_label],\n","        [\"F1 (P)\",f1_P_label],\n","        [\"F1 (O)\",f1_O_label],\n","        [\"F1 (D)\",f1_D_label],\n","        [\"F1 (C)\",f1_C_label]]\n","\n","col_names=[\"twolayer_CRF_bilstm_model\",\"F1 Score\"]\n","\n","Eval_Model_Name.append(\"twolayer_CRF_bilstm_model\")\n","Eval_result.append([el[1] for el in table])\n","\n","print(tabulate(table, headers=col_names,tablefmt='fancy_grid'))"]},{"cell_type":"markdown","metadata":{"id":"ieSoWS4ON_zQ"},"source":["## 3-layers"]},{"cell_type":"code","source":["#use the best embedding method from above\n","\n","#w2v_POS_domain_emb_table\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","HIDDEN_DIM = 50\n","EMBEDDING_TAB=w2v_POS_domain_emb_table\n","EMBEDDING_DIM=EMBEDDING_TAB.shape[1]\n","num_layers=3\n","\n","model = BiLSTM_CRF(len(word_dict), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,EMBEDDING_TAB,num_layers).to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"],"metadata":{"id":"Y8-ik_GqhwKw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kxnkwYTFRjDj"},"outputs":[],"source":["import datetime\n","\n","for epoch in range(2):  \n","    time1 = datetime.datetime.now()\n","    train_loss = 0\n","\n","    model.train()\n","    for i, idxs in enumerate(train_input_index):\n","        tags_index = train_output_index[i]\n","\n","        # Step 1. Remember that Pytorch accumulates gradients.\n","        # We need to clear them out before each instance\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is,\n","        # turn them into Tensors of word indices.\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","\n","        # Step 3. Run our forward pass.\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        # calling optimizer.step()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss+=loss.item()\n","\n","    model.eval()\n","    # Call the cal_acc functions you implemented as required\n","    _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n","    _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n","\n","    val_loss = 0\n","    for i, idxs in enumerate(val_input_index):\n","        tags_index = val_output_index[i]\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","        val_loss+=loss.item()\n","    time2 = datetime.datetime.now()\n","\n","    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n","\n"]},{"cell_type":"code","source":["torch.save(model,'/content/drive/MyDrive/COMP5046-NLP-AS2/Models/threelayer_CRF_bilstm_model.pt')"],"metadata":{"id":"AlBsOv1Qh1rL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["threelayer_CRF_bilstm_model=torch.load('/content/drive/MyDrive/COMP5046-NLP-AS2/Models/threelayer_CRF_bilstm_model.pt')"],"metadata":{"id":"4stCv_gnh0e6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEzmtqUhN-di","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653909751076,"user_tz":-600,"elapsed":26917,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"}},"outputId":"421bd22a-a0ba-44ad-e56c-1939b3bc5263"},"outputs":[{"output_type":"stream","name":"stdout","text":["╒═══════════════════════════════╤════════════╕\n","│ threelayer_CRF_bilstm_model   │   F1 Score │\n","╞═══════════════════════════════╪════════════╡\n","│ F1                            │   0.984919 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (T)                        │   0.924438 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (S)                        │   0.975918 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (P)                        │   0.996443 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (O)                        │   0.99705  │\n","├───────────────────────────────┼────────────┤\n","│ F1 (D)                        │   0.615578 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (C)                        │   0.945765 │\n","╘═══════════════════════════════╧════════════╛\n"]}],"source":["threelayer_CRF_bilstm_model.eval()\n","predict=[]\n","true_label =[]\n","for sentence, tags in zip(tokensized_validation_docs,tokensized_validation_label_docs):\n","\n","  sentence_in = prepare_sequence(sentence, word_dict).to(device)\n","  targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long).to(device)\n","  _, y_pred=threelayer_CRF_bilstm_model(sentence_in)\n","  predict.extend(y_pred)\n","  true_label.extend(targets.cpu().numpy().tolist())\n","\n","f1, f1_T_label, f1_S_label, f1_P_label, f1_O_label, f1_D_label, f1_C_label=calculate_f1(predict, true_label)\n","\n","#Manually put the result in the table as the below might not be run again.\n","\n","#create table\n","table= [[\"F1\",f1],\n","        [\"F1 (T)\",f1_T_label],\n","        [\"F1 (S)\",f1_S_label],\n","        [\"F1 (P)\",f1_P_label],\n","        [\"F1 (O)\",f1_O_label],\n","        [\"F1 (D)\",f1_D_label],\n","        [\"F1 (C)\",f1_C_label]]\n","\n","col_names=[\"threelayer_CRF_bilstm_model\",\"F1 Score\"]\n","\n","Eval_Model_Name.append(\"threelayer_CRF_bilstm_model\")\n","Eval_result.append([el[1] for el in table])\n","\n","print(tabulate(table, headers=col_names,tablefmt='fancy_grid'))\n"]},{"cell_type":"markdown","metadata":{"id":"2xv-KeEEQZSE"},"source":["#Model- Bi-LSTM with Different Attention Scores\n"]},{"cell_type":"markdown","metadata":{"id":"Rop0dBXLTulI"},"source":["##Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vjw6oD36TsFI"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","# default num_layer = 1 is the best\n","# default embedding table = w2v_POS_domain_emb_table is the best\n","\n","\n","class BiLSTM_CRF_ATTENTION(nn.Module):\n","\n","    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, attention='Scale Dot Product'):\n","        super(BiLSTM_CRF_ATTENTION, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.vocab_size = vocab_size\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","\n","        self.attention=attention\n","\n","\n","        #add new codes end\n","\n","        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n","\n","        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n","        #self.word_embeds.weight.data.copy_(torch.from_numpy(w2v_sg_emb_dim)) \n","        self.word_embeds.weight.data.copy_(torch.from_numpy(np.asarray(w2v_POS_domain_emb_table)))\n","        \n","        #print('Type of embedding_dim is: ', type(embedding_dim),embedding_dim)\n","        #a=hidden_dim\n","        #print('Type of hidden_dim is: ', type(a),a)\n","        \n"," \n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, \n","                            num_layers=1, bidirectional=True)\n","        \n","        #add new code -Q K V\n","        if self.attention:\n","          self.Q = nn.Linear(hidden_dim, hidden_dim)\n","          self.K = nn.Linear(hidden_dim, hidden_dim)\n","          self.V = nn.Linear(hidden_dim, hidden_dim)\n","          self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n","        else:\n","          self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n","\n","        if self.attention == 'General':\n","          self.general = nn.Linear(hidden_dim, hidden_dim)\n","\n","        # Matrix of transition parameters.  Entry i,j is the score of\n","        # transitioning *to* i *from* j.\n","        self.transitions = nn.Parameter(\n","            torch.randn(self.tagset_size, self.tagset_size))\n","\n","        # These two statements enforce the constraint that we never transfer\n","        # to the start tag and we never transfer from the stop tag\n","        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n","        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n","        self.hidden = self.init_hidden()\n","\n","    def init_hidden(self):\n","        #changed to 2*self.nums_layers\n","        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n","                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n","\n","    def _forward_alg(self, feats):\n","        # Do the forward algorithm to compute the partition function\n","        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n","        # START_TAG has all of the score.\n","        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n","\n","        # Wrap in a variable so that we will get automatic backprop\n","        forward_var = init_alphas\n","\n","        # Iterate through the sentence\n","        for feat in feats:\n","            alphas_t = []  # The forward tensors at this timestep\n","            for next_tag in range(self.tagset_size):\n","                # broadcast the emission score: it is the same regardless of\n","                # the previous tag\n","                emit_score = feat[next_tag].view(\n","                    1, -1).expand(1, self.tagset_size)\n","                # the ith entry of trans_score is the score of transitioning to\n","                # next_tag from i\n","                trans_score = self.transitions[next_tag].view(1, -1)\n","                # The ith entry of next_tag_var is the value for the\n","                # edge (i -> next_tag) before we do log-sum-exp\n","                next_tag_var = forward_var + trans_score + emit_score\n","                # The forward variable for this tag is log-sum-exp of all the\n","                # scores.\n","                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n","            forward_var = torch.cat(alphas_t).view(1, -1)\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        alpha = log_sum_exp(terminal_var)\n","        return alpha\n","\n","    #create self-attention here\n","    def cal_attention(self, lstm_out):\n","      #remove the batch size from the lstm output: (sequence length,batch_size, hidden_dim)-> (sequence length, hidden_dim)\n","      lstm_out=lstm_out.squeeze(1)\n","      #(batch size, sequnece len, hidden )\n","      query= self.Q(lstm_out).unsqueeze(0)\n","      key = self.K(lstm_out)\n","      if self.attention == 'General':\n","        key =self.general(key)\n","      key= key.transpose(1,0).unsqueeze(0)\n","      value= self.V(lstm_out).unsqueeze(0) \n","      #value= value.transpose(1,0).unsqueeze(0)\n","\n","\n","      if self.attention == 'Scale Dot Product':\n","        #attention weight\n","        attention_weight = F.softmax(torch.bmm(query, key)/torch.sqrt(torch.Tensor([self.hidden_dim]).to(device)), dim=-1) #softmax on the last dimension\n","      elif self.attention == 'Dot Product':\n","        attention_weight = F.softmax(torch.bmm(query, key), dim=-1)\n","      #linear transfer K\n","      elif self.attention == 'General':\n","        attention_weight = F.softmax(torch.bmm(query, key), dim=-1)\n","      #batch seq_len, hidden-> seq_len,batch, hidden\n","      \n","      attention_output = torch.bmm(attention_weight, value).transpose(1,0)\n","      #print('attention output shape is:', attention_output.shape)\n","      return(attention_output)\n","\n","\n","    #def _get_lstm_features(self, sentence):\n","    def _get_lstm_features(self, sentence):\n","        \n","        self.hidden = self.init_hidden()\n","        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n","        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n","\n","        # add attention\n","\n","        if self.attention:\n","          attention_output =self.cal_attention(lstm_out)\n","          #concat at the last dimension\n","          lstm_out= torch.cat((attention_output, lstm_out), -1)\n","          \n","          #remove batch size from the dimension\n","          lstm_out= lstm_out.view(len(sentence), self.hidden_dim*2)\n","          #print('lstm out with attention is: ', len(lstm_out),lstm_out[0])\n","        else:\n","          lstm_out= lstm_out.view(len(sentence), self.hidden_dim)\n","          #print('lstm out  is: ', len(lstm_out),lstm_out[0])\n","\n","        lstm_feats = self.hidden2tag(lstm_out)\n","\n","        return lstm_feats\n","\n","    def _score_sentence(self, feats, tags):\n","        # Gives the score of a provided tag sequence\n","        score = torch.zeros(1).to(device)\n","        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n","        for i, feat in enumerate(feats):\n","            score = score + \\\n","                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n","        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n","        return score\n","\n","    def _viterbi_decode(self, feats):\n","        backpointers = []\n","\n","        # Initialize the viterbi variables in log space\n","        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n","        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n","\n","        # forward_var at step i holds the viterbi variables for step i-1\n","        forward_var = init_vvars\n","        for feat in feats:\n","            bptrs_t = []  # holds the backpointers for this step\n","            viterbivars_t = []  # holds the viterbi variables for this step\n","\n","            for next_tag in range(self.tagset_size):\n","                # next_tag_var[i] holds the viterbi variable for tag i at the\n","                # previous step, plus the score of transitioning\n","                # from tag i to next_tag.\n","                # We don't include the emission scores here because the max\n","                # does not depend on them (we add them in below)\n","                next_tag_var = forward_var + self.transitions[next_tag]\n","                best_tag_id = argmax(next_tag_var)\n","                bptrs_t.append(best_tag_id)\n","                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n","            # Now add in the emission scores, and assign forward_var to the set\n","            # of viterbi variables we just computed\n","            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n","            backpointers.append(bptrs_t)\n","\n","        # Transition to STOP_TAG\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        best_tag_id = argmax(terminal_var)\n","        path_score = terminal_var[0][best_tag_id]\n","\n","        # Follow the back pointers to decode the best path.\n","        best_path = [best_tag_id]\n","        for bptrs_t in reversed(backpointers):\n","            best_tag_id = bptrs_t[best_tag_id]\n","            best_path.append(best_tag_id)\n","        # Pop off the start tag (we dont want to return that to the caller)\n","        start = best_path.pop()\n","        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n","        best_path.reverse()\n","        return path_score, best_path\n","\n","    def neg_log_likelihood(self, sentence, tags):\n","        feats = self._get_lstm_features(sentence)\n","        forward_score = self._forward_alg(feats)\n","        gold_score = self._score_sentence(feats, tags)\n","        return forward_score - gold_score\n","    \n","    def forward(self, sentence):\n","    #def forward(self, sentence):  # dont confuse this with _forward_alg above.\n","        # Get the emission scores from the BiLSTM\n","        lstm_feats = self._get_lstm_features(sentence)\n","        score, tag_seq = self._viterbi_decode(lstm_feats)\n","\n","        return score, tag_seq\n","      \n"]},{"cell_type":"code","source":["#Attention code references\n","#https://github.com/sooftware/attentions/blob/master/attentions.py\n","#https://github.com/datnnt1997/multi-head_self-attention/blob/master/SelfAttention.ipynb\n","#https://colab.research.google.com/github/mrm8488/shared_colab_notebooks/blob/master/basic_self_attention_.ipynb#scrollTo=0XQ6NsIuDtgr\n","#https://medium.com/@makeesyai/transformer-model-self-attention-implementation-50e68cd4de39"],"metadata":{"id":"0euKGd_uRDsh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## General"],"metadata":{"id":"ByzXGf2aUbfH"}},{"cell_type":"markdown","metadata":{"id":"Kg0SRU3jTyiL"},"source":["### Training - General"]},{"cell_type":"code","source":["HIDDEN_DIM = 50\n","EMBEDDING_TAB=w2v_POS_domain_emb_table\n","EMBEDDING_DIM=EMBEDDING_TAB.shape[1]\n","\n","model=BiLSTM_CRF_ATTENTION(len(word_dict), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, attention='General').to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"],"metadata":{"id":"Zm5ue-pgiJN6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1136122,"status":"ok","timestamp":1653830534773,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"st-SdwQH6Frp","outputId":"c4ffbe19-1b9a-4e71-d496-d50a6f8aa9c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch:1, Training loss: 19323.05, train acc: 0.9946, val loss: 1298.72, val acc: 0.9917, time: 566.86s\n","Epoch:2, Training loss: 1875.29, train acc: 0.9991, val loss: 871.80, val acc: 0.9955, time: 568.52s\n"]}],"source":["import datetime\n","\n","for epoch in range(2):  \n","    time1 = datetime.datetime.now()\n","    train_loss = 0\n","\n","    model.train()\n","    for i, idxs in enumerate(train_input_index):\n","        tags_index = train_output_index[i]\n","\n","        # Step 1. Remember that Pytorch accumulates gradients.\n","        # We need to clear them out before each instance\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is,\n","        # turn them into Tensors of word indices.\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","\n","        # Step 3. Run our forward pass.\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        # calling optimizer.step()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss+=loss.item()\n","\n","    model.eval()\n","    # Call the cal_acc functions you implemented as required\n","    _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n","    _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n","\n","    val_loss = 0\n","    for i, idxs in enumerate(val_input_index):\n","        tags_index = val_output_index[i]\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","        val_loss+=loss.item()\n","    time2 = datetime.datetime.now()\n","\n","    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))"]},{"cell_type":"markdown","metadata":{"id":"psMRHClzT0QR"},"source":["### Save and Load "]},{"cell_type":"code","source":["torch.save(model,'/content/drive/MyDrive/COMP5046-NLP-AS2/Models/attention_general_model.pt')"],"metadata":{"id":"217H5Z8FiM-i"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wxKTscJ6UDHm"},"outputs":[],"source":["attention_general_model=torch.load('/content/drive/MyDrive/COMP5046-NLP-AS2/Models/attention_general_model.pt')"]},{"cell_type":"markdown","metadata":{"id":"BwU9Dld2WxyX"},"source":["### Evaluation - Attention Score (General)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CHAZ5FoGWxFI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653909841258,"user_tz":-600,"elapsed":27638,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"}},"outputId":"dd329c2c-719f-4bb1-8824-b77c08b841f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["╒═══════════════════════════╤════════════╕\n","│ attention_general_model   │   F1 Score │\n","╞═══════════════════════════╪════════════╡\n","│ F1                        │   0.995623 │\n","├───────────────────────────┼────────────┤\n","│ F1 (T)                    │   0.965963 │\n","├───────────────────────────┼────────────┤\n","│ F1 (S)                    │   0.992474 │\n","├───────────────────────────┼────────────┤\n","│ F1 (P)                    │   0.99873  │\n","├───────────────────────────┼────────────┤\n","│ F1 (O)                    │   0.999684 │\n","├───────────────────────────┼────────────┤\n","│ F1 (D)                    │   0.957286 │\n","├───────────────────────────┼────────────┤\n","│ F1 (C)                    │   0.973796 │\n","╘═══════════════════════════╧════════════╛\n"]}],"source":["attention_general_model.eval()\n","predict=[]\n","true_label =[]\n","for sentence, tags in zip(tokensized_validation_docs,tokensized_validation_label_docs):\n","  sentence_in = prepare_sequence(sentence, word_dict).to(device)\n","  targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long).to(device)\n","  _, y_pred=attention_general_model(sentence_in)\n","  predict.extend(y_pred)\n","  true_label.extend(targets.cpu().numpy().tolist())\n","\n","f1, f1_T_label, f1_S_label, f1_P_label, f1_O_label, f1_D_label, f1_C_label=calculate_f1(predict, true_label)\n","\n","#Manually put the result in the table as the below might not be run again.\n","\n","#create table\n","table= [[\"F1\",f1],\n","        [\"F1 (T)\",f1_T_label],\n","        [\"F1 (S)\",f1_S_label],\n","        [\"F1 (P)\",f1_P_label],\n","        [\"F1 (O)\",f1_O_label],\n","        [\"F1 (D)\",f1_D_label],\n","        [\"F1 (C)\",f1_C_label]]\n","\n","col_names=[\"attention_general_model\",\"F1 Score\"]\n","\n","Eval_Model_Name.append(\"attention_general_model\")\n","Eval_result.append([el[1] for el in table])\n","\n","print(tabulate(table, headers=col_names,tablefmt='fancy_grid'))"]},{"cell_type":"markdown","source":["##General- fasttext"],"metadata":{"id":"m56lYpXYdy81"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","# default num_layer = 1 is the best\n","# default embedding table = w2v_POS_domain_emb_table is the best\n","\n","\n","class BiLSTM_CRF_ATTENTION_2(nn.Module):\n","\n","    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, attention='Scale Dot Product'):\n","        super(BiLSTM_CRF_ATTENTION_2, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.vocab_size = vocab_size\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","\n","        self.attention=attention\n","\n","\n","        #add new codes end\n","\n","        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n","\n","        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n","        #self.word_embeds.weight.data.copy_(torch.from_numpy(w2v_sg_emb_dim)) \n","        self.word_embeds.weight.data.copy_(torch.from_numpy(np.asarray(ft_POS_domain_emb_table)))\n","        \n","        #print('Type of embedding_dim is: ', type(embedding_dim),embedding_dim)\n","        #a=hidden_dim\n","        #print('Type of hidden_dim is: ', type(a),a)\n","        \n"," \n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, \n","                            num_layers=1, bidirectional=True)\n","        \n","        #add new code -Q K V\n","        if self.attention:\n","          self.Q = nn.Linear(hidden_dim, hidden_dim)\n","          self.K = nn.Linear(hidden_dim, hidden_dim)\n","          self.V = nn.Linear(hidden_dim, hidden_dim)\n","          self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n","        else:\n","          self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n","\n","        if self.attention == 'General':\n","          self.general = nn.Linear(hidden_dim, hidden_dim)\n","\n","        # Matrix of transition parameters.  Entry i,j is the score of\n","        # transitioning *to* i *from* j.\n","        self.transitions = nn.Parameter(\n","            torch.randn(self.tagset_size, self.tagset_size))\n","\n","        # These two statements enforce the constraint that we never transfer\n","        # to the start tag and we never transfer from the stop tag\n","        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n","        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n","        self.hidden = self.init_hidden()\n","\n","    def init_hidden(self):\n","        #changed to 2*self.nums_layers\n","        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n","                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n","\n","    def _forward_alg(self, feats):\n","        # Do the forward algorithm to compute the partition function\n","        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n","        # START_TAG has all of the score.\n","        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n","\n","        # Wrap in a variable so that we will get automatic backprop\n","        forward_var = init_alphas\n","\n","        # Iterate through the sentence\n","        for feat in feats:\n","            alphas_t = []  # The forward tensors at this timestep\n","            for next_tag in range(self.tagset_size):\n","                # broadcast the emission score: it is the same regardless of\n","                # the previous tag\n","                emit_score = feat[next_tag].view(\n","                    1, -1).expand(1, self.tagset_size)\n","                # the ith entry of trans_score is the score of transitioning to\n","                # next_tag from i\n","                trans_score = self.transitions[next_tag].view(1, -1)\n","                # The ith entry of next_tag_var is the value for the\n","                # edge (i -> next_tag) before we do log-sum-exp\n","                next_tag_var = forward_var + trans_score + emit_score\n","                # The forward variable for this tag is log-sum-exp of all the\n","                # scores.\n","                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n","            forward_var = torch.cat(alphas_t).view(1, -1)\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        alpha = log_sum_exp(terminal_var)\n","        return alpha\n","\n","    #create self-attention here\n","    def cal_attention(self, lstm_out):\n","      #remove the batch size from the lstm output: (sequence length,batch_size, hidden_dim)-> (sequence length, hidden_dim)\n","      lstm_out=lstm_out.squeeze(1)\n","      #(batch size, sequnece len, hidden )\n","      query= self.Q(lstm_out).unsqueeze(0)\n","      key = self.K(lstm_out)\n","      if self.attention == 'General':\n","        key =self.general(key)\n","      key= key.transpose(1,0).unsqueeze(0)\n","      value= self.V(lstm_out).unsqueeze(0) \n","      #value= value.transpose(1,0).unsqueeze(0)\n","\n","\n","      if self.attention == 'Scale Dot Product':\n","        #attention weight\n","        attention_weight = F.softmax(torch.bmm(query, key)/torch.sqrt(torch.Tensor([self.hidden_dim]).to(device)), dim=-1) #softmax on the last dimension\n","      elif self.attention == 'Dot Product':\n","        attention_weight = F.softmax(torch.bmm(query, key), dim=-1)\n","      #linear transfer K\n","      elif self.attention == 'General':\n","        attention_weight = F.softmax(torch.bmm(query, key), dim=-1)\n","      #batch seq_len, hidden-> seq_len,batch, hidden\n","      \n","      attention_output = torch.bmm(attention_weight, value).transpose(1,0)\n","      #print('attention output shape is:', attention_output.shape)\n","      return(attention_output)\n","\n","\n","    #def _get_lstm_features(self, sentence):\n","    def _get_lstm_features(self, sentence):\n","        \n","        self.hidden = self.init_hidden()\n","        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n","        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n","\n","        # add attention\n","\n","        if self.attention:\n","          attention_output =self.cal_attention(lstm_out)\n","          #concat at the last dimension\n","          lstm_out= torch.cat((attention_output, lstm_out), -1)\n","          \n","          #remove batch size from the dimension\n","          lstm_out= lstm_out.view(len(sentence), self.hidden_dim*2)\n","          #print('lstm out with attention is: ', len(lstm_out),lstm_out[0])\n","        else:\n","          lstm_out= lstm_out.view(len(sentence), self.hidden_dim)\n","          #print('lstm out  is: ', len(lstm_out),lstm_out[0])\n","\n","        lstm_feats = self.hidden2tag(lstm_out)\n","\n","        return lstm_feats\n","\n","    def _score_sentence(self, feats, tags):\n","        # Gives the score of a provided tag sequence\n","        score = torch.zeros(1).to(device)\n","        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n","        for i, feat in enumerate(feats):\n","            score = score + \\\n","                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n","        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n","        return score\n","\n","    def _viterbi_decode(self, feats):\n","        backpointers = []\n","\n","        # Initialize the viterbi variables in log space\n","        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n","        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n","\n","        # forward_var at step i holds the viterbi variables for step i-1\n","        forward_var = init_vvars\n","        for feat in feats:\n","            bptrs_t = []  # holds the backpointers for this step\n","            viterbivars_t = []  # holds the viterbi variables for this step\n","\n","            for next_tag in range(self.tagset_size):\n","                # next_tag_var[i] holds the viterbi variable for tag i at the\n","                # previous step, plus the score of transitioning\n","                # from tag i to next_tag.\n","                # We don't include the emission scores here because the max\n","                # does not depend on them (we add them in below)\n","                next_tag_var = forward_var + self.transitions[next_tag]\n","                best_tag_id = argmax(next_tag_var)\n","                bptrs_t.append(best_tag_id)\n","                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n","            # Now add in the emission scores, and assign forward_var to the set\n","            # of viterbi variables we just computed\n","            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n","            backpointers.append(bptrs_t)\n","\n","        # Transition to STOP_TAG\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        best_tag_id = argmax(terminal_var)\n","        path_score = terminal_var[0][best_tag_id]\n","\n","        # Follow the back pointers to decode the best path.\n","        best_path = [best_tag_id]\n","        for bptrs_t in reversed(backpointers):\n","            best_tag_id = bptrs_t[best_tag_id]\n","            best_path.append(best_tag_id)\n","        # Pop off the start tag (we dont want to return that to the caller)\n","        start = best_path.pop()\n","        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n","        best_path.reverse()\n","        return path_score, best_path\n","\n","    def neg_log_likelihood(self, sentence, tags):\n","        feats = self._get_lstm_features(sentence)\n","        forward_score = self._forward_alg(feats)\n","        gold_score = self._score_sentence(feats, tags)\n","        return forward_score - gold_score\n","    \n","    def forward(self, sentence):\n","    #def forward(self, sentence):  # dont confuse this with _forward_alg above.\n","        # Get the emission scores from the BiLSTM\n","        lstm_feats = self._get_lstm_features(sentence)\n","        score, tag_seq = self._viterbi_decode(lstm_feats)\n","\n","        return score, tag_seq\n","      \n"],"metadata":{"id":"jqD6ne50eTuv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["HIDDEN_DIM = 50\n","EMBEDDING_TAB=ft_POS_domain_emb_table\n","EMBEDDING_DIM=EMBEDDING_TAB.shape[1]\n","\n","model=BiLSTM_CRF_ATTENTION_2(len(word_dict), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, attention='General').to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n","\n","import datetime\n","\n","for epoch in range(2):  \n","    time1 = datetime.datetime.now()\n","    train_loss = 0\n","\n","    model.train()\n","    for i, idxs in enumerate(train_input_index):\n","        tags_index = train_output_index[i]\n","\n","        # Step 1. Remember that Pytorch accumulates gradients.\n","        # We need to clear them out before each instance\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is,\n","        # turn them into Tensors of word indices.\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","\n","        # Step 3. Run our forward pass.\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        # calling optimizer.step()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss+=loss.item()\n","\n","    model.eval()\n","    # Call the cal_acc functions you implemented as required\n","    _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n","    _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n","\n","    val_loss = 0\n","    for i, idxs in enumerate(val_input_index):\n","        tags_index = val_output_index[i]\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","        val_loss+=loss.item()\n","    time2 = datetime.datetime.now()\n","\n","    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRCOFH9Od11_","executionInfo":{"status":"ok","timestamp":1654075074701,"user_tz":-600,"elapsed":1331672,"user":{"displayName":"roger wang","userId":"04313800435014207614"}},"outputId":"22ece54c-399c-4be3-f405-66b325391569"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:1, Training loss: 18790.36, train acc: 0.9952, val loss: 1159.22, val acc: 0.9931, time: 689.69s\n","Epoch:2, Training loss: 1657.33, train acc: 0.9990, val loss: 866.71, val acc: 0.9947, time: 641.82s\n"]}]},{"cell_type":"code","source":["torch.save(model,'/content/drive/MyDrive/COMP5046-NLP-AS2/Models/ft_attention_general_model.pt')\n","ft_attention_general_model=torch.load('/content/drive/MyDrive/COMP5046-NLP-AS2/Models/ft_attention_general_model.pt')"],"metadata":{"id":"10uI4Vdqeosz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ft_attention_general_model.eval()\n","predict=[]\n","true_label =[]\n","for sentence, tags in zip(tokensized_validation_docs,tokensized_validation_label_docs):\n","\n","\n","  sentence_in = prepare_sequence(sentence, word_dict).to(device)\n","  targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long).to(device)\n","  _, y_pred=ft_attention_general_model(sentence_in)\n","  predict.extend(y_pred)\n","  true_label.extend(targets.cpu().numpy().tolist())\n","\n","f1, f1_T_label, f1_S_label, f1_P_label, f1_O_label, f1_D_label, f1_C_label=calculate_f1(predict, true_label)\n","\n","table_2= [[\"F1\",f1],\n","        [\"F1 (T)\",f1_T_label],\n","        [\"F1 (S)\",f1_S_label],\n","        [\"F1 (P)\",f1_P_label],\n","        [\"F1 (O)\",f1_O_label],\n","        [\"F1 (D)\",f1_D_label],\n","        [\"F1 (C)\",f1_C_label]\n","    \n","        ]\n","col_names=[\"F1 Labels\",\"F1 Score\"]\n","\n","print(tabulate(table_2, headers=col_names,tablefmt='fancy_grid'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-MbKX6fpe7jP","executionInfo":{"status":"ok","timestamp":1654081149134,"user_tz":-600,"elapsed":29194,"user":{"displayName":"roger wang","userId":"04313800435014207614"}},"outputId":"16208eed-7e3f-4083-898e-a3a1f6332f66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["╒═════════════╤════════════╕\n","│ F1 Labels   │   F1 Score │\n","╞═════════════╪════════════╡\n","│ F1          │   0.994843 │\n","├─────────────┼────────────┤\n","│ F1 (T)      │   0.971409 │\n","├─────────────┼────────────┤\n","│ F1 (S)      │   0.991872 │\n","├─────────────┼────────────┤\n","│ F1 (P)      │   0.999492 │\n","├─────────────┼────────────┤\n","│ F1 (O)      │   0.997524 │\n","├─────────────┼────────────┤\n","│ F1 (D)      │   0.967337 │\n","├─────────────┼────────────┤\n","│ F1 (C)      │   0.975015 │\n","╘═════════════╧════════════╛\n"]}]},{"cell_type":"markdown","source":["## Scale Dot Product"],"metadata":{"id":"cdhRblFcUfuF"}},{"cell_type":"markdown","metadata":{"id":"4ZmgGR_-UgAz"},"source":["### Training - Scale Dot Product"]},{"cell_type":"code","source":["HIDDEN_DIM = 50\n","EMBEDDING_TAB=w2v_POS_domain_emb_table\n","EMBEDDING_DIM=EMBEDDING_TAB.shape[1]\n","\n","model=BiLSTM_CRF_ATTENTION(len(word_dict), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, attention='Scale Dot Product').to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"],"metadata":{"id":"aiwD0UU-iSwa"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fvwONDazUk6E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653892903289,"user_tz":-600,"elapsed":1207075,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"}},"outputId":"d62acd01-4cd6-47c9-db71-3ce853564705"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:1, Training loss: 18397.71, train acc: 0.9944, val loss: 1264.00, val acc: 0.9927, time: 605.80s\n","Epoch:2, Training loss: 1807.66, train acc: 0.9985, val loss: 866.39, val acc: 0.9953, time: 600.92s\n"]}],"source":["import datetime\n","\n","for epoch in range(2):  \n","    time1 = datetime.datetime.now()\n","    train_loss = 0\n","\n","    model.train()\n","    for i, idxs in enumerate(train_input_index):\n","        tags_index = train_output_index[i]\n","\n","        # Step 1. Remember that Pytorch accumulates gradients.\n","        # We need to clear them out before each instance\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is,\n","        # turn them into Tensors of word indices.\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","\n","        # Step 3. Run our forward pass.\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        # calling optimizer.step()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss+=loss.item()\n","\n","    model.eval()\n","    # Call the cal_acc functions you implemented as required\n","    _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n","    _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n","\n","    val_loss = 0\n","    for i, idxs in enumerate(val_input_index):\n","        tags_index = val_output_index[i]\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","        val_loss+=loss.item()\n","    time2 = datetime.datetime.now()\n","\n","    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n"]},{"cell_type":"markdown","metadata":{"id":"mjWPAkUtZI3o"},"source":["### Save and Load"]},{"cell_type":"code","source":["torch.save(model,'/content/drive/MyDrive/COMP5046-NLP-AS2/Models/attention_scale_dot_model.pt')"],"metadata":{"id":"yQid1Ou0iXHv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CAy5_daSZK0Z"},"outputs":[],"source":["attention_scale_dot_model=torch.load('/content/drive/MyDrive/COMP5046-NLP-AS2/Models/attention_scale_dot_model.pt')"]},{"cell_type":"markdown","metadata":{"id":"YbisqPHyZTKL"},"source":["### Evaluation - Attention Score (Scale Dot Product)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27969,"status":"ok","timestamp":1653909869722,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"mbQm4UbbZYc4","outputId":"450d7a38-1f12-4dba-f235-64e6d6fadde7"},"outputs":[{"output_type":"stream","name":"stdout","text":["╒═════════════════════════════╤════════════╕\n","│ attention_scale_dot_model   │   F1 Score │\n","╞═════════════════════════════╪════════════╡\n","│ F1                          │   0.995413 │\n","├─────────────────────────────┼────────────┤\n","│ F1 (T)                      │   0.967325 │\n","├─────────────────────────────┼────────────┤\n","│ F1 (S)                      │   0.992474 │\n","├─────────────────────────────┼────────────┤\n","│ F1 (P)                      │   0.99873  │\n","├─────────────────────────────┼────────────┤\n","│ F1 (O)                      │   0.999789 │\n","├─────────────────────────────┼────────────┤\n","│ F1 (D)                      │   0.922111 │\n","├─────────────────────────────┼────────────┤\n","│ F1 (C)                      │   0.975625 │\n","╘═════════════════════════════╧════════════╛\n"]}],"source":["attention_scale_dot_model.eval()\n","predict=[]\n","true_label =[]\n","for sentence, tags in zip(tokensized_validation_docs,tokensized_validation_label_docs):\n","  sentence_in = prepare_sequence(sentence, word_dict).to(device)\n","  targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long).to(device)\n","  _, y_pred=attention_scale_dot_model(sentence_in)\n","  predict.extend(y_pred)\n","  true_label.extend(targets.cpu().numpy().tolist())\n","\n","f1, f1_T_label, f1_S_label, f1_P_label, f1_O_label, f1_D_label, f1_C_label=calculate_f1(predict, true_label)\n","\n","#Manually put the result in the table as the below might not be run again.\n","\n","#create table\n","table= [[\"F1\",f1],\n","        [\"F1 (T)\",f1_T_label],\n","        [\"F1 (S)\",f1_S_label],\n","        [\"F1 (P)\",f1_P_label],\n","        [\"F1 (O)\",f1_O_label],\n","        [\"F1 (D)\",f1_D_label],\n","        [\"F1 (C)\",f1_C_label]]\n","\n","col_names=[\"attention_scale_dot_model\",\"F1 Score\"]\n","\n","Eval_Model_Name.append(\"attention_scale_dot_model\")\n","Eval_result.append([el[1] for el in table])\n","\n","print(tabulate(table, headers=col_names,tablefmt='fancy_grid'))"]},{"cell_type":"markdown","source":["## Dot Product"],"metadata":{"id":"3ChXf9N0UmI6"}},{"cell_type":"markdown","metadata":{"id":"7XCbhHwkU5B_"},"source":["### Training - Dot Product"]},{"cell_type":"code","source":["HIDDEN_DIM = 50\n","EMBEDDING_TAB=w2v_POS_domain_emb_table\n","EMBEDDING_DIM=EMBEDDING_TAB.shape[1]\n","\n","model=BiLSTM_CRF_ATTENTION(len(word_dict), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, attention='Dot Product').to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"],"metadata":{"id":"y_7PSoYyoQ9D"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1192049,"status":"ok","timestamp":1653894147172,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"bAM-nIT2U8ZN","outputId":"6c89396d-9225-4bb1-a5ff-cc0beedc6619"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:1, Training loss: 18448.00, train acc: 0.9947, val loss: 1300.85, val acc: 0.9925, time: 596.29s\n","Epoch:2, Training loss: 1769.95, train acc: 0.9987, val loss: 872.20, val acc: 0.9954, time: 595.44s\n"]}],"source":["import datetime\n","\n","for epoch in range(2):  \n","    time1 = datetime.datetime.now()\n","    train_loss = 0\n","\n","    model.train()\n","    for i, idxs in enumerate(train_input_index):\n","        tags_index = train_output_index[i]\n","\n","        # Step 1. Remember that Pytorch accumulates gradients.\n","        # We need to clear them out before each instance\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is,\n","        # turn them into Tensors of word indices.\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","\n","        # Step 3. Run our forward pass.\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        # calling optimizer.step()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss+=loss.item()\n","\n","    model.eval()\n","    # Call the cal_acc functions you implemented as required\n","    _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n","    _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n","\n","    val_loss = 0\n","    for i, idxs in enumerate(val_input_index):\n","        tags_index = val_output_index[i]\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","        val_loss+=loss.item()\n","    time2 = datetime.datetime.now()\n","\n","    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n"]},{"cell_type":"markdown","metadata":{"id":"oui7iwLVaDxh"},"source":["### Save and Load"]},{"cell_type":"code","source":["torch.save(model,'/content/drive/MyDrive/COMP5046-NLP-AS2/Models/attention_dot_product_model.pt')"],"metadata":{"id":"_UFvRpRwij7s"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w53WJrkTaGT4"},"outputs":[],"source":["attention_dot_product_model=torch.load('/content/drive/MyDrive/COMP5046-NLP-AS2/Models/attention_dot_product_model.pt')"]},{"cell_type":"markdown","metadata":{"id":"3RbrPw0-T2Mx"},"source":["### Evaluation - Attention Score (Dot Product)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27252,"status":"ok","timestamp":1653909897515,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"S_TtMEKpUEWm","outputId":"4053f3ed-a8a0-46b4-acf1-b04ad1002dfe"},"outputs":[{"output_type":"stream","name":"stdout","text":["╒═══════════════════════════════╤════════════╕\n","│ attention_dot_product_model   │   F1 Score │\n","╞═══════════════════════════════╪════════════╡\n","│ F1                            │   0.995503 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (T)                        │   0.967325 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (S)                        │   0.991872 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (P)                        │   0.998984 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (O)                        │   0.999789 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (D)                        │   0.944724 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (C)                        │   0.972578 │\n","╘═══════════════════════════════╧════════════╛\n"]}],"source":["attention_dot_product_model.eval()\n","predict=[]\n","true_label =[]\n","for sentence, tags in zip(tokensized_validation_docs,tokensized_validation_label_docs):\n","  sentence_in = prepare_sequence(sentence, word_dict).to(device)\n","  targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long).to(device)\n","  _, y_pred=attention_dot_product_model(sentence_in)\n","  predict.extend(y_pred)\n","  true_label.extend(targets.cpu().numpy().tolist())\n","\n","f1, f1_T_label, f1_S_label, f1_P_label, f1_O_label, f1_D_label, f1_C_label=calculate_f1(predict, true_label)\n","\n","#Manually put the result in the table as the below might not be run again.\n","\n","#create table\n","table= [[\"F1\",f1],\n","        [\"F1 (T)\",f1_T_label],\n","        [\"F1 (S)\",f1_S_label],\n","        [\"F1 (P)\",f1_P_label],\n","        [\"F1 (O)\",f1_O_label],\n","        [\"F1 (D)\",f1_D_label],\n","        [\"F1 (C)\",f1_C_label]]\n","\n","col_names=[\"attention_dot_product_model\",\"F1 Score\"]\n","\n","Eval_Model_Name.append(\"attention_dot_product_model\")\n","Eval_result.append([el[1] for el in table])\n","\n","print(tabulate(table, headers=col_names,tablefmt='fancy_grid'))"]},{"cell_type":"markdown","metadata":{"id":"D8mDOlNeENJv"},"source":["#Model- Bi-LSTM without CRF"]},{"cell_type":"markdown","source":["## Bi-LSTM without Attention without CRF"],"metadata":{"id":"X7fVnWlOF_yN"}},{"cell_type":"markdown","source":["### Build Model\n"],"metadata":{"id":"PEeK8XJpGQTk"}},{"cell_type":"code","source":["# default num_layer = 1 is the best\n","# default embedding table = w2v_POS_domain_emb_table is the best\n","\n","class BiLSTM_only(nn.Module):\n","\n","    def __init__(self, vocab_size, tag_to_ix,embedding_dim, hidden_dim):\n","        super(BiLSTM_only, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.vocab_size = vocab_size\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","\n","        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n","        self.word_embeds.weight.data.copy_(torch.from_numpy(np.asarray(w2v_POS_domain_emb_table))) \n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n","                            num_layers=1, bidirectional=True)\n","      \n","\n","        # Maps the output of the LSTM into tag space.\n","        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n","\n","        # Matrix of transition parameters.  Entry i,j is the score of\n","        # transitioning *to* i *from* j.\n","        self.transitions = nn.Parameter(\n","            torch.randn(self.tagset_size, self.tagset_size))\n","\n","        # These two statements enforce the constraint that we never transfer\n","        # to the start tag and we never transfer from the stop tag\n","        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n","        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n","\n","        self.hidden = self.init_hidden()\n","\n","    def init_hidden(self):\n","        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n","                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n","\n","    def _forward_alg(self, feats):\n","        # Do the forward algorithm to compute the partition function\n","        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n","        # START_TAG has all of the score.\n","        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n","\n","        # Wrap in a variable so that we will get automatic backprop\n","        forward_var = init_alphas\n","\n","        # Iterate through the sentence\n","        for feat in feats:\n","            alphas_t = []  # The forward tensors at this timestep\n","            for next_tag in range(self.tagset_size):\n","                # broadcast the emission score: it is the same regardless of\n","                # the previous tag\n","                emit_score = feat[next_tag].view(\n","                    1, -1).expand(1, self.tagset_size)\n","                # the ith entry of trans_score is the score of transitioning to\n","                # next_tag from i\n","                trans_score = self.transitions[next_tag].view(1, -1)\n","                # The ith entry of next_tag_var is the value for the\n","                # edge (i -> next_tag) before we do log-sum-exp\n","                next_tag_var = forward_var + trans_score + emit_score\n","                # The forward variable for this tag is log-sum-exp of all the\n","                # scores.\n","                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n","            forward_var = torch.cat(alphas_t).view(1, -1)\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        alpha = log_sum_exp(terminal_var)\n","        return alpha\n","\n","    def _get_lstm_features(self, sentence):\n","        self.hidden = self.init_hidden()\n","        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n","        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n","        \n","        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n","        #print(lstm_out.shape)\n","        lstm_feats = self.hidden2tag(lstm_out)\n","        #print(lstm_feats.shape)\n","        return lstm_feats\n","\n","    def _score_sentence(self, feats, tags):\n","        # Gives the score of a provided tag sequence\n","        score = torch.zeros(1).to(device)\n","        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n","        for i, feat in enumerate(feats):\n","            score = score + \\\n","                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n","        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n","        return score\n","    \n","    def neg_log_likelihood(self, sentence, tags):\n","        feats = self._get_lstm_features(sentence)\n","        forward_score = self._forward_alg(feats)\n","        gold_score = self._score_sentence(feats, tags)\n","        return forward_score - gold_score\n","\n","    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n","        # Get the emission scores from the BiLSTM\n","        lstm_feats = self._get_lstm_features(sentence)\n","\n","        # Find the best path, given the features.\n","        #score, tag_seq = self._viterbi_decode(lstm_feats)\n","        tag_seq = torch.argmax(F.softmax(lstm_feats, -1), -1)\n","        return  tag_seq.cpu().numpy().tolist()"],"metadata":{"id":"9NNyOrvp6m48"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"voiUf0o9sWj3"},"source":["### Training"]},{"cell_type":"code","source":["def cal_acc(model, input_index, output_index):\n","    ground_truth = []\n","    predicted = []\n","    for i,idxs in enumerate(input_index):\n","        ground_truth += output_index[i]\n","        pred = model(torch.tensor(idxs, dtype=torch.long).to(device))\n","        predicted += pred\n","    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n","    return predicted, ground_truth, accuracy"],"metadata":{"id":"N7Y0jn9xqBKT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#default embedding table = w2v_POS_domain_emb_table\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","HIDDEN_DIM = 50\n","EMBEDDING_TAB= w2v_POS_domain_emb_table\n","EMBEDDING_DIM=EMBEDDING_TAB.shape[1]\n","\n","model = BiLSTM_only(len(word_dict), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM).to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"],"metadata":{"id":"J9dpTpDs7H8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datetime\n","\n","for epoch in range(2):  \n","    time1 = datetime.datetime.now()\n","    train_loss = 0\n","\n","    model.train()\n","    for i, idxs in enumerate(train_input_index):\n","        tags_index = train_output_index[i]\n","\n","        # Step 1. Remember that Pytorch accumulates gradients.\n","        # We need to clear them out before each instance\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is,\n","        # turn them into Tensors of word indices.\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","\n","        # Step 3. Run our forward pass.\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        # calling optimizer.step()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss+=loss.item()\n","\n","    model.eval()\n","    # Call the cal_acc functions you implemented as required\n","    _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n","    _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n","\n","    val_loss = 0\n","    for i, idxs in enumerate(val_input_index):\n","        tags_index = val_output_index[i]\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","        val_loss+=loss.item()\n","    time2 = datetime.datetime.now()\n","\n","    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3nWeYJi77YtU","executionInfo":{"status":"ok","timestamp":1653900855401,"user_tz":-600,"elapsed":1000460,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"}},"outputId":"3473fbab-e94b-40e2-c1a7-f8b9dd5fa583"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:1, Training loss: 17707.22,  val loss: 1212.42,  time: 514.55s\n","Epoch:2, Training loss: 1769.19,  val loss: 854.31,  time: 485.41s\n"]}]},{"cell_type":"markdown","source":["### Save and Load\n"],"metadata":{"id":"vEuFm3MpGcTv"}},{"cell_type":"code","source":["torch.save(model,'/content/drive/MyDrive/COMP5046-NLP-AS2/Models/no_attention_no_CRF_model.pt')"],"metadata":{"id":"R-_CX6mNGfzJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["no_attention_no_CRF_model=torch.load('/content/drive/MyDrive/COMP5046-NLP-AS2/Models/no_attention_no_CRF_model.pt')"],"metadata":{"id":"PVWuAZoqGpqQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation"],"metadata":{"id":"0ZRoNyxDGgdZ"}},{"cell_type":"code","source":["no_attention_no_CRF_model.eval()\n","predict=[]\n","true_label =[]\n","for sentence, tags in zip(tokensized_validation_docs,tokensized_validation_label_docs):\n","  sentence_in = prepare_sequence(sentence, word_dict).to(device)\n","  targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long).to(device)\n","  y_pred=no_attention_no_CRF_model(sentence_in)\n","  predict.extend(y_pred)\n","  true_label.extend(targets.cpu().numpy().tolist())\n","\n","f1, f1_T_label, f1_S_label, f1_P_label, f1_O_label, f1_D_label, f1_C_label=calculate_f1(predict, true_label)"],"metadata":{"id":"ro428Vm67i4c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Manually put the result in the table as the below might not be run again.\n","\n","#create table\n","table= [[\"F1\",f1],\n","        [\"F1 (T)\",f1_T_label],\n","        [\"F1 (S)\",f1_S_label],\n","        [\"F1 (P)\",f1_P_label],\n","        [\"F1 (O)\",f1_O_label],\n","        [\"F1 (D)\",f1_D_label],\n","        [\"F1 (C)\",f1_C_label]]\n","\n","col_names=[\"no_attention_no_CRF_model\",\"F1 Score\"]\n","\n","Eval_Model_Name.append(\"no_attention_no_CRF_model\")\n","Eval_result.append([el[1] for el in table])\n","\n","print(tabulate(table, headers=col_names,tablefmt='fancy_grid'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CnkGNTjL7lzU","executionInfo":{"status":"ok","timestamp":1653909941645,"user_tz":-600,"elapsed":319,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"}},"outputId":"8e3fde71-2077-444c-ea5a-9ece17486ce4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["╒═════════════════════════════╤════════════╕\n","│ no_attention_no_CRF_model   │   F1 Score │\n","╞═════════════════════════════╪════════════╡\n","│ F1                          │   0.995443 │\n","├─────────────────────────────┼────────────┤\n","│ F1 (T)                      │   0.966644 │\n","├─────────────────────────────┼────────────┤\n","│ F1 (S)                      │   0.992474 │\n","├─────────────────────────────┼────────────┤\n","│ F1 (P)                      │   0.998222 │\n","├─────────────────────────────┼────────────┤\n","│ F1 (O)                      │   1        │\n","├─────────────────────────────┼────────────┤\n","│ F1 (D)                      │   0.929648 │\n","├─────────────────────────────┼────────────┤\n","│ F1 (C)                      │   0.973796 │\n","╘═════════════════════════════╧════════════╛\n"]}]},{"cell_type":"markdown","metadata":{"id":"1fP5ldiTsRp1"},"source":["## Bi-LSTM with Attention without CRF"]},{"cell_type":"markdown","source":["### Build Model\n"],"metadata":{"id":"QiFWKblHHGaV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qf2YAvbHQc9F"},"outputs":[],"source":["# default num_layer = 1 is the best\n","# default embedding table = w2v_POS_domain_emb_table is the best\n","\n","class BiLSTM_ATTENTION_no_CRF(nn.Module):\n","\n","    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, attention='Scale Dot Product'): \n","        super(BiLSTM_ATTENTION_no_CRF, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.vocab_size = vocab_size\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","        #add new codes start\n","        #add new initial variable self.nums_layers\n","        #self.domain_dim = domain_dim\n","        self.attention=attention\n","\n","        #if domain_dim >0:\n","\n","        #add new codes end\n","\n","        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n","\n","        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n","        #self.word_embeds.weight.data.copy_(torch.from_numpy(w2v_sg_emb_dim)) \n","        self.word_embeds.weight.data.copy_(torch.from_numpy(np.asarray(w2v_POS_domain_emb_table))) # need to replace w2v_sg_emb_dim to other embedding table\n","        \n","\n","\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n","                            num_layers=1, bidirectional=True) #+ domain_dim\n","        \n","        #add new code -Q K V\n","        if self.attention:\n","          self.Q = nn.Linear(hidden_dim, hidden_dim)\n","          self.K = nn.Linear(hidden_dim, hidden_dim)\n","          self.V = nn.Linear(hidden_dim, hidden_dim)\n","          self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n","        else:\n","          self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n","\n","        if self.attention == 'General':\n","          self.general = nn.Linear(hidden_dim, hidden_dim)\n","\n","        # Matrix of transition parameters.  Entry i,j is the score of\n","        # transitioning *to* i *from* j.\n","        self.transitions = nn.Parameter(\n","            torch.randn(self.tagset_size, self.tagset_size))\n","\n","        # These two statements enforce the constraint that we never transfer\n","        # to the start tag and we never transfer from the stop tag\n","        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n","        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n","        self.hidden = self.init_hidden()\n","\n","    def init_hidden(self):\n","\n","        return (torch.randn(2*1, 1, self.hidden_dim // 2).to(device),\n","                torch.randn(2*1, 1, self.hidden_dim // 2).to(device))\n","\n","    def _forward_alg(self, feats):\n","        # Do the forward algorithm to compute the partition function\n","        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n","        # START_TAG has all of the score.\n","        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n","\n","        # Wrap in a variable so that we will get automatic backprop\n","        forward_var = init_alphas\n","\n","        # Iterate through the sentence\n","        for feat in feats:\n","            alphas_t = []  # The forward tensors at this timestep\n","            for next_tag in range(self.tagset_size):\n","                # broadcast the emission score: it is the same regardless of\n","                # the previous tag\n","                emit_score = feat[next_tag].view(\n","                    1, -1).expand(1, self.tagset_size)\n","                # the ith entry of trans_score is the score of transitioning to\n","                # next_tag from i\n","                trans_score = self.transitions[next_tag].view(1, -1)\n","                # The ith entry of next_tag_var is the value for the\n","                # edge (i -> next_tag) before we do log-sum-exp\n","                next_tag_var = forward_var + trans_score + emit_score\n","                # The forward variable for this tag is log-sum-exp of all the\n","                # scores.\n","                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n","            forward_var = torch.cat(alphas_t).view(1, -1)\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        alpha = log_sum_exp(terminal_var)\n","        return alpha\n","\n","    #create self-attention here\n","    def cal_attention(self, lstm_out):\n","      #remove the batch size from the lstm output: (sequence length,batch_size, hidden_dim)-> (sequence length, hidden_dim)\n","      lstm_out=lstm_out.squeeze(1)\n","      #(batch size, sequnece len, hidden )\n","      query= self.Q(lstm_out).unsqueeze(0)\n","      key = self.K(lstm_out)\n","\n","      if self.attention == 'General':\n","        key =self.general(key)\n","\n","      key= key.transpose(1,0).unsqueeze(0)\n","      value= self.V(lstm_out).unsqueeze(0)\n","\n","      if self.attention == 'Scale Dot Product':\n","        #attention weight\n","        attention_weight = F.softmax(torch.bmm(query, key)/torch.sqrt(torch.Tensor([self.hidden_dim]).to(device)), dim=-1) #softmax on the last dimension\n","      elif self.attention == 'Dot Product':\n","        attention_weight = F.softmax(torch.bmm(query, key).to(device), dim=-1)\n","      #linear transfer K\n","      elif self.attention == 'General':\n","        attention_weight = F.softmax(torch.bmm(query, key).to(device), dim=-1)\n","\n","      attention_output = torch.bmm(attention_weight, value).transpose(1,0)\n","      \n","      return(attention_output)\n","\n","\n","    #def _get_lstm_features(self, sentence):\n","    def _get_lstm_features(self, sentence):\n","        \n","        self.hidden = self.init_hidden()\n","        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n","\n","\n","        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n","\n","        # add attention\n","\n","        if self.attention:\n","          attention_output =self.cal_attention(lstm_out)\n","          lstm_out= torch.cat((attention_output, lstm_out), -1)\n","          \n","          #remove batch size from the dimension\n","          lstm_out= lstm_out.view(len(sentence), self.hidden_dim*2)\n","        else:\n","          lstm_out= lstm_out.view(len(sentence), self.hidden_dim)\n","\n","        lstm_feats = self.hidden2tag(lstm_out)\n","\n","        return lstm_feats\n","\n","    def _score_sentence(self, feats, tags):\n","        # Gives the score of a provided tag sequence\n","        score = torch.zeros(1).to(device)\n","        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n","        for i, feat in enumerate(feats):\n","            score = score + \\\n","                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n","        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n","        return score\n","\n","    def _viterbi_decode(self, feats):\n","        backpointers = []\n","\n","        # Initialize the viterbi variables in log space\n","        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n","        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n","\n","        # forward_var at step i holds the viterbi variables for step i-1\n","        forward_var = init_vvars\n","        for feat in feats:\n","            bptrs_t = []  # holds the backpointers for this step\n","            viterbivars_t = []  # holds the viterbi variables for this step\n","\n","            for next_tag in range(self.tagset_size):\n","                # next_tag_var[i] holds the viterbi variable for tag i at the\n","                # previous step, plus the score of transitioning\n","                # from tag i to next_tag.\n","                # We don't include the emission scores here because the max\n","                # does not depend on them (we add them in below)\n","                next_tag_var = forward_var + self.transitions[next_tag]\n","                best_tag_id = argmax(next_tag_var)\n","                bptrs_t.append(best_tag_id)\n","                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n","            # Now add in the emission scores, and assign forward_var to the set\n","            # of viterbi variables we just computed\n","            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n","            backpointers.append(bptrs_t)\n","\n","        # Transition to STOP_TAG\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        best_tag_id = argmax(terminal_var)\n","        path_score = terminal_var[0][best_tag_id]\n","\n","        # Follow the back pointers to decode the best path.\n","        best_path = [best_tag_id]\n","        for bptrs_t in reversed(backpointers):\n","            best_tag_id = bptrs_t[best_tag_id]\n","            best_path.append(best_tag_id)\n","        # Pop off the start tag (we dont want to return that to the caller)\n","        start = best_path.pop()\n","        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n","        best_path.reverse()\n","        return path_score, best_path\n","\n","    def neg_log_likelihood(self, sentence, tags):\n","        feats = self._get_lstm_features(sentence)\n","        forward_score = self._forward_alg(feats)\n","        gold_score = self._score_sentence(feats, tags)\n","        return forward_score - gold_score\n","    \n","    def forward(self, sentence):\n","    #def forward(self, sentence):  # dont confuse this with _forward_alg above.\n","        # Get the emission scores from the BiLSTM\n","        lstm_feats = self._get_lstm_features(sentence)\n","\n","\n","        tag_seq = torch.argmax(F.softmax(lstm_feats, -1), -1)\n","        #print('Type is:', type(tag_seq))      \n","        return tag_seq.cpu().numpy().tolist()"]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"nAWeenppH3Ua"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","HIDDEN_DIM = 50\n","EMBEDDING_TAB= w2v_POS_domain_emb_table\n","EMBEDDING_DIM=EMBEDDING_TAB.shape[1]\n","ATTENTION='General'\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model=BiLSTM_ATTENTION_no_CRF(len(word_dict), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, ATTENTION).to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"],"metadata":{"id":"5x180nYcH5k-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datetime\n","\n","for epoch in range(2):  \n","    time1 = datetime.datetime.now()\n","    train_loss = 0\n","\n","    model.train()\n","    for i, idxs in enumerate(train_input_index):\n","        tags_index = train_output_index[i]\n","\n","        # Step 1. Remember that Pytorch accumulates gradients.\n","        # We need to clear them out before each instance\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is,\n","        # turn them into Tensors of word indices.\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","\n","        # Step 3. Run our forward pass.\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        # calling optimizer.step()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss+=loss.item()\n","\n","    model.eval()\n","    # Call the cal_acc functions you implemented as required\n","    _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n","    _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n","\n","    val_loss = 0\n","    for i, idxs in enumerate(val_input_index):\n","        tags_index = val_output_index[i]\n","        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n","        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","        val_loss+=loss.item()\n","    time2 = datetime.datetime.now()\n","\n","    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n"],"metadata":{"id":"Ce_UOIaxjJIe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653910998043,"user_tz":-600,"elapsed":1031369,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"}},"outputId":"7aab5142-6a94-45ee-9031-9edc6cde29cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:1, Training loss: 19164.41, train acc: 0.9936, val loss: 1307.09, val acc: 0.9911, time: 519.59s\n","Epoch:2, Training loss: 1889.45, train acc: 0.9987, val loss: 864.59, val acc: 0.9953, time: 511.38s\n"]}]},{"cell_type":"markdown","metadata":{"id":"LHWC5kvxbp1G"},"source":["###Save and Load Model"]},{"cell_type":"code","source":["torch.save(model,'/content/drive/MyDrive/COMP5046-NLP-AS2/Models/BiLSTM_ATTENTION_no_CRF_model.pt')"],"metadata":{"id":"jLSiQwg4yVfg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K5eSgxHAbsDY"},"outputs":[],"source":["BiLSTM_ATTENTION_no_CRF_model=torch.load('/content/drive/MyDrive/COMP5046-NLP-AS2/Models/BiLSTM_ATTENTION_no_CRF_model.pt')"]},{"cell_type":"markdown","metadata":{"id":"uV5Et1iWv1pI"},"source":["###Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6923,"status":"ok","timestamp":1653911004961,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"},"user_tz":-600},"id":"NBZt0zhsrHJi","outputId":"008feca0-505a-4142-f4cf-d96d1f0af956"},"outputs":[{"output_type":"stream","name":"stdout","text":["╒═══════════════════════════════╤════════════╕\n","│ baseline_CRF_bilstm_model_1   │   F1 Score │\n","╞═══════════════════════════════╪════════════╡\n","│ F1                            │   0.995293 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (T)                        │   0.966644 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (S)                        │   0.992173 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (P)                        │   0.99873  │\n","├───────────────────────────────┼────────────┤\n","│ F1 (O)                        │   0.999579 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (D)                        │   0.934673 │\n","├───────────────────────────────┼────────────┤\n","│ F1 (C)                        │   0.973796 │\n","╘═══════════════════════════════╧════════════╛\n"]}],"source":["BiLSTM_ATTENTION_no_CRF_model.eval()\n","predict=[]\n","true_label =[]\n","for sentence, tags in zip(tokensized_validation_docs,tokensized_validation_label_docs):\n","  sentence_in = prepare_sequence(sentence, word_dict).to(device)\n","  targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long).to(device)\n","  y_pred=BiLSTM_ATTENTION_no_CRF_model(sentence_in)\n","  predict.extend(y_pred)\n","  true_label.extend(targets.cpu().numpy().tolist())\n","\n","f1, f1_T_label, f1_S_label, f1_P_label, f1_O_label, f1_D_label, f1_C_label=calculate_f1(predict, true_label)\n","\n","\n","table= [[\"F1\",f1],\n","        [\"F1 (T)\",f1_T_label],\n","        [\"F1 (S)\",f1_S_label],\n","        [\"F1 (P)\",f1_P_label],\n","        [\"F1 (O)\",f1_O_label],\n","        [\"F1 (D)\",f1_D_label],\n","        [\"F1 (C)\",f1_C_label]\n","    \n","        ]\n","col_names=[\"baseline_CRF_bilstm_model_1\",\"F1 Score\"]\n","\n","Eval_Model_Name.append(\"BiLSTM_ATTENTION_no_CRF_model\")\n","Eval_result.append([el[1] for el in table])\n","\n","print(tabulate(table, headers=col_names,tablefmt='fancy_grid'))"]},{"cell_type":"markdown","source":["# Comparison Table - All"],"metadata":{"id":"Qoeu-8GMt85q"}},{"cell_type":"code","source":["result_df = pd.DataFrame(columns=['F1', 'F1 (T)', 'F1 (S)','F1 (P)','F1 (O)','F1 (D)','F1 (C) '])\n","for i in range(len(Eval_result)):\n","    result_df.loc[i] = Eval_result[i]\n","\n","result_df.index = Eval_Model_Name\n","result_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":457},"id":"RdW8coBvt8LX","executionInfo":{"status":"ok","timestamp":1653911020002,"user_tz":-600,"elapsed":349,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"}},"outputId":"6b91dfc9-709a-468d-bc8c-5b79c1fe2bf4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                       F1    F1 (T)    F1 (S)    F1 (P)  \\\n","baseline_CRF_bilstm_model_1      0.990466  0.947583  0.986454  0.995935   \n","w2v_CRF_bilstm_model             0.995473  0.965963  0.992775  0.997713   \n","POS_only_CRF_bilstm_model        0.994304  0.956433  0.987959  0.998222   \n","POS_w2v_CRF_bilstm_model         0.994483  0.957114  0.987959  0.998730   \n","POS_domain_CRF_bilstm_model      0.995233  0.967325  0.991872  0.998222   \n","POS_w2v_domain_CRF_bilstm_model  0.995593  0.965963  0.992173  0.998730   \n","twolayer_CRF_bilstm_model        0.994483  0.960517  0.990066  0.998984   \n","threelayer_CRF_bilstm_model      0.984919  0.924438  0.975918  0.996443   \n","attention_general_model          0.995623  0.965963  0.992474  0.998730   \n","attention_scale_dot_model        0.995413  0.967325  0.992474  0.998730   \n","attention_dot_product_model      0.995503  0.967325  0.991872  0.998984   \n","no_attention_no_CRF_model        0.995443  0.966644  0.992474  0.998222   \n","BiLSTM_ATTENTION_no_CRF_model    0.995293  0.966644  0.992173  0.998730   \n","\n","                                   F1 (O)    F1 (D)   F1 (C)   \n","baseline_CRF_bilstm_model_1      0.996313  0.871859  0.964046  \n","w2v_CRF_bilstm_model             1.000000  0.937186  0.973796  \n","POS_only_CRF_bilstm_model        0.999737  0.927136  0.972578  \n","POS_w2v_CRF_bilstm_model         0.999684  0.937186  0.972578  \n","POS_domain_CRF_bilstm_model      0.999842  0.917085  0.975015  \n","POS_w2v_domain_CRF_bilstm_model  0.999895  0.947236  0.973796  \n","twolayer_CRF_bilstm_model        0.999737  0.896985  0.973796  \n","threelayer_CRF_bilstm_model      0.997050  0.615578  0.945765  \n","attention_general_model          0.999684  0.957286  0.973796  \n","attention_scale_dot_model        0.999789  0.922111  0.975625  \n","attention_dot_product_model      0.999789  0.944724  0.972578  \n","no_attention_no_CRF_model        1.000000  0.929648  0.973796  \n","BiLSTM_ATTENTION_no_CRF_model    0.999579  0.934673  0.973796  "],"text/html":["\n","  <div id=\"df-5f2791b0-f09e-4e71-813a-3e7a022cf685\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>F1</th>\n","      <th>F1 (T)</th>\n","      <th>F1 (S)</th>\n","      <th>F1 (P)</th>\n","      <th>F1 (O)</th>\n","      <th>F1 (D)</th>\n","      <th>F1 (C)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>baseline_CRF_bilstm_model_1</th>\n","      <td>0.990466</td>\n","      <td>0.947583</td>\n","      <td>0.986454</td>\n","      <td>0.995935</td>\n","      <td>0.996313</td>\n","      <td>0.871859</td>\n","      <td>0.964046</td>\n","    </tr>\n","    <tr>\n","      <th>w2v_CRF_bilstm_model</th>\n","      <td>0.995473</td>\n","      <td>0.965963</td>\n","      <td>0.992775</td>\n","      <td>0.997713</td>\n","      <td>1.000000</td>\n","      <td>0.937186</td>\n","      <td>0.973796</td>\n","    </tr>\n","    <tr>\n","      <th>POS_only_CRF_bilstm_model</th>\n","      <td>0.994304</td>\n","      <td>0.956433</td>\n","      <td>0.987959</td>\n","      <td>0.998222</td>\n","      <td>0.999737</td>\n","      <td>0.927136</td>\n","      <td>0.972578</td>\n","    </tr>\n","    <tr>\n","      <th>POS_w2v_CRF_bilstm_model</th>\n","      <td>0.994483</td>\n","      <td>0.957114</td>\n","      <td>0.987959</td>\n","      <td>0.998730</td>\n","      <td>0.999684</td>\n","      <td>0.937186</td>\n","      <td>0.972578</td>\n","    </tr>\n","    <tr>\n","      <th>POS_domain_CRF_bilstm_model</th>\n","      <td>0.995233</td>\n","      <td>0.967325</td>\n","      <td>0.991872</td>\n","      <td>0.998222</td>\n","      <td>0.999842</td>\n","      <td>0.917085</td>\n","      <td>0.975015</td>\n","    </tr>\n","    <tr>\n","      <th>POS_w2v_domain_CRF_bilstm_model</th>\n","      <td>0.995593</td>\n","      <td>0.965963</td>\n","      <td>0.992173</td>\n","      <td>0.998730</td>\n","      <td>0.999895</td>\n","      <td>0.947236</td>\n","      <td>0.973796</td>\n","    </tr>\n","    <tr>\n","      <th>twolayer_CRF_bilstm_model</th>\n","      <td>0.994483</td>\n","      <td>0.960517</td>\n","      <td>0.990066</td>\n","      <td>0.998984</td>\n","      <td>0.999737</td>\n","      <td>0.896985</td>\n","      <td>0.973796</td>\n","    </tr>\n","    <tr>\n","      <th>threelayer_CRF_bilstm_model</th>\n","      <td>0.984919</td>\n","      <td>0.924438</td>\n","      <td>0.975918</td>\n","      <td>0.996443</td>\n","      <td>0.997050</td>\n","      <td>0.615578</td>\n","      <td>0.945765</td>\n","    </tr>\n","    <tr>\n","      <th>attention_general_model</th>\n","      <td>0.995623</td>\n","      <td>0.965963</td>\n","      <td>0.992474</td>\n","      <td>0.998730</td>\n","      <td>0.999684</td>\n","      <td>0.957286</td>\n","      <td>0.973796</td>\n","    </tr>\n","    <tr>\n","      <th>attention_scale_dot_model</th>\n","      <td>0.995413</td>\n","      <td>0.967325</td>\n","      <td>0.992474</td>\n","      <td>0.998730</td>\n","      <td>0.999789</td>\n","      <td>0.922111</td>\n","      <td>0.975625</td>\n","    </tr>\n","    <tr>\n","      <th>attention_dot_product_model</th>\n","      <td>0.995503</td>\n","      <td>0.967325</td>\n","      <td>0.991872</td>\n","      <td>0.998984</td>\n","      <td>0.999789</td>\n","      <td>0.944724</td>\n","      <td>0.972578</td>\n","    </tr>\n","    <tr>\n","      <th>no_attention_no_CRF_model</th>\n","      <td>0.995443</td>\n","      <td>0.966644</td>\n","      <td>0.992474</td>\n","      <td>0.998222</td>\n","      <td>1.000000</td>\n","      <td>0.929648</td>\n","      <td>0.973796</td>\n","    </tr>\n","    <tr>\n","      <th>BiLSTM_ATTENTION_no_CRF_model</th>\n","      <td>0.995293</td>\n","      <td>0.966644</td>\n","      <td>0.992173</td>\n","      <td>0.998730</td>\n","      <td>0.999579</td>\n","      <td>0.934673</td>\n","      <td>0.973796</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f2791b0-f09e-4e71-813a-3e7a022cf685')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-5f2791b0-f09e-4e71-813a-3e7a022cf685 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5f2791b0-f09e-4e71-813a-3e7a022cf685');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":76}]},{"cell_type":"code","source":["#embedding result:\n","result_df.iloc[[0,1,2,3,4,5], :]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"5OFo4S2J7Tcn","executionInfo":{"status":"ok","timestamp":1653913878590,"user_tz":-600,"elapsed":331,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"}},"outputId":"92bde197-5d75-4097-ca7b-24581892b9a3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                       F1    F1 (T)    F1 (S)    F1 (P)  \\\n","baseline_CRF_bilstm_model_1      0.990466  0.947583  0.986454  0.995935   \n","w2v_CRF_bilstm_model             0.995473  0.965963  0.992775  0.997713   \n","POS_only_CRF_bilstm_model        0.994304  0.956433  0.987959  0.998222   \n","POS_w2v_CRF_bilstm_model         0.994483  0.957114  0.987959  0.998730   \n","POS_domain_CRF_bilstm_model      0.995233  0.967325  0.991872  0.998222   \n","POS_w2v_domain_CRF_bilstm_model  0.995593  0.965963  0.992173  0.998730   \n","\n","                                   F1 (O)    F1 (D)   F1 (C)   \n","baseline_CRF_bilstm_model_1      0.996313  0.871859  0.964046  \n","w2v_CRF_bilstm_model             1.000000  0.937186  0.973796  \n","POS_only_CRF_bilstm_model        0.999737  0.927136  0.972578  \n","POS_w2v_CRF_bilstm_model         0.999684  0.937186  0.972578  \n","POS_domain_CRF_bilstm_model      0.999842  0.917085  0.975015  \n","POS_w2v_domain_CRF_bilstm_model  0.999895  0.947236  0.973796  "],"text/html":["\n","  <div id=\"df-953a102f-fa76-45e2-8b2a-c873c5c06f66\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>F1</th>\n","      <th>F1 (T)</th>\n","      <th>F1 (S)</th>\n","      <th>F1 (P)</th>\n","      <th>F1 (O)</th>\n","      <th>F1 (D)</th>\n","      <th>F1 (C)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>baseline_CRF_bilstm_model_1</th>\n","      <td>0.990466</td>\n","      <td>0.947583</td>\n","      <td>0.986454</td>\n","      <td>0.995935</td>\n","      <td>0.996313</td>\n","      <td>0.871859</td>\n","      <td>0.964046</td>\n","    </tr>\n","    <tr>\n","      <th>w2v_CRF_bilstm_model</th>\n","      <td>0.995473</td>\n","      <td>0.965963</td>\n","      <td>0.992775</td>\n","      <td>0.997713</td>\n","      <td>1.000000</td>\n","      <td>0.937186</td>\n","      <td>0.973796</td>\n","    </tr>\n","    <tr>\n","      <th>POS_only_CRF_bilstm_model</th>\n","      <td>0.994304</td>\n","      <td>0.956433</td>\n","      <td>0.987959</td>\n","      <td>0.998222</td>\n","      <td>0.999737</td>\n","      <td>0.927136</td>\n","      <td>0.972578</td>\n","    </tr>\n","    <tr>\n","      <th>POS_w2v_CRF_bilstm_model</th>\n","      <td>0.994483</td>\n","      <td>0.957114</td>\n","      <td>0.987959</td>\n","      <td>0.998730</td>\n","      <td>0.999684</td>\n","      <td>0.937186</td>\n","      <td>0.972578</td>\n","    </tr>\n","    <tr>\n","      <th>POS_domain_CRF_bilstm_model</th>\n","      <td>0.995233</td>\n","      <td>0.967325</td>\n","      <td>0.991872</td>\n","      <td>0.998222</td>\n","      <td>0.999842</td>\n","      <td>0.917085</td>\n","      <td>0.975015</td>\n","    </tr>\n","    <tr>\n","      <th>POS_w2v_domain_CRF_bilstm_model</th>\n","      <td>0.995593</td>\n","      <td>0.965963</td>\n","      <td>0.992173</td>\n","      <td>0.998730</td>\n","      <td>0.999895</td>\n","      <td>0.947236</td>\n","      <td>0.973796</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-953a102f-fa76-45e2-8b2a-c873c5c06f66')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-953a102f-fa76-45e2-8b2a-c873c5c06f66 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-953a102f-fa76-45e2-8b2a-c873c5c06f66');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":101}]},{"cell_type":"code","source":["#number of layer result\n","result_df.iloc[[5,6,7], :]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"N71Aza9D8w4p","executionInfo":{"status":"ok","timestamp":1653913907973,"user_tz":-600,"elapsed":306,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"}},"outputId":"bc601a23-cf3e-4ba7-d7ac-1f0181642824"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                       F1    F1 (T)    F1 (S)    F1 (P)  \\\n","POS_w2v_domain_CRF_bilstm_model  0.995593  0.965963  0.992173  0.998730   \n","twolayer_CRF_bilstm_model        0.994483  0.960517  0.990066  0.998984   \n","threelayer_CRF_bilstm_model      0.984919  0.924438  0.975918  0.996443   \n","\n","                                   F1 (O)    F1 (D)   F1 (C)   \n","POS_w2v_domain_CRF_bilstm_model  0.999895  0.947236  0.973796  \n","twolayer_CRF_bilstm_model        0.999737  0.896985  0.973796  \n","threelayer_CRF_bilstm_model      0.997050  0.615578  0.945765  "],"text/html":["\n","  <div id=\"df-2dcee0fe-a012-4b30-ab8a-08b7fd63ae66\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>F1</th>\n","      <th>F1 (T)</th>\n","      <th>F1 (S)</th>\n","      <th>F1 (P)</th>\n","      <th>F1 (O)</th>\n","      <th>F1 (D)</th>\n","      <th>F1 (C)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>POS_w2v_domain_CRF_bilstm_model</th>\n","      <td>0.995593</td>\n","      <td>0.965963</td>\n","      <td>0.992173</td>\n","      <td>0.998730</td>\n","      <td>0.999895</td>\n","      <td>0.947236</td>\n","      <td>0.973796</td>\n","    </tr>\n","    <tr>\n","      <th>twolayer_CRF_bilstm_model</th>\n","      <td>0.994483</td>\n","      <td>0.960517</td>\n","      <td>0.990066</td>\n","      <td>0.998984</td>\n","      <td>0.999737</td>\n","      <td>0.896985</td>\n","      <td>0.973796</td>\n","    </tr>\n","    <tr>\n","      <th>threelayer_CRF_bilstm_model</th>\n","      <td>0.984919</td>\n","      <td>0.924438</td>\n","      <td>0.975918</td>\n","      <td>0.996443</td>\n","      <td>0.997050</td>\n","      <td>0.615578</td>\n","      <td>0.945765</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2dcee0fe-a012-4b30-ab8a-08b7fd63ae66')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2dcee0fe-a012-4b30-ab8a-08b7fd63ae66 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2dcee0fe-a012-4b30-ab8a-08b7fd63ae66');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":102}]},{"cell_type":"code","source":["#Attention result\n","result_df.iloc[[5,8,9,10], :]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"arum2VLv84wz","executionInfo":{"status":"ok","timestamp":1653913945919,"user_tz":-600,"elapsed":467,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"}},"outputId":"26b2771d-f43c-4de4-d124-840534ad1cdb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                       F1    F1 (T)    F1 (S)    F1 (P)  \\\n","POS_w2v_domain_CRF_bilstm_model  0.995593  0.965963  0.992173  0.998730   \n","attention_general_model          0.995623  0.965963  0.992474  0.998730   \n","attention_scale_dot_model        0.995413  0.967325  0.992474  0.998730   \n","attention_dot_product_model      0.995503  0.967325  0.991872  0.998984   \n","\n","                                   F1 (O)    F1 (D)   F1 (C)   \n","POS_w2v_domain_CRF_bilstm_model  0.999895  0.947236  0.973796  \n","attention_general_model          0.999684  0.957286  0.973796  \n","attention_scale_dot_model        0.999789  0.922111  0.975625  \n","attention_dot_product_model      0.999789  0.944724  0.972578  "],"text/html":["\n","  <div id=\"df-58ab45a9-f332-4c8c-ac66-799277369ead\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>F1</th>\n","      <th>F1 (T)</th>\n","      <th>F1 (S)</th>\n","      <th>F1 (P)</th>\n","      <th>F1 (O)</th>\n","      <th>F1 (D)</th>\n","      <th>F1 (C)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>POS_w2v_domain_CRF_bilstm_model</th>\n","      <td>0.995593</td>\n","      <td>0.965963</td>\n","      <td>0.992173</td>\n","      <td>0.998730</td>\n","      <td>0.999895</td>\n","      <td>0.947236</td>\n","      <td>0.973796</td>\n","    </tr>\n","    <tr>\n","      <th>attention_general_model</th>\n","      <td>0.995623</td>\n","      <td>0.965963</td>\n","      <td>0.992474</td>\n","      <td>0.998730</td>\n","      <td>0.999684</td>\n","      <td>0.957286</td>\n","      <td>0.973796</td>\n","    </tr>\n","    <tr>\n","      <th>attention_scale_dot_model</th>\n","      <td>0.995413</td>\n","      <td>0.967325</td>\n","      <td>0.992474</td>\n","      <td>0.998730</td>\n","      <td>0.999789</td>\n","      <td>0.922111</td>\n","      <td>0.975625</td>\n","    </tr>\n","    <tr>\n","      <th>attention_dot_product_model</th>\n","      <td>0.995503</td>\n","      <td>0.967325</td>\n","      <td>0.991872</td>\n","      <td>0.998984</td>\n","      <td>0.999789</td>\n","      <td>0.944724</td>\n","      <td>0.972578</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58ab45a9-f332-4c8c-ac66-799277369ead')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-58ab45a9-f332-4c8c-ac66-799277369ead button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-58ab45a9-f332-4c8c-ac66-799277369ead');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":103}]},{"cell_type":"code","source":["#with and without CRF\n","result_df.iloc[[5,-2,8,-1], :]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"UpCBe8eW9A63","executionInfo":{"status":"ok","timestamp":1653914059502,"user_tz":-600,"elapsed":311,"user":{"displayName":"Yilin Qiao","userId":"11493101491075150914"}},"outputId":"ca0721fc-bcc4-4c6e-8a0e-0abd0847b1a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                       F1    F1 (T)    F1 (S)    F1 (P)  \\\n","POS_w2v_domain_CRF_bilstm_model  0.995593  0.965963  0.992173  0.998730   \n","no_attention_no_CRF_model        0.995443  0.966644  0.992474  0.998222   \n","attention_general_model          0.995623  0.965963  0.992474  0.998730   \n","BiLSTM_ATTENTION_no_CRF_model    0.995293  0.966644  0.992173  0.998730   \n","\n","                                   F1 (O)    F1 (D)   F1 (C)   \n","POS_w2v_domain_CRF_bilstm_model  0.999895  0.947236  0.973796  \n","no_attention_no_CRF_model        1.000000  0.929648  0.973796  \n","attention_general_model          0.999684  0.957286  0.973796  \n","BiLSTM_ATTENTION_no_CRF_model    0.999579  0.934673  0.973796  "],"text/html":["\n","  <div id=\"df-bef34e4b-3403-47d1-a6c5-16eb42ad712e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>F1</th>\n","      <th>F1 (T)</th>\n","      <th>F1 (S)</th>\n","      <th>F1 (P)</th>\n","      <th>F1 (O)</th>\n","      <th>F1 (D)</th>\n","      <th>F1 (C)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>POS_w2v_domain_CRF_bilstm_model</th>\n","      <td>0.995593</td>\n","      <td>0.965963</td>\n","      <td>0.992173</td>\n","      <td>0.998730</td>\n","      <td>0.999895</td>\n","      <td>0.947236</td>\n","      <td>0.973796</td>\n","    </tr>\n","    <tr>\n","      <th>no_attention_no_CRF_model</th>\n","      <td>0.995443</td>\n","      <td>0.966644</td>\n","      <td>0.992474</td>\n","      <td>0.998222</td>\n","      <td>1.000000</td>\n","      <td>0.929648</td>\n","      <td>0.973796</td>\n","    </tr>\n","    <tr>\n","      <th>attention_general_model</th>\n","      <td>0.995623</td>\n","      <td>0.965963</td>\n","      <td>0.992474</td>\n","      <td>0.998730</td>\n","      <td>0.999684</td>\n","      <td>0.957286</td>\n","      <td>0.973796</td>\n","    </tr>\n","    <tr>\n","      <th>BiLSTM_ATTENTION_no_CRF_model</th>\n","      <td>0.995293</td>\n","      <td>0.966644</td>\n","      <td>0.992173</td>\n","      <td>0.998730</td>\n","      <td>0.999579</td>\n","      <td>0.934673</td>\n","      <td>0.973796</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bef34e4b-3403-47d1-a6c5-16eb42ad712e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-bef34e4b-3403-47d1-a6c5-16eb42ad712e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-bef34e4b-3403-47d1-a6c5-16eb42ad712e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":106}]},{"cell_type":"markdown","source":["#Final Model testing -Kaggle"],"metadata":{"id":"gGkKmVmvqXEI"}},{"cell_type":"code","source":["attention_general_model\n","attention_general_model.eval()\n","predict=[]\n","true_label =[]\n","for sentence, tags in zip(tokensized_testing_docs,tokensized_validation_label_docs):\n","  sentence_in = prepare_sequence(sentence, word_dict).to(device)\n","  targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long).to(device)\n","  _, y_pred=attention_general_model(sentence_in)\n","  predict.extend(y_pred)\n","  true_label.extend(targets.cpu().numpy().tolist())"],"metadata":{"id":"J_k-pz5_nxcM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(predict)\n","\n","ix_to_tag= {0:\"C\", 1:\"D\", 2:\"O\", 3:\"P\", 4:\"S\", 5:\"SEPA\", 6:\"T\", 7:\"<START>\", 8:\"<STOP>\"}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MTd1Tr7nojBD","executionInfo":{"status":"ok","timestamp":1654076367732,"user_tz":-600,"elapsed":4,"user":{"displayName":"roger wang","userId":"04313800435014207614"}},"outputId":"9637b468-ebe6-4f1f-dd87-1a87253f9e1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[6, 2, 4, 2, 6, 4, 2, 4, 6, 6, 2, 4, 2, 4, 6, 2, 2, 2, 2, 2, 2, 2, 2, 0, 4, 2, 2, 6, 0, 0, 1, 2, 2, 2, 0, 4, 6, 1, 4, 6, 6, 2, 1, 6, 0, 4, 0, 0, 4, 4, 0, 4, 2, 1, 4, 1, 4, 4, 4, 0, 2, 2, 2, 4, 2, 6, 6, 2, 4, 4, 2, 0, 2, 2, 2, 4, 4, 3, 6, 2, 4, 4, 4, 2, 6, 4, 0, 2, 4, 0, 2, 2, 6, 0, 2, 6, 2, 1, 2, 3, 2, 3, 2, 2, 6, 3, 2, 2, 2, 2, 3, 4, 2, 6, 1, 5, 2, 2, 2, 5, 2, 6, 4, 6, 2, 1, 2, 2, 0, 2, 6, 2, 2, 2, 2, 3, 0, 2, 0, 6, 6, 4, 2, 4, 4, 2, 1, 3, 0, 6, 3, 2, 2, 0, 2, 4, 4, 2, 5, 4, 2, 2, 0, 2, 4, 4, 4, 4, 2, 2, 6, 3, 2, 1, 6, 1, 2, 4, 0, 2, 0, 2, 2, 4, 2, 2, 6, 2, 2, 6, 6, 4, 0, 6, 2, 6, 2, 2, 3, 2, 2, 6, 0, 2, 6, 2, 2, 0, 4, 6, 2, 0, 2, 3, 2, 3, 2, 0, 2, 2, 4, 0, 3, 2, 0, 4, 2, 2, 4, 2, 4, 3, 2, 2, 2, 2, 2, 3, 6, 3, 6, 6, 2, 6, 0, 2, 1, 2, 3, 6, 2, 2, 2, 0, 0, 2, 3, 4, 2, 2, 2, 3, 2, 2, 2, 6, 2, 2, 2, 6, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 4, 6, 2, 2, 2, 0, 2, 2, 2, 1, 2, 2, 4, 2, 2, 2, 1, 1, 0, 5, 2, 2, 2, 0, 2, 0, 5, 2, 2, 0, 2, 3, 2, 3, 3, 2, 4, 2, 0, 0, 2, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 6, 4, 4, 4, 4, 3, 5, 2, 5, 6, 5, 3, 5, 6, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 6, 2, 2, 2, 0, 2, 2, 3, 2, 2, 2, 2, 2, 2, 5, 2, 0, 5, 2, 5, 1, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 0, 2, 5, 4, 2, 2, 2, 5, 2, 1, 2, 5, 2, 5, 2, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 1, 2, 2, 2, 3, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 1, 6, 5, 4, 0, 6, 1, 0, 5, 6, 3, 2, 2, 2, 2, 0, 6, 5, 0, 2, 2, 5, 2, 6, 3, 3, 4, 2, 6, 3, 2, 3, 0, 2, 2, 5, 2, 4, 5, 6, 3, 1, 2, 2, 5, 6, 3, 6, 1, 6, 5, 4, 6, 3, 1, 2, 2, 2, 2, 2, 3, 2, 2, 3, 1, 2, 0, 5, 3, 2, 2, 6, 0, 2, 5, 0, 3, 2, 5, 2, 6, 3, 2, 6, 0, 2, 1, 2, 5, 2, 2, 2, 2, 2, 0, 3, 2, 3, 2, 2, 3, 2, 3, 1, 0, 6, 3, 6, 2, 3, 2, 6, 0, 2, 4, 3, 6, 2, 0, 5, 2, 3, 0, 2, 3, 3, 5, 2, 6, 3, 0, 2, 3, 0, 0, 2, 6, 3, 2, 6, 2, 5, 4, 2, 5, 1, 3, 2, 6, 2, 1, 2, 1, 4, 2, 2, 0, 2, 1, 3, 2, 3, 4, 4, 5, 2, 6, 2, 1, 2, 5, 2, 3, 1, 2, 2, 2, 5, 2, 5, 2, 2, 0, 5, 2, 5, 6, 4, 5, 4, 2, 2, 0, 3, 0, 2, 1, 2, 5, 3, 2, 4, 4, 2, 0, 3, 5, 2, 3, 3, 2, 2, 1, 0, 6, 5, 2, 6, 3, 0, 3, 2, 6, 4, 1, 2, 3, 2, 3, 0, 0, 2, 1, 4, 5, 2, 2, 5, 6, 2, 3, 0, 5, 4, 2, 0, 2, 3, 2, 2, 2, 2, 5, 2, 1, 2, 2, 2, 2, 2, 0, 2, 5, 3, 2, 2, 1, 4, 4, 6, 5, 2, 2, 3, 2, 2, 6, 3, 2, 2, 0, 3, 2, 2, 2, 2, 0, 5, 2, 2, 2, 4, 1, 5, 2, 2, 4, 1, 3, 2, 1, 2, 2, 3, 2, 6, 1, 3, 2, 2, 6, 2, 2, 2, 2, 5, 2, 4, 0, 1, 2, 2, 4, 2, 3, 2, 3, 2, 2, 4, 2, 5, 0, 2, 2, 2, 3, 2, 2, 1, 5, 2, 2, 5, 1, 5, 0, 5, 2, 2, 2, 5, 2, 2, 2, 2, 3, 1, 2, 2, 3, 2, 2, 3, 2, 3, 3, 1, 2, 3, 2, 6, 3, 2, 2, 6, 4, 4, 5, 1, 6, 0, 2, 3, 2, 0, 2, 2, 6, 2, 4, 2, 2, 2, 5, 2, 5, 6, 3, 2, 3, 2, 0, 4, 2, 1, 2, 2, 2, 0, 4, 3, 2, 6, 2, 2, 2, 3, 0, 3, 6, 2, 2, 3, 2, 3, 0, 2, 2, 3, 3, 2, 1, 3, 2, 2, 4, 5, 2, 3, 5, 2, 3, 2, 0, 2, 2, 2, 5, 2, 5, 6, 5, 2, 4, 2, 2, 3, 2, 3, 2, 2, 3, 0, 2, 2, 2, 3, 2, 3, 6, 2, 1, 2, 2, 4, 2, 2, 5, 0, 2, 0, 2, 2, 5, 2, 2, 5, 2, 5, 0, 5, 2, 2, 2, 5, 2, 6, 2, 5, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 4, 2, 2, 2, 0, 4, 2, 5, 2, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 6, 5, 3, 2, 2, 6, 2, 2, 2, 5, 2, 2, 0, 3, 2, 2, 2, 2, 3, 2, 4, 2, 2, 3, 2, 2, 2, 3, 1, 2, 3, 2, 2, 2, 2, 4, 3, 2, 2, 2, 0, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 4, 2, 2, 5, 2, 2, 2, 2, 3, 2, 1, 2, 2, 2, 5, 0, 5, 2, 5, 2, 5, 2, 2, 2, 2, 2, 3, 4, 2, 2, 5, 2, 2, 2, 4, 5, 3, 2, 2, 0, 2, 2, 2, 2, 5, 2, 2, 2, 2, 2, 2, 0, 5, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 3, 2, 0, 5, 3, 2, 4, 5, 3, 2, 4, 5, 2, 2, 2, 6, 2, 2, 3, 3, 2, 0, 5, 3, 2, 4, 3, 2, 2, 2, 2, 2, 5, 4, 5, 2, 5, 0, 2, 5, 2, 3, 2, 2, 2, 2, 5, 2, 5, 2, 2, 5, 2, 2, 3, 2, 2, 2, 2, 2, 5, 2, 0, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 1, 5, 2, 2, 2, 2, 2, 2, 1, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 6, 5, 2, 2, 5, 2, 5, 2, 5, 2, 2, 2, 5, 2, 2, 6, 0, 2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 3, 2, 2, 4, 3, 6, 5, 2, 3, 1, 2, 2, 2, 2, 5, 2, 5, 2, 5, 2, 2, 2, 5, 2, 5, 2, 5, 2, 5, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 6, 3, 2, 3, 2, 3, 2, 3, 2, 2, 4, 5, 2, 0, 4, 4, 5, 6, 6, 5, 2, 4, 3, 5, 4, 2, 0, 2, 3, 6, 2, 5, 6, 0, 2, 5, 3, 2, 4, 5, 2, 5, 2, 6, 2, 5, 0, 0, 5, 0, 5, 5, 2, 5, 0, 6, 2, 5, 4, 2, 3, 6, 2, 2, 3, 5, 2, 3, 2, 2, 0, 6, 6, 5, 3, 2, 2, 2, 3, 5, 1, 0, 1, 5, 2, 1, 4, 5, 2, 2, 5, 4, 5, 6, 4, 5, 2, 2, 0, 2, 2, 5, 1, 5, 2, 4, 5, 2, 5, 2, 2, 4, 5, 6, 5, 4, 5, 6, 3, 6, 2, 2, 0, 5, 2, 2, 0, 6, 5, 2, 3, 2, 6, 2, 2, 2, 3, 0, 2, 0, 0, 2, 6, 6, 2, 0, 0, 2, 1, 0, 6, 2, 1, 2, 0, 2, 5, 0, 1, 0, 2, 5, 2, 0, 5, 2, 5, 6, 2, 3, 2, 3, 2, 3, 1, 0, 2, 2, 2, 2, 0, 2, 1, 5, 2, 0, 5, 2, 5, 2, 4, 2, 2, 5, 6, 5, 4, 4, 2, 5, 3, 2, 5, 4, 4, 6, 2, 3, 2, 4, 4, 5, 4, 5, 2, 2, 5, 2, 5, 0, 2, 6, 2, 5, 2, 2, 2, 5, 3, 2, 5, 2, 3, 2, 2, 3, 2, 2, 5, 6, 2, 0, 5, 2, 2, 2, 1, 5, 2, 3, 1, 5, 4, 2, 5, 2, 4, 0, 2, 2, 3, 2, 2, 5, 3, 2, 4, 3, 0, 2, 2, 6, 5, 3, 2, 2, 2, 1, 0, 2, 3, 2, 4, 2, 2, 5, 2, 3, 5, 2, 6, 3, 6, 0, 2, 3, 2, 2, 2, 2, 2, 5, 2, 1, 4, 4, 5, 2, 2, 2, 5, 2, 3, 2, 2, 2, 6, 4, 0, 2, 3, 2, 6, 2, 2, 2, 2, 0, 5, 6, 2, 4, 2, 5, 2, 4, 2, 4, 6, 2, 0, 2, 0, 2, 6, 5, 0, 2, 2, 4, 2, 5, 2, 5, 2, 2, 2, 3, 2, 2, 4, 2, 4, 2, 2, 3, 6, 2, 2, 2, 2, 5, 2, 3, 2, 6, 2, 1, 2, 5, 2, 2, 2, 5, 2, 3, 6, 2, 4, 0, 5, 2, 2, 5, 2, 5, 2, 3, 2, 3, 5, 3, 2, 2, 2, 2, 3, 2, 3, 5, 0, 4, 1, 5, 1, 5, 2, 2, 5, 3, 2, 2, 2, 6, 2, 2, 2, 2, 2, 5, 4, 0, 2, 2, 2, 2, 3, 6, 3, 2, 5, 2, 5, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 5, 2, 2, 2, 2, 2, 2, 1, 5, 6, 5, 2, 3, 2, 2, 2, 2, 2, 5, 2, 3, 3, 2, 0, 2, 3, 3, 5, 2, 2, 2, 2, 2, 3, 2, 5, 2, 2, 2, 2, 5, 0, 2, 3, 5, 3, 2, 2, 5, 2, 2, 3, 2, 5, 2, 5, 2, 3, 2, 1, 3, 2, 2, 3, 3, 2, 2, 2, 3, 6, 2, 6, 2, 5, 2, 6, 5, 6, 3, 2, 2, 2, 1, 2, 5, 0, 2, 2, 2, 2, 2, 2, 3, 2, 5, 1, 2, 5, 2, 3, 2, 2, 2, 3, 2, 2, 6, 3, 0, 4, 5, 2, 0, 4, 5, 2, 5, 2, 2, 5, 2, 5, 2, 2, 2, 2, 2, 2, 3, 2, 5, 6, 2, 0, 2, 5, 4, 2, 3, 5, 4, 0, 2, 0, 4, 0, 5, 0, 2, 2, 2, 2, 4, 0, 5, 2, 3, 1, 5, 3, 0, 2, 4, 5, 2, 5, 3, 6, 3, 2, 4, 2, 2, 2, 5, 4, 2, 2, 1, 2, 2, 2, 2, 4, 4, 4, 3, 1, 5, 2, 3, 2, 2, 2, 5, 3, 2, 1, 2, 2, 2, 2, 2, 2, 0, 2, 5, 2, 2, 5, 2, 4, 2, 2, 3, 2, 2, 4, 2, 2, 1, 2, 2, 2, 1, 1, 2, 5, 3, 2, 2, 4, 5, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 4, 0, 2, 0, 2, 2, 2, 4, 2, 1, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 5, 2, 2, 2, 2, 5, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 5, 3, 2, 2, 2, 4, 2, 2, 5, 2, 3, 5, 2, 2, 2, 2, 5, 2, 2, 2, 2, 3, 2, 6, 3, 2, 2, 6, 2, 2, 3, 5, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 5, 3, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 3, 2, 5, 2, 2, 6, 2, 5, 6, 2, 2, 2, 2, 2, 2, 5, 2, 2, 2, 2, 2, 2, 2, 1, 2, 4, 2, 2, 0, 5, 2, 2, 5, 0, 2, 2, 2, 2, 2, 5, 2, 2, 2, 5, 2, 2, 4, 5, 0, 4, 5, 2, 2, 5, 4, 4, 0, 5, 3, 2, 2, 4, 0, 3, 2, 6, 2, 0, 5, 1, 2, 5, 6, 6, 2, 6, 3, 0, 5, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 6, 0, 2, 2, 5, 0, 2, 3, 2, 2, 2, 3, 5, 2, 5, 2, 3, 0, 0, 5, 3, 2, 2, 2, 1, 2, 5, 0, 2, 0, 5, 2, 2, 2, 5, 0, 5, 2, 2, 2, 5, 4, 3, 4, 0, 1, 2, 5, 2, 3, 2, 6, 0, 0, 4, 2, 2, 4, 5, 2, 4, 2, 5, 6, 5, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 6, 3]\n"]}]},{"cell_type":"code","source":["result_list=[]\n","\n","for i in predict:\n","  b=ix_to_tag[i]\n","  result_list.append(b)"],"metadata":{"id":"Vn-bhKF5o3qv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv\n","result = pd.DataFrame(result_list)\n","result.to_csv('/content/drive/MyDrive/COMP5046-NLP-AS2/predict_result.csv',index=False)"],"metadata":{"id":"cL7GT94-pSq1"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"AS2.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}